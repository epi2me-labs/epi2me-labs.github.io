<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[EPI2ME Labs's RSS Feed]]></title><description><![CDATA[Tutorials and workflows for nanopore sequencing.]]></description><link>https://labs.epi2me.io</link><generator>GatsbyJS</generator><lastBuildDate>Wed, 26 Jun 2024 12:04:25 GMT</lastBuildDate><item><title><![CDATA[IGV for EPI2ME workflows]]></title><description><![CDATA[IGV integration for EPI2ME workflows.]]></description><link>https://labs.epi2me.io/igv-integration</link><guid isPermaLink="false">https://labs.epi2me.io/igv-integration</guid><pubDate>Mon, 10 Jun 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/4706606e75320d7ea3ed39e6fbc3c2ae/59ccf/syrian-hamster.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;While we strive to automate data analysis as much as possible, producing summaries, variant calls and sometimes full interpretations of sequencing data, it’s still very important to understand the raw data behind our results. It can be useful to poke and prod at the raw sequences underlying, for instance, a marginal variant call, or the arrangement of a structural variant. Sirisha recently wrote about doing this in her practical &lt;a href=&quot;https://labs.epi2me.io/reviewing-bam/&quot;&gt;guide to reviewing BAM files&lt;/a&gt;. To further streamline the process of manually reviewing workflow outputs, we’ve added built-in support for the Integrative Genomics Viewer (&lt;a href=&quot;https://igv.org&quot;&gt;IGV&lt;/a&gt;) in the latest version of the &lt;a href=&quot;https://labs.epi2me.io/downloads/&quot;&gt;EPI2ME Desktop app&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;We’re adding IGV to all of our workflows where read-level exploration is useful, starting with the five supported today. Whether for identifying PCR artifacts with &lt;code&gt;wf-amplicon&lt;/code&gt; or quality controlling vector-based biopharmaceuticals with &lt;code&gt;wf-aav-qc&lt;/code&gt;, IGV offers an interactive, read-level view of your sequencing data without leaving the EPI2ME app. As of version 5.1.14, IGV is available in the following workflows from the new &lt;code&gt;Viewer&lt;/code&gt; tab:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;&lt;code&gt;wf-alignment&lt;/code&gt;&lt;/strong&gt;: &lt;em&gt;Are aligned reads of good quality; do they have secondary alignments?&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;code&gt;wf-amplicon&lt;/code&gt;&lt;/strong&gt;: &lt;em&gt;Are there any unexpected variants, perhaps at low frequency?&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;code&gt;wf-metagenomics&lt;/code&gt;&lt;/strong&gt; &amp;amp; &lt;strong&gt;&lt;code&gt;wf-16s&lt;/code&gt;&lt;/strong&gt;: &lt;em&gt;Is a taxonomic assignment supported by even read coverage?&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;code&gt;wf-aav-qc&lt;/code&gt;&lt;/strong&gt;: &lt;em&gt;Are there any sequence changes or structural abnormalities in the product&lt;/em&gt;?&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;scrutinising-variant-calls-using-igv-in-epi2me-desktop&quot;&gt;Scrutinising variant calls using IGV in EPI2ME Desktop&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/4ae267d9a05dc33e060998dcadbf1c6c/wf-amplicon-igv-demo.gif&quot; alt=&quot;Using IGV working inside EPI2ME&quot;/&gt;&lt;/p&gt;&lt;p&gt;Running the &lt;a href=&quot;https://github.com/epi2me-labs/wf-amplicon&quot;&gt;&lt;code&gt;wf-amplicon&lt;/code&gt;&lt;/a&gt; workflow using built-in test data now generates a tab named &lt;code&gt;Viewer&lt;/code&gt;. Opening this reveals an interactive IGV track showing read support for the reference sequence. Nucleotide positions differing from the reference sequence are displayed as coloured blocks. In this case we see well-supported nucleotide substitutions at reference positions 315 and 318.&lt;/p&gt;&lt;p&gt;This functionality is implemented by embedding a IGV.js instance in the Viewer tab of the EPI2ME app. As soon as an IGV config file is produced by a workflow and all of the files referenced within it are present, the &lt;code&gt;Viewer&lt;/code&gt; tab automatically renders an IGV browser. Try it for yourself by downloading &lt;a href=&quot;https://labs.epi2me.io/downloads/&quot;&gt;EPI2ME Desktop&lt;/a&gt;.&lt;/p&gt;&lt;h2 id=&quot;related-articles&quot;&gt;Related articles&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://labs.epi2me.io/reviewing-bam/&quot;&gt;A guide to reviewing BAM files&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://labs.epi2me.io/querying-vcf-files/&quot;&gt;Querying a VCF file&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://labs.epi2me.io/quickstart/&quot;&gt;EPI2ME Desktop quickstart&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/4706606e75320d7ea3ed39e6fbc3c2ae/59ccf/syrian-hamster.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/4706606e75320d7ea3ed39e6fbc3c2ae/syrian-hamster.jpeg</content:toenail></item><item><title><![CDATA[EPI2ME 24.06-01 Release]]></title><description><![CDATA[With all the excitement of London Calling we skipped our previous release window, so this one is a cracker!]]></description><link>https://labs.epi2me.io/epi2me-24.06-01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-24.06-01-release</guid><pubDate>Wed, 05 Jun 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/3d09deeabbbf5386322d5acc806ac637/59ccf/sheep-race.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Dear Nanopore Community&lt;/p&gt;&lt;p&gt;In this post London Calling ‘24 release we are delighted to introduce a broad collection of updates, improvements, and modifications to our EPI2ME Desktop Application, and the bioinformatics workflows that it can orchestrate. These updates were presented by Sirisha, Sarah, and Natalia during their presentations.&lt;/p&gt;&lt;h2 id=&quot;epi2me-desktop-application-v5114&quot;&gt;EPI2ME Desktop Application &lt;a href=&quot;/downloads/&quot;&gt;v5.1.14&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Introduction of IGV to the Desktop Application - browse alignments and query your BAM files directly within the application - this is supported across all workflows that produce BAM format output.&lt;/li&gt;&lt;li&gt;Minimum workflow specifications - the Desktop Application will help you avoid running computionally and resource demanding workflows on inappropriate computer infrastructure.&lt;/li&gt;&lt;li&gt;Support for self signed certificates&lt;/li&gt;&lt;li&gt;Support for input data on non C: drives for Windows users &lt;/li&gt;&lt;li&gt;Various other general fixes and enhancements&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img src=&quot;./baby.gif&quot; alt=&quot;IGV in EPI2ME Desktop&quot; title=&quot;Figure 1. EPI2ME Desktop Application [v5.1.4] introduces the IGV browser as a plugin to further explore your mapped sequence data from workflows that produce BAM files. In this example a couple of SNPs identified by the wf-amplicon workflow are being investigated.&quot;/&gt;&lt;/p&gt;&lt;h2 id=&quot;workflow-updates&quot;&gt;Workflow Updates&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-template/releases/tag/v5.1.4&quot;&gt;wf-template v5.1.4&lt;/a&gt;    &lt;ul&gt;&lt;li&gt;IGV support examples&lt;/li&gt;&lt;li&gt;Sample grouping implementation&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling/releases/tag/v1.2.0&quot;&gt;wf-basecalling v1.2.0&lt;/a&gt;&lt;ul&gt;&lt;li&gt;dorado updated to 0.7.1&lt;/li&gt;&lt;li&gt;&lt;strong&gt;New!&lt;/strong&gt; Poly(A) tail length calling with the new parameter &lt;code&gt;--poly-a-config&lt;/code&gt; (More info here: &lt;a href=&quot;https://github.com/nanoporetech/dorado?tab=readme-ov-file#polya-tail-estimation&quot;&gt;https://github.com/nanoporetech/dorado?tab=readme-ov-file#polya-tail-estimation&lt;/a&gt;)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;New!&lt;/strong&gt; Demultiplexing support. The &lt;code&gt;BC&lt;/code&gt; (barcode) tag will be added to the output &lt;code&gt;CRAM&lt;/code&gt; file and demuxed &lt;code&gt;BAM&lt;/code&gt;s will be produced in the folder&lt;code&gt;demuxed&lt;/code&gt; when you specify &lt;code&gt;--barcode_kit&lt;/code&gt;. Demuxing parameters can be tuned with the addition of any of &lt;code&gt;dorado demux&lt;/code&gt; args with the parameter &lt;code&gt;--demux_args&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-amplicon/releases/tag/v1.1.0&quot;&gt;wf-amplicon v1.1.0&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;New!&lt;/strong&gt; IGV support&lt;/li&gt;&lt;li&gt;Some bug fixes and minor improvements to the report    &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-alignment/releases/tag/v1.2.0&quot;&gt;wf-alignment v1.2.0&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;New!&lt;/strong&gt; IGV support&lt;/li&gt;&lt;li&gt;Streamlined and simplified report; should use a lot less memory now    &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics/releases/tag/v2.10.0&quot;&gt;wf-metagenomics v2.10.0&lt;/a&gt; &lt;a href=&quot;https://github.com/epi2me-labs/wf-16s/releases/tag/v2.10.0&quot;&gt;wf-16s v1.2.0&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;New!&lt;/strong&gt; IGV support in the minimap2 approach&lt;/li&gt;&lt;li&gt;Added:&lt;ul&gt;&lt;li&gt;Reads below percentages of identity (&lt;code&gt;min_percent_identity&lt;/code&gt;) and the reference covered (&lt;code&gt;min_ref_coverage&lt;/code&gt;) are considered as unclassified in the minimap2 approach.&lt;/li&gt;&lt;li&gt;The workflow now uses the `fastcat` read length and quality histograms instead of the per-read stats in the report process which reduces the memory use by the process.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Fixed:&lt;ul&gt;&lt;li&gt;Files that are empty following the fastcat filtering are discarded from downstream analyses.&lt;/li&gt;&lt;li&gt;Checking the correspondence between the reference and ref2taxid now also works with compressed references.&lt;/li&gt;&lt;li&gt;“Can only use .dt accessor with datetimelike values” error in makeReport&lt;/li&gt;&lt;li&gt;“invalid literal for int() with base 10” error in makeRepory&lt;/li&gt;&lt;li&gt;Request less memory if &lt;code&gt;kraken2_memory_mapping&lt;/code&gt; is used.&lt;/li&gt;&lt;li&gt;Show the percentage of each species when hovering over the taxonomy bar plot.        &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation/releases/tag/v1.3.0&quot;&gt;wf-clone-validation v1.3.0&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;New!&lt;/strong&gt; &lt;code&gt;full_reference&lt;/code&gt; parameter for some additional assembly vs provided reference QC and a pLannotate bug fix&lt;/li&gt;&lt;li&gt;BAM inputs now accepted (&lt;code&gt;--bam&lt;/code&gt;)    &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation/releases/tag/v2.2.3&quot;&gt;wf-human-variation v2.2.3&lt;/a&gt;&lt;ul&gt;&lt;li&gt;Improvements to memory management for alignment should finally rid users of a class of 137 errors. &lt;/li&gt;&lt;li&gt;Minor performance improvements to the methylation subworkflow. &lt;/li&gt;&lt;li&gt;Simplified phasing options.&lt;/li&gt;&lt;li&gt;The workflow now supports ingress of multiple BAM files. &lt;/li&gt;&lt;li&gt;Updates to Clair3, Longphase, Modkit and Spectre.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-aav-qc/releases/tag/v1.1.0&quot;&gt;wf-aav v1.1.0&lt;/a&gt;&lt;ul&gt;&lt;li&gt;Output BAM files tagged with assigned AAV genome type (&lt;code&gt;AV:Z&lt;/code&gt;).&lt;/li&gt;&lt;li&gt;BAM files can now be split by assigned genome type using the option  &lt;code&gt;output_genometype_bams&lt;/code&gt;.&lt;/li&gt;&lt;li&gt;Medaka model is now automatically selected by identifying basecaller model with the input reads.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;New!&lt;/strong&gt; Alignment results are now visible in the EPI2ME desktop app IGV viewer. &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell/releases/tag/v2.1.0&quot;&gt;wf-single-cell v2.1.0&lt;/a&gt;&lt;ul&gt;&lt;li&gt;This release Includes changes introduced since v2.0.0&lt;/li&gt;&lt;li&gt;Several parts of the workflow are reworked to provide significant performance improvement&lt;ul&gt;&lt;li&gt;Users can expect ~ 3x speed up.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Various updates to decrease peak memory usage including at the UMAP and expression matrix construction stages.&lt;/li&gt;&lt;li&gt;BAM inputs now accepted (&lt;code&gt;--bam&lt;/code&gt;)&lt;/li&gt;&lt;li&gt;UMAP generation is now fast and so is now created by default (removed &lt;code&gt;--plot_umaps&lt;/code&gt;)&lt;/li&gt;&lt;li&gt;BAMs are output by sample not per chromosome (removed &lt;code&gt;--merge_bam&lt;/code&gt;)&lt;/li&gt;&lt;li&gt;Read batching is simplified with &lt;code&gt;--adapter_scan_chunk_size&lt;/code&gt; and &lt;code&gt;--process_chunk_size&lt;/code&gt; replaced with &lt;code&gt;--fastq_chunk&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;--kit_name&lt;/code&gt; and &lt;code&gt;--kit_version&lt;/code&gt; replaced with &lt;code&gt;--kit&lt;/code&gt; (eg:  &lt;code&gt;--kit 3prime:v3&lt;/code&gt;)&lt;/li&gt;&lt;li&gt;5prime:v2 kit added to available 10x kits&lt;/li&gt;&lt;li&gt;Expression matrices are now output as sparse MEX files.&lt;/li&gt;&lt;li&gt;updating Stringtie&lt;/li&gt;&lt;li&gt;Several bug fixes including:&lt;ul&gt;&lt;li&gt;Report table displaying cell count off by one&lt;/li&gt;&lt;li&gt;Corrected number of unique genes and transcripts in report summary table&lt;/li&gt;&lt;li&gt;Fixed outputing incomplete BAM files introduced in v1.1.0&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation/releases/tag/v1.2.1&quot;&gt;wf-somatic-variation v1.2.1&lt;/a&gt;&lt;ul&gt;&lt;li&gt;General performance improvements of the workflow&lt;/li&gt;&lt;li&gt;Introducing Severus as the somatic structural variant caller&lt;/li&gt;&lt;li&gt;Updates to Clair3, modkit and snpEff&lt;/li&gt;&lt;li&gt;Support for multiple BAM for single sample&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes/releases/tags/v1.2.0&quot;&gt;wf-transcriptomes v1.2.0&lt;/a&gt;&lt;ul&gt;&lt;li&gt;This workflow now accepts BAM input&lt;/li&gt;&lt;li&gt;MA plot in output directory has been updated to match the MA plot in the report.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Please do let us know your feedback and we’re eager to hear wonderful new ideas for workflows and tutorials. Do watch our blog posts that will further describe some of these updates.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/3d09deeabbbf5386322d5acc806ac637/59ccf/sheep-race.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/3d09deeabbbf5386322d5acc806ac637/sheep-race.jpeg</content:toenail></item><item><title><![CDATA[Nanopore-only T2T assembly of a human genome]]></title><description><![CDATA[We present the only state-of-the-art assembly of a human genome using a single sequencing technology.]]></description><link>https://labs.epi2me.io/lc2024_t2t</link><guid isPermaLink="false">https://labs.epi2me.io/lc2024_t2t</guid><pubDate>Wed, 22 May 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/8756c974d21b4e9e0bcec008cabe1e84/32411/epi2me-t2t-assembly.png" length="0" type="image/png"/><content:encoded>&lt;p&gt;We are pleased to announce the release of a new addition to the Oxford Nanopore Open Data project as part of London Calling 2024: a Nanopore-only telomere-to-telomere (T2T) assembly dataset.
This dataset was created using our new telomere-to-telomere (T2T) workflow, combining ultra-long reads, Pore-C and our new assembly-polishing chemistry to completely resolve haplotypes and achieve a state-of-the-art Q50 human assembly.
To register your interest in the T2T workflow, &lt;a href=&quot;https://register.nanoporetech.com/telomere-to-telomere-bundle&quot;&gt;please click here&lt;/a&gt;. &lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The following cell line samples were obtained from the NIGMS Human Genetic Cell
Repository at the Coriell Institute for Medical Research: GM24385&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;This dataset contains the inputs and outputs of the T2T workflow, comprising basecalled reads from sequencing runs using four PromethION flow cells:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Two Ultra Long (SQK-ULK114) runs, basecalled using the model &lt;code&gt;dna_r10.4.1_e8.2_400bps_sup@v5.0.0&lt;/code&gt;&lt;/li&gt;&lt;li&gt;One Pore-C (SQK-LSK114) run, basecalled with the model &lt;code&gt;dna_r10.4.1_e8.2_400bps_hac@v4.3.0&lt;/code&gt;&lt;/li&gt;&lt;li&gt;One Assembly Polishing (SQK-APK114, using the 6b4 method) run, basecalled with a bespoke APK model that will be provided alongside the T2T bundle.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;data-location&quot;&gt;Data location&lt;/h3&gt;&lt;p&gt;As with previous releases the new dataset is available for anonymous download from
an Amazon Web Services S3 bucket. The bucket is part of the &lt;a href=&quot;https://aws.amazon.com/opendata/&quot;&gt;Open Data on AWS&lt;/a&gt;
project enabling sharing and analysis of a wide range of data.&lt;/p&gt;&lt;p&gt;The data is located at:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;s3://ont-open-data/londoncalling2024/assembly/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;See the &lt;a href=&quot;/tutorials/&quot;&gt;tutorials&lt;/a&gt; page for information on downloading datasets from S3.&lt;/p&gt;&lt;p&gt;The structure of the S3 prefix is shown below.
The primary assembly outputs are present under the &lt;code&gt;assm&lt;/code&gt; prefix.
The basecalling outputs have been converted for to BAM for consistency using &lt;code&gt;samtools import&lt;/code&gt; where there were not already present in this format.
The APK and and ULK reads aligned to the v1.0.1 HG002 T2T assembly (see below) are present in the &lt;code&gt;qc-demo&lt;/code&gt; prefix.
Finally, a copy of this reference and a corresponding minimap2 index are under the &lt;code&gt;ref&lt;/code&gt; prefix.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;.
├── assm
│   ├── all_correct.fasta.gz
│   ├── assembly.fasta
│   ├── assembly.homopolymer-compressed.gfa
│   └── medaka-6b4.fastq.gz
├── basecalling
│   ├── apk
│   │   └── PAW41746.bam
│   ├── pore-c
│   │   └── PAW44788.bam
│   └── ulk
│       ├── PAW42495.bam
│       └── PAW42666.bam
├── qc-demo
│   ├── apk-align
│   │   ├── PAW41746.bam
│   │   ├── PAW41746.bam.bai
│   │   ├── PAW41746.histograms
│   │   └── PAW41746.stats.gz
│   └── ulk-align
│       ├── PAW42495.bam
│       ├── PAW42495.bam.bai
│       ├── PAW42495.flagstats
│       ├── PAW42495.histograms
│       ├── PAW42495.stats.gz
│       ├── PAW42666.bam
│       ├── PAW42666.bam.bai
│       ├── PAW42666.flagstats
│       ├── PAW42666.histograms
│       └── PAW42666.stats.gz
├── ref
│   ├── hg002v1.0.1.fasta.gz
│   └── hg002v1.0.1.fasta.gz.mmi
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;basecalling-and-qc-analysis&quot;&gt;Basecalling and QC analysis&lt;/h3&gt;&lt;p&gt;Basecalling was performed using research-grade &lt;a href=&quot;https://github.com/nanoporetech/bonito&quot;&gt;bonito&lt;/a&gt;, though results would be comparable with the newest version of &lt;a href=&quot;https://github.com/nanoporetech/dorado&quot;&gt;dorado&lt;/a&gt; supporting transformer basecalling (v5.0.0).&lt;/p&gt;&lt;p&gt;For the purposes of exposition of data quality only, reads have been aligned to the v1.0.1 HG002 T2T assembly from the &lt;a href=&quot;https://sites.google.com/ucsc.edu/t2tworkinggroup&quot;&gt;Telomere-to-Telomere Consortium&lt;/a&gt;. Alignment was performed for the ULK and APK datasets only, using the &lt;code&gt;-x lr:hq&lt;/code&gt; preset of &lt;a href=&quot;https://github.com/lh3/minimap2&quot;&gt;minimap2&lt;/a&gt;. Alignment statistics were calculated using the &lt;code&gt;bamstats&lt;/code&gt; program available in the &lt;a href=&quot;https://github.com/epi2me-labs/fastcat&quot;&gt;fastcat&lt;/a&gt; package.&lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;statsPlot&quot; plotCaption=&quot;Figure 1. Sequencing summary metrics for one sequencing run each using Oxford Nanopore Technologies ULK and APK sequencing chemistries. Alignment accuracy was measured with the bamstats program from the fastcat suite.&quot;&gt;&lt;/div&gt;&lt;h3 id=&quot;assembly-workflow&quot;&gt;Assembly workflow&lt;/h3&gt;&lt;p&gt;The Nanopore-only telomere-to-telomere assembly workflow is summarised in Figure 2.
Ultra Long (SQK-ULK114) reads pass through the self-correction algorithm now available in dorado.
This algorithm used is derived from that available in the &lt;a href=&quot;https://github.com/lbcb-sci/herro&quot;&gt;herro&lt;/a&gt; project.
The corrected reads are then assembled using the &lt;a href=&quot;https://github.com/marbl/verkko&quot;&gt;verkko&lt;/a&gt; assembler. The assembler is run until the &lt;a href=&quot;https://github.com/marbl/rukki&quot;&gt;rukki&lt;/a&gt; step, at which data from Pore-C is injected into the &lt;code&gt;verkko&lt;/code&gt; pipeline with &lt;a href=&quot;https://github.com/rlorigro/GFAse&quot;&gt;GFAse&lt;/a&gt; being run to incorporate the Pore-C data into the assembly. The &lt;code&gt;verkko&lt;/code&gt; pipeline is resumed by first re-running &lt;code&gt;rukki&lt;/code&gt; and then continuing as normal. Finally &lt;a href=&quot;https://github.com/nanoporetech/medaka&quot;&gt;&lt;code&gt;medaka&lt;/code&gt;&lt;/a&gt; is run to correct the assembly further using the data from APK sequencing.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:69.82456140350877%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAACpklEQVR42lVS+UsqYRSd/zQoacEg3k9BUZmZWmRpiVubuUQRCQb+oKaG5TJuYy6jpb1XjbmXL0gdzaVTvtfjXT6YM2fu/ebccy/R6/UR7fb7+zt7cnI7M3OlUFzL5ZRMFtnaojY2IsBgZDJqevqKJPOdDtvpdPtfQQwe9Xq93+/S9PP5+aPHU/D7835/wePJud0MSRZ9vjxIu/2hWKy322yr1fpT7HbnNJr47m5sfz95cEDH4xWwoVDp8DBtMmVOT2+NxnQ0WgaZSFTVamTG9/YSEOVyMQTQ3JxXLA6KxYH5+YBE4pyd/SGReDkch0BA8ni+sTGXSOTg8abl8qvRURdIHA7Hhl6Is7M7VCqVUZXqemUlZLVmGSZjsdyJREG5PKpQRNfWwhZL5v7+xun8JRaHpNLI5iaFErP5jnh+bqZSL8HgYyxWBqhWm1BYrTbS6RpFFcLhJwDkgHx9bWWzv5PJIk2XM5kaMonj45uJCYdIFBAI/OPjDp2ORp5en5qcdK6uBoXCAJfrPDq6AQmNU1MuiSSEBrlcB/ol0DemotUm4Nb6ethmu0eexfIT16nVMRyhkByQFxfM8nIAmjG8pSXSZMoStRoLDRTF0HQVYKCw9zV9lmUbjWb/X3zSb2+NRoMFwAsBSSMjNj7fv7DgGx62aTQxZGEN8HlQDNDt9r5vbDYx5zYASMLrzUObTpc0GlNK5TU24TsPy4D6/v/Bss12+++SVCqfxobDTDJZSadfBrILhTpFVUKhJ5JkAHK5N5Dlcj0YLHm9jz5fLhAowfnPJRkassKAxUUfACTgxwZDAs5jQ/h8EgDS0IrZnEECGsQI0KBUShFYZpgMq7VauB1xuSC7d3nJ7Owk9HraYEhtb8ftdrjdiUZLSmUMa4vpqlQxq/XhAyB9lBHAYcJ0AAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;High-level assembly workflow&quot; title=&quot;Figure 2. Nanopore-only T2T assembly workflow. All read types are basecalled with dorado, with ULK reads being self-corrected with the new correction algorithm available in dorado. The Verkko assembler is used with GFAse for phasing and medaka for polishing.&quot; src=&quot;/static/99a6806bf214c64f2dc0e9d0677f2035/b5cea/assm-workflow.png&quot; srcSet=&quot;/static/99a6806bf214c64f2dc0e9d0677f2035/0e2fe/assm-workflow.png 285w,/static/99a6806bf214c64f2dc0e9d0677f2035/432e7/assm-workflow.png 570w,/static/99a6806bf214c64f2dc0e9d0677f2035/b5cea/assm-workflow.png 1140w,/static/99a6806bf214c64f2dc0e9d0677f2035/c45c7/assm-workflow.png 1346w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 2. Nanopore-only T2T assembly workflow. All read types are basecalled with dorado, with ULK reads being self-corrected with the new correction algorithm available in dorado. The Verkko assembler is used with GFAse for phasing and medaka for polishing.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;This workflow is embodied in the following prototype bash snippets.
It is assumed all the tools are available in the user’s environment. We begin by defining some files and paths and running &lt;code&gt;verkko&lt;/code&gt; until the &lt;code&gt;rukki&lt;/code&gt; step:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# set paths to relevant files
export GFASE_DIR=&amp;lt;gfase install dir&amp;gt;
export POREC_FASTA=&amp;lt;path to porec reads&amp;gt;
export CORRECTED_READS=&amp;lt;path to corrected ulk reads&amp;gt;
export RAW_READS=&amp;lt;path to uncorrected ulk reads&amp;gt;

# run verkko first time, up until rukki step
verrko --hifi &amp;quot;${CORRECTED_READS}&amp;quot; --nano &amp;quot;${RAW_READS}&amp;quot; \
    --hap-kmers /dev/null /dev/null trio -d verkko_results \
    --snakeopts &amp;quot;--until rukki&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We then incorporate the Pore-C sequencing data. This must be homopolymer compressed and then aligned to the unitigs created by &lt;code&gt;verkko&lt;/code&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export MINIMAP_THREADS=256

# homopolymer compress pore-c fasta
seqtk hpc $POREC_FASTA &amp;gt; porec_simplex.hpc.fasta

# align pore-c data
pushd verkko_results/6-rukki
minimap2 -ax lr:hq -t ${MINIMAP_THREADS} -I 20G \
    unitig-unrolled-unitig-unrolled-popped-unitig-normal-connected-tip.fasta \
    ../../porec_simplex.hpc.fasta \
    | samtools view -b@20 -q 1 \
    &amp;gt; porecVedges.bam
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and phase the assembly graph using &lt;code&gt;GFAse&lt;/code&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export GFASE_THREADS=48

# run gfase
$GFASE_DIR/build/phase_contacts_with_monte_carlo -i porecVedges.bam \
    -g ../5-untip/unitig-unrolled-unitig-unrolled-popped-unitig-normal-connected-tip.gfa \
    -o GFase --skip_unzip --use_homology -t ${GFASE_THREADS} -m 2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After this we need to do a little wrangling to obtain files from which we can continue the &lt;code&gt;verkko&lt;/code&gt; workflow:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# convert gfase outputs to rukki inputs
mv unitig-unrolled-unitig-unrolled-popped-unitig-normal-connected-tip.colors.csv \
    trio.colors.csv

head -n 1 trio.colors.csv &amp;gt; gfase.colors.csv

cat GFase/phases.csv | awk &amp;#x27;BEGIN{FS=&amp;quot;,&amp;quot;;OFS=&amp;quot;\t&amp;quot;} \
    $2==&amp;quot;1&amp;quot;&amp;amp;&amp;amp;$3&amp;gt;=10{print $1,$3*10,&amp;quot;0&amp;quot;,$3*10&amp;quot;:0&amp;quot;,&amp;quot;#FF8888&amp;quot;} \
    $2==&amp;quot;-1&amp;quot;&amp;amp;&amp;amp;$3&amp;gt;=10{print $1,&amp;quot;0&amp;quot;,$3*10,&amp;quot;0:&amp;quot;$3*10,&amp;quot;#8888FF&amp;quot;} \
    $2==&amp;quot;1&amp;quot;&amp;amp;&amp;amp;$3&amp;lt;10{print $1,$3*10,&amp;quot;0&amp;quot;,$3*10&amp;quot;:0&amp;quot;,&amp;quot;#AAAAAA&amp;quot;} \
    $2==&amp;quot;-1&amp;quot;&amp;amp;&amp;amp;$3&amp;lt;10{print $1,&amp;quot;0&amp;quot;,$3*10,&amp;quot;0:&amp;quot;$3*10,&amp;quot;#AAAAAA&amp;quot;}&amp;#x27; \
    &amp;gt;&amp;gt; gfase.colors.csv

awk &amp;#x27;{print $1&amp;quot;\t&amp;quot;}&amp;#x27; gfase.colors.csv &amp;gt; gfasedNodes.list

fgrep -f gfasedNodes.list -v \
    unitig-unrolled-unitig-unrolled-popped-unitig-normal-connected-tip.noseq.gfa \
    | awk &amp;#x27;/^S/{print $2&amp;quot;\t0\t0\t0:0\t#AAAAAA&amp;quot;}&amp;#x27; \
    &amp;gt;&amp;gt; gfase.colors.csv

cp gfase.colors.csv \
    unitig-unrolled-unitig-unrolled-popped-unitig-normal-connected-tip.colors.csv

# jump back up to the top-level directory
popd 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With that minor detour out of the way, we can continue our journey and finish off the &lt;code&gt;verkko&lt;/code&gt; pipeline:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;verrko --hifi &amp;quot;${CORRECTED_READS}&amp;quot; --nano &amp;quot;${RAW_READS}&amp;quot; \
    --hap-kmers /dev/null /dev/null trio -d verkko_results
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Having run &lt;code&gt;verkko&lt;/code&gt; in its entirety, the last step is to polish the assembly using data from the APK sequencing kit. We do this using &lt;code&gt;medaka&lt;/code&gt; and a special consensus model that simultaneously uses both ULK and APK data for error correction:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;APK_BAM=&amp;lt;path to apk reads as unaligned BAM&amp;gt;
ULK_BAM=&amp;lt;path to ulk reads as unaligned BAM&amp;gt;
VERKKO_ASSEMBLY=&amp;lt;path to verkko assembly&amp;gt;
OUTPUT=assm-corrected

medaka_consensus_joint \
    -i &amp;quot;${APK_BAM}&amp;quot; -v apk -i &amp;quot;${ULK_BAM}&amp;quot; -v ulk \
    -t ${THREADS} -o &amp;quot;${OUTPUT}&amp;quot; \
    -m r1041_e82_260bps_joint_apk_ulk_v5.0.0 \
    -d &amp;quot;${VERKKO_ASSEMBLY}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;further-information&quot;&gt;Further information&lt;/h3&gt;&lt;p&gt;To register your interest in the T2T workflow, &lt;a href=&quot;https://register.nanoporetech.com/telomere-to-telomere-bundle&quot;&gt;please click here&lt;/a&gt;.
For additional information regarding these data please contact &lt;a href=&quot;mailto:support@nanoporetech.com&quot;&gt;support@nanoporetech.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;We hope that these data and analyses provide a useful resource to the community.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/8756c974d21b4e9e0bcec008cabe1e84/32411/epi2me-t2t-assembly.png</content:thumbnail><content:toenail>https://labs.epi2me.io/static/8756c974d21b4e9e0bcec008cabe1e84/epi2me-t2t-assembly.png</content:toenail></item><item><title><![CDATA[London calling datasets 2024]]></title><description><![CDATA[Access to the datasets used in the London calling 2024 EPI2ME posters.]]></description><link>https://labs.epi2me.io/lc2024-datasets</link><guid isPermaLink="false">https://labs.epi2me.io/lc2024-datasets</guid><pubDate>Tue, 21 May 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/75cd548901a71555b4bdc5badb8ba451/59ccf/ears_pricked.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;We are excited to share these datasets which can be used to explore features of some of our workflows and the EPI2ME Desktop Application.&lt;/p&gt;&lt;p&gt;You may have seen one of our posters at an event (like &lt;a href=&quot;https://nanoporetech.com/about/events/conferences/lc24&quot;&gt;London Calling 2024&lt;/a&gt;) and followed a link to find the dataset that was featured in the poster.
Alternatively, please take a look at the associated posters linked below.&lt;/p&gt;&lt;p&gt;Also, please see the &lt;a href=&quot;https://labs.epi2me.io/tutorials/&quot;&gt;tutorials&lt;/a&gt; page for information on downloading the datasets.&lt;/p&gt;&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;&lt;p&gt;At London Calling 2024 we are making available a suite of datasets to illustrate how routine and simple biological questions may be answered with Oxford Nanopore Technologies’ sequencing devices and data analysis through bioinformatics workflows provided through EPI2ME.&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Data set&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;th&gt;Download address&lt;/th&gt;&lt;th&gt;Associated poster&lt;/th&gt;&lt;th&gt;EPI2ME Workflow&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;ZymoBIOMICS Fecal Reference&lt;/td&gt;&lt;td&gt;High diversity metagenomic sample to perform gut microbiome studies from &lt;a href=&quot;https://zymoresearch.eu/pages/zymobiomics-fecal-reference-with-trumatrix-technology&quot;&gt;ZymoBIOMICS&lt;/a&gt;&lt;/td&gt;&lt;td&gt;s3://ont-open-data/londoncalling2024/zymo-metagenomics&lt;/td&gt;&lt;td&gt;A straightforward approach to find out what is in your sample.&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot;&gt;wf-metagenomics&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Plasmid&lt;/td&gt;&lt;td&gt;Demultiplexed plasmid samples (5 replicates) containing an inserted sequence of 7mers&lt;/td&gt;&lt;td&gt;s3://ont-open-data/londoncalling2024/rbk-plasmid&lt;/td&gt;&lt;td&gt;Validating a molecular cloning experiment using &lt;em&gt;de novo&lt;/em&gt; assembly of nanopore reads.&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation&quot;&gt;wf-clone-validation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Lambda and &lt;em&gt;E. coli&lt;/em&gt;&lt;/td&gt;&lt;td&gt;Selection of 5 amplicons with lengths between 500 and 5000 bp (0.5, 1, 2 and 5 kb from Lambda phage 1kb from &lt;em&gt;E .coli&lt;/em&gt;)&lt;/td&gt;&lt;td&gt;s3://ont-open-data/londoncalling2024/lambda-ecoli-amplicons&lt;/td&gt;&lt;td&gt;Quick and simple analysis of PCR products with Nanopore reads.&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-amplicon&quot;&gt;wf-amplicon&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;All of our workflows include a demonstration dataset available through the “Run with demo data” option in our EPI2ME Desktop application, and detailed in the workflow source code repositories. We also have many other open data sets available &lt;a href=&quot;https://labs.epi2me.io/dataindex/&quot;&gt;here&lt;/a&gt;. Moreover, you can find example reports and links to extensive explanatory documentation for each of our workflows &lt;a href=&quot;https://labs.epi2me.io/wfindex/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h2 id=&quot;further-information&quot;&gt;Further information&lt;/h2&gt;&lt;p&gt;For additional information or questions regarding any of these resources please contact &lt;a href=&quot;mailto:support@nanoporetech.com&quot;&gt;support@nanoporetech.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;We hope that these datasets and posters provide a useful resource to the community!&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/75cd548901a71555b4bdc5badb8ba451/59ccf/ears_pricked.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/75cd548901a71555b4bdc5badb8ba451/ears_pricked.jpeg</content:toenail></item><item><title><![CDATA[London Calling 2024]]></title><description><![CDATA[How to get your EPI2ME Labs questions answered at London Calling 2024.]]></description><link>https://labs.epi2me.io/london-calling-2024</link><guid isPermaLink="false">https://labs.epi2me.io/london-calling-2024</guid><pubDate>Fri, 17 May 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/60cfbf5c73b2bb51a7e0f13b4a0feae3/59ccf/lc24.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;TEN YEARS!!! Over the last decade we’ve seen some incredible innovation presented by users of the technology at London Calling and we’re proud to be a part of that by enabling the analysis of anything by anyone. The EPI2ME team is excited to be in attendance for this special iteration of London Calling. Here is a rundown of what we’ll be up to and where you can find us across the 4 days at &lt;a href=&quot;https://londoncallingconf.co.uk/lc24&quot;&gt;London Calling 2024&lt;/a&gt;:&lt;/p&gt;&lt;p&gt;The EPI2ME team are available at the venue across the whole event. You’ll find us downstairs in the data analysis lounge or hanging around one of our new integrated end-to-end
products. We have a presentation at Data after dark, demos in the data analysis lounge with users of EPI2ME, and demos on Friday’s Clinical and BioPharma day.&lt;/p&gt;&lt;p&gt;More information about London Calling 2024, how to register for online attendance (in person is SOLD OUT), and the jam packed agenda can be found on the conference website: &lt;a href=&quot;https://londoncallingconf.co.uk/lc24&quot;&gt;https://londoncallingconf.co.uk/lc24&lt;/a&gt;. Please take a look!&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Data after dark: Sirisha Hesketh - &lt;strong&gt;Tuesday @ 18:45 (The Jam)&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;Product Demo Bioinformatics: Sarah Giffiths - &lt;strong&gt;Wednesday @ 12:30 (Data Analysis Lounge)&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;Product Demo Bioinformatics: Natalia Garcia - &lt;strong&gt;Wednesday @ 17:15 (Data Analysis Lounge)&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;Product Demo Clinical and BioPharma applications: Matt Parker &amp;amp; Sirisha Hesketh - &lt;strong&gt;Friday @ 14:55 (Live Lounge)&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;nikol-prokopova&quot;&gt;Nikol Prokopova&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Days&lt;/strong&gt;: Wed &amp;amp; Thurs&lt;/p&gt;&lt;p&gt;Nikol is our newest developer on the EPI2ME desktop application team! She’s ready to hear your requests for the app.&lt;/p&gt;&lt;h3 id=&quot;sarah-griffiths&quot;&gt;Sarah Griffiths&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Days&lt;/strong&gt;: Wed &amp;amp; Thurs&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Workflow specialties&lt;/strong&gt;: wf-clone-validation, wf-bacterial-genomes, wf-pore-c&lt;/p&gt;&lt;p&gt;Sarah will also be MC’ing a demo with users of the EPI2ME application and workflows in the data analysis lounge on Wednesday.&lt;/p&gt;&lt;h3 id=&quot;sam-nicholls&quot;&gt;Sam Nicholls&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Days&lt;/strong&gt;: Wed &amp;amp; Thurs&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Workflow specialties&lt;/strong&gt;: wf-human-variation, wf-amplicon and much much more!&lt;/p&gt;&lt;h3 id=&quot;sirisha-hesketh&quot;&gt;Sirisha Hesketh&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Days&lt;/strong&gt;: Wed, Thurs &amp;amp; Fri&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Workflow specialties&lt;/strong&gt;: wf-human-variation, wf-amplicon&lt;/p&gt;&lt;p&gt;Sirisha will reveal what is coming to the EPI2ME platform at Data after Dark on Tuesday evening in the “EPI2ME Everywhere” talk. Sirisha will also be demoing our human variation workflow in the Live Lounge on Friday.&lt;/p&gt;&lt;h3 id=&quot;natalia-garcia&quot;&gt;Natalia Garcia&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Days&lt;/strong&gt;: Wed &amp;amp; Thurs&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Workflow specialties&lt;/strong&gt;: wf-metagenomics, wf-16S&lt;/p&gt;&lt;p&gt;Natalie will also be hosting users in the data analysis lounge on Wednesday.&lt;/p&gt;&lt;h3 id=&quot;chris-alder&quot;&gt;Chris Alder&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Days&lt;/strong&gt;: Wed &amp;amp; Thurs&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Workflow specialties&lt;/strong&gt;: wf-bacterial-genomes, wf-flu, wf-tb-amr (AmPORE-TB)&lt;/p&gt;&lt;h3 id=&quot;matt-parker-me&quot;&gt;Matt Parker (Me!)&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Days&lt;/strong&gt;: Wed, Thurs &amp;amp; Fri&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Workflow specialties&lt;/strong&gt;: wf-flu, wf-artic, wf-tb-amr (AmPORE-TB)&lt;/p&gt;&lt;p&gt;Matt is going to be demoing AmPORE-TB and BioPharma applications in the Live Lounge on Friday.&lt;/p&gt;&lt;hr/&gt;&lt;p&gt;A few of us will be around on Tuesday for the advanced bioinformatics training hackathon, including both Neil Horner and our venerable product manager; Stephen Rudd. &lt;/p&gt;&lt;p&gt;As always, the rest of the EPI2ME team will be available via the online platform to answer any questions our audience might have during the event, if you have any questions for us, don’t hesitate to reach out.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/10561b683ce75ec7b5f4b09e54bfc969/sam_matt.jpg&quot; alt=&quot;Sam and Matt at the LC2023 tech talk.&quot;/&gt;&lt;/p&gt;&lt;p&gt;See you in London!&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/60cfbf5c73b2bb51a7e0f13b4a0feae3/59ccf/lc24.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/60cfbf5c73b2bb51a7e0f13b4a0feae3/lc24.jpg</content:toenail></item><item><title><![CDATA[The first EPI2ME Hackathon]]></title><description><![CDATA[The EPI2ME Mainz Hackhathon experience]]></description><link>https://labs.epi2me.io/mainz-hackathon</link><guid isPermaLink="false">https://labs.epi2me.io/mainz-hackathon</guid><pubDate>Tue, 26 Mar 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/7d3f9360c53a8e066700003c1bd51fbf/59ccf/mainz-hackathon.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;It hasn’t been long since we held the inaugural EPI2ME hackathon in Mainz, Germany on the 4th March.
The event was co-organised by &lt;a href=&quot;https://nanoporetech.com/&quot;&gt;Oxford Nanopore Technologies&lt;/a&gt; and we enjoyed it so much that we intend to host many more in the future.
Look out for upcoming announcements if you want to get involved!&lt;/p&gt;&lt;p&gt;The event was also co-hosted by Professor Dr. Susanne Gerber from the Computational Systems Genomics Group at the Institute of Human Genetics, &lt;a href=&quot;https://www.unimedizin-mainz.de/index.html&quot;&gt;University Medical Center Mainz&lt;/a&gt;, who contributed her fantastic expertise in the analysis of Oxford Nanopore RNA sequencing data.&lt;/p&gt;&lt;p&gt;During the event, co-organizers Stefan Pastore and Andre Holzer also did an excellent job of keeping everyone engaged, informed and on-mission. &lt;/p&gt;&lt;p&gt;The themes of the event were building high quality bioinformatics workflows, integration into EPI2ME and unleashing real-time long-read analytics.
We gathered scientists from around Europe and as far as the USA to talk bioinformatics, dashboards, Nextflow and more over the course of a week.&lt;/p&gt;&lt;h2 id=&quot;preparing-the-canvas&quot;&gt;Preparing the canvas&lt;/h2&gt;&lt;p&gt;Before the co-organisers had the opportunity to introduce their work, and prior to any teams being formed, we — the attending members of the EPI2ME team and other members from ONT — presented on a range of topics, focussing on sharing information about our process and best practices for authoring workflows.&lt;/p&gt;&lt;p&gt;Firstly there were general introductions to Nanopore sequencing and analysis solutions, and afterwards we elaborated on the EPI2ME software universe, describing all our workflows, our Desktop Application and the scalability of the out-of-the-box workflows we equip our customers with.&lt;/p&gt;&lt;p&gt;Then we moved onto workflow development, highlighting key considerations when writing a workflow, as well as a description of how to integrate workflows into EPI2ME with the least effort possible.&lt;/p&gt;&lt;p&gt;We also provided an insight into &lt;a href=&quot;https://github.com/epi2me-labs/wf-template&quot;&gt;wf-template&lt;/a&gt;, our entry point to best practice workflow development, and how this can be used in the context of the Hackathon to speed up efforts!&lt;/p&gt;&lt;p&gt;Things only got more technical as the morning went on, specifically with a presentation about how we balance speed of execution, resilience and complexity within the design of our workflows.
In particular, we covered how we tackle design issues during the development of high-performance workflows using our popular &lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation&quot;&gt;wf-human-variation&lt;/a&gt; project as an example.&lt;/p&gt;&lt;p&gt;In the afternoon, we re-focussed on one of the overall themes of the event - real time analysis - and gave a presentation and insights into how &lt;a href=&quot;https://labs.epi2me.io/progressive-kraken2/&quot;&gt;real time&lt;/a&gt; has been implemented in &lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot;&gt;wf-metagenomics&lt;/a&gt; using Nextflow, starting with easy to follow examples and moving on to more difficult ones.&lt;/p&gt;&lt;p&gt;Whilst it was a lot to take in, we really hope that everyone who attended got a flavour of how and why we do things.
We’re positive that we shared some good tips for writing maintainable, high-performance, real-time analysis pipelines that integrate with the EPI2ME ecosystem and Desktop Application.&lt;/p&gt;&lt;h2 id=&quot;setting-the-scene&quot;&gt;Setting the scene&lt;/h2&gt;&lt;p&gt;During the hackathon, the participants were given the opportunity to contribute to one of two different projects:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;RT-metagenomics: a 16S rRNA subworkflow. A pipeline from &lt;a href=&quot;https://elifesciences.org/articles/61504&quot;&gt;Urban &lt;em&gt;et al&lt;/em&gt;. 2021&lt;/a&gt;, presented to attendees by Andre Holzer, in which nanopore long reads are used to monitor freshwater microbial communities by profiling their taxonomy through 16S rRNA gene amplicons.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/Nanopore-Hackathon/NanopoReaTA_hackathon&quot;&gt;NanopoReaTA - Nanopore Real-Time Transcriptional Analysis Tool&lt;/a&gt;. A workflow from &lt;a href=&quot;https://academic.oup.com/bioinformatics/article/39/8/btad492/7238212&quot;&gt;Wierczeiko &lt;em&gt;et al&lt;/em&gt;. 2023&lt;/a&gt;, presented to participants by Stefan Pastore and Anna Wierczeiko, as a real-time analysis toolbox for cDNA and direct RNA-sequencing data from ONT with an intuitive dashboard, aimed at simplifying analyses such as differential expression profiles between conditions.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So, we had two different applications, two distinct pipelines but one common goal: harness the built-in advantages of Oxford Nanopore Technologies sequencing devices to perform real-time analysis.
Naturally, participants split up into two main teams with the aim of developing the workflow to follow best practices, integrate with EPI2ME, and enhance the real-time aspects of the workflows especially in regards to visualisation.&lt;/p&gt;&lt;h2 id=&quot;sketching-the-workflows&quot;&gt;Sketching the workflows&lt;/h2&gt;&lt;p&gt;At the end of three days, only one of the workflows had made it into EPI2ME but a massive amount of work had gone in to improving everything from the overall architecture of the pipelines, down to the dockerisation of the tools they rely on, documentation, configuration and of course enhancements to the real time dashboards they produced.&lt;/p&gt;&lt;p&gt;One of the themes of the talks on the first day was that good workflows are well maintained, living projects that are always gradually evolving.
We are certain that with another push the two teams will arrive at the goal of a maintainable, distributable bioinformatic tool that ‘just works’ when run in EPI2ME.&lt;/p&gt;&lt;h2 id=&quot;admiring-the-results&quot;&gt;Admiring the results&lt;/h2&gt;&lt;p&gt;The hackathon allowed participants to come together and share ideas, experience, anecdotes and perspectives on how to use Nextflow, Oxford Nanopore software and devices, and more.
In addition, EPI2ME team members were on hand throughout to offer guidance and the odd bit of debugging — we look forward to seeing many of you at London Calling!&lt;/p&gt;&lt;p&gt;This has been a great experience for us, and we hope the same can be said for all the participants and the organisers who we are very grateful to.
During the event we also had the opportunity to hear feedback from the community about our products, and we’ll be incorporating much of this as we go forward.&lt;/p&gt;&lt;p&gt;Once again, watch out for the next hackathon announcement and don’t miss your opportunity to get involved!&lt;/p&gt;&lt;p&gt;Andrea Talenti, Natalia Garcia, Tom Rich&lt;/p&gt;&lt;p&gt;&lt;em&gt;Join us on Tuesday 21st May 2014 in person at Old Billingsgate to take part in one of our pre London Calling workshops:&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;From single cells to whole genomes: nanopore data analysis - £225.00 + VAT In this one-day practical workshop, attendees will be introduced to Oxford Nanopore’s EPI2ME workflows for single cell transcriptomics, human whole genome genetic variation (SNP, indel, structural variation, methylation, CNVs and STRs) and metagenomics.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Advanced workshop - bioinformatics challenge - £225.00 + VAT An advanced event where attendees will participate in bioinformatics challenges to develop (EPI2ME compatible) workflows for the analysis of Oxford Nanopore sequence data. The event is intended for bioinformaticians and coaching in core technologies will be provided.&lt;/em&gt;&lt;/p&gt;&lt;p&gt; &lt;a href=&quot;https://nanoporetech.swoogo.com/lc24&quot;&gt;https://nanoporetech.swoogo.com/lc24&lt;/a&gt;&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/7d3f9360c53a8e066700003c1bd51fbf/59ccf/mainz-hackathon.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/7d3f9360c53a8e066700003c1bd51fbf/mainz-hackathon.jpeg</content:toenail></item><item><title><![CDATA[Simulating sequencing datasets]]></title><description><![CDATA[How to simulate sequencing datasets for benchmarking analysis performance.]]></description><link>https://labs.epi2me.io/simulating-datasets</link><guid isPermaLink="false">https://labs.epi2me.io/simulating-datasets</guid><pubDate>Mon, 18 Mar 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/676ccc6f1894f739f61cc237322c970e/59ccf/polar_bear.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Bioinformatics tools and pipelines, as with any analysis methods, need to be tested to ensure they work as expected.
This requires benchmarking them to determine their accuracy and to assess the impact of various factors on the results.
Such a task requires data for which the ground truth is known, i.e. knowing what a 100% accurate result should look like.
While there are some situations where users may be able to rely on real sequencing data from robustly characterised mock samples in the lab, simulating datasets on a computer is commonly the only practical and reliable option.&lt;/p&gt;&lt;p&gt;But how do you go about simulating datasets fit for this purpose?
This is a huge field of research, as simulating realistic sequencing data is not trivial.
The goal is to create a set of sequencing reads that reflect not only the biology of an underlying hypothetical sample, but also the patterns and noise associated with real sequencing data.
Such variation results from a variety of sources, including:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;library preparation - preparation introduces several sources of variation. Fragmentation of molecules, errors and biases introduced by PCR amplification, formation of chimeric molecules, and biases introduced through ligation or hybridization of molecules to artificial constructs or probes, all contribute to the complexity of the resulting dataset.&lt;/li&gt;&lt;li&gt;sequencing - the sequencing process itself, including the sequencing platform and basecaller, can introduce both systematic and random error into the recorded base sequences.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Having unrealistic data can prevent you from getting accurate benchmarking results, and so your choice of methods for simulating data needs to be considered carefully.
But don’t let this put you off!
This guide will help you to understand the challenges associated with simulating sequencing datasets, so that you can avoid potential pitfalls and gain reliable benchmarking results.&lt;/p&gt;&lt;p&gt;Numerous genome simulation tools exist to automate many of the steps involved in simulating sequencing datasets, with new ones frequently becoming available and each tailored to different scenarios. &lt;/p&gt;&lt;p&gt;While we won’t delve into the full range of simulation tools here (due to their sheer abundance and variety), we’ll cover the general approaches employed by the different methods and mention a few examples towards the end.&lt;/p&gt;&lt;h2 id=&quot;general-approaches&quot;&gt;General approaches&lt;/h2&gt;&lt;p&gt;The most generic, or at least brute force, approach to simulating sequencing data would be start from known reference sequences and fully replicate biological and experimental variation.
Such a method gives precise knowledge and traceability of all the reads that come out of the simulation.
However this method naturally requires exquisite knowledge of physical processes in the laboratory and the sequencing system, so can be hard to accomplish.&lt;/p&gt;&lt;p&gt;An alternative approach is to manipulate real sequencing reads in a controlled fashion.
This has the benefit of not needing to characterise explicitely the complete experimental setup, but does require having adequate knowledge of precisely what was sequenced in the first instance.&lt;/p&gt;&lt;p&gt;We discuss both methods below.&lt;/p&gt;&lt;h3 id=&quot;simulation-from-reference-sequences&quot;&gt;Simulation from reference sequences&lt;/h3&gt;&lt;p&gt;Simulating sequencing reads starting from a small collection of reference sequences can be described as a “generative approach”.
It aims to create simulated reads by applying a chain of (probabilistic) logical transforms which aim to reflect reality. &lt;/p&gt;&lt;p&gt;The first step is to simulate biological variation that might be present in a sample, that is not present in our collection of reference sequences.
We can introduce mutations in the reference sequences, that are then used in subsequent steps.
For example, we could take FASTA file containing a genome sequence and swap the base at position 100 from a T to a G to simulate a point mutation.
Or we could copy the sequence from position 50-59 and insert it between bases 59 and 60, to simulate a copy number variant at position 50-59 with a copy number of 2.&lt;/p&gt;&lt;p&gt;One of the challenges with modifying a genome sequence is keeping track of where each base corresponds to in the original sequence.
As soon as any insertions or deletions occur in a sequence, the Nth base in the original sequence may not match up with the Nth base in the modified sequence, and so consideration needs to be given into working out where any subsequent variants should be placed.
Various approaches are used to handle this.
One option is to incorporate variants in order from last to first so that the sequence prior to the last incorporated variant position always corresponds to the original sequence.
This works well when the larger insertions don’t overlap other variants, but in situations where they do, more sophisticated methods are required.
Another option is to use a “blueprint” representation of the sequence, that can more easily be manipulated during the simulation.
This blueprint takes the form of a list of sections, each representing either a portion of the original sequence, in which case only a start and end position is recorded, or instead a variant sequence.
The list of sections is sequentially modified to represent each new variant, and once all the variants are incorporated, it can be used as instructions for building the final simulated sequence.
For each section either the relevant portion of the original sequence or, if present, the recorded variant sequence is copied and concatenated together to form the full simulated sequence.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:39.64912280701754%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Blueprint approach&quot; title=&quot;An illustration of the blueprint approach for genome simulation.&quot; src=&quot;/static/c44b95d93e791c0e98246e37737d9333/b5cea/blueprint.png&quot; srcSet=&quot;/static/c44b95d93e791c0e98246e37737d9333/0e2fe/blueprint.png 285w,/static/c44b95d93e791c0e98246e37737d9333/432e7/blueprint.png 570w,/static/c44b95d93e791c0e98246e37737d9333/b5cea/blueprint.png 1140w,/static/c44b95d93e791c0e98246e37737d9333/09ede/blueprint.png 1710w,/static/c44b95d93e791c0e98246e37737d9333/d50e7/blueprint.png 2280w,/static/c44b95d93e791c0e98246e37737d9333/4fe3b/blueprint.png 2901w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;An illustration of the blueprint approach for genome simulation.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;Once we have our modified reference sequence, we next need to generate reads from it.
The aim to mimic real library preparation and the sequencing process by essentially selecting regions from the simulated genome sequences, using specified location and length distributions.
However, achieving this in a realistic manor is a significant challenge.
There are at least two important considerations, elucidated below.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Read coverage&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;
Almost all library preparation and sequencing methods result in at least some degree of non-uniform read coverage along a genome.
Further biases may also manifest in the direction of reads, with a tendency for more reads to originate from either the forward or reverse strand.
In short-read sequencing, GC composition is a strong factor in how efficiently a region is sequenced, with many short read simulators taking this into account.
Further consideration needs to be taken when simulating amplicon, whole-exome, or other targeted sequencing datasets.
While simply limiting the reference sequence sampling to the relevant regions of the genome may be sufficient in some experiments being performed with simulated data, more tailored approaches are needed to reflect the nuances of these datasets.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Incorporating sequencing error&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;
Real-world sequencing data contains various errors; if it didn’t then variant calling would be a lot easier!
These errors need to be incorporated into the simulated reads, with base quality scores that reflect the uncertainty.
At a basic level this might, for example, involve measuring empirical errors and quality distributions and applying these to the “perfect” reads derived from the previous step.
Such models can be made increasingly sophisticated and incorporate models of the surrounding sequence and read position context when simulating quality scores and errors.
For example in short-read sequencing by synthesis, a T nucleotide following GG is more likely to be incorrectly sequenced than a T following AA (&lt;a href=&quot;https://doi.org/10.1093/nargab/lqab019&quot;&gt;Stoler and Nekrutenko 2021&lt;/a&gt;).
Similarly positions closer to the ends of reads from such sequencing methods can also have an increased error likelihood.&lt;/p&gt;&lt;p&gt;Taking the idea of the generative approach to the extreme one might instead physically model the measurement process and apply the same data processing as would ordinarily be applied to real data.
Such an approach was previously used within &lt;a href=&quot;https://github.com/nanoporetech/scrappie&quot;&gt;scrappie&lt;/a&gt;, where a MinION “squiggles” (ionic current traces) could be simulated from a provided nucleotide sequenced, additional noise and variation added, with the resulting signal passed to a basecaller to produce a simulated basecall.&lt;/p&gt;&lt;p&gt;Many such simulation methods allow the user to train their own models on real data in order to account for the specific chemistry, library preparation, and genome context relevant to their study.
Even so, the reads generated by these methods can vary in how realistically they reflect real data. &lt;/p&gt;&lt;p&gt;The complexity in accurately modelling physical processes is the main rationale for using alternative simulation approaches which utilise and modify real sequencing datasets. &lt;/p&gt;&lt;h3 id=&quot;simulation-from-real-data&quot;&gt;Simulation from real-data&lt;/h3&gt;&lt;p&gt;As a somewhat simpler approach, an alternative strategy for simulating read sets is to sample reads from existing experimental data.
This does not require any in-depth specialist knowlege and so can more easily achieve the desired datasets.&lt;/p&gt;&lt;p&gt;The basic idea is to subsample reads from an existing biological sample and merge them together with reads from other samples at required frequencies.
This can generate datasets with a specified coverage, and with variants, species, or lineages at specified frequencies.&lt;/p&gt;&lt;p&gt;The process isn’t without its disadvantages however.
Firstly flexibility is limited.
The simulation is restricted by the coverage, variants, species that are available in existing datasets.&lt;/p&gt;&lt;p&gt;Secondly, the technique requires thorough characterisation of the initial datasets with respect to the type of benchmarking planned.
If you want to test variant calling at different allele frequencies, then you need to know the positions of all true variants in the initial datasets, and if you want to test lineage analysis, then you need to be confident of the lineage composition of the initial datasets.
If this characterisation is not accurate, then any benchmarking performed on samples created from them will be unreliable.&lt;/p&gt;&lt;p&gt;In order to combat the first of these disadvantages, a modified method is to introduce mutations into the library of sequenced reads.
This approach avoids many of the issues associated with artificially generating reads &lt;em&gt;de novo&lt;/em&gt;, though it comes with some difficulties of its own.&lt;/p&gt;&lt;p&gt;The process relies on having accurate initial identification of the reads which are mutated: if we cannot confidently identify the reads then we will be introducing mutations other than those which we thought we were creating.
The identity of reads is typically made through alignment, and introduction of variants is through known genomic co-ordinates.
The mis-indentification (and therefore application of variants) can therefore occur due to ambiguities in the read alignment process.&lt;/p&gt;&lt;p&gt;The ambiguous alignment of reads is a whole topic of conversation in itself; let us discuss briefly consider one such example.
Consider the case where a genome contains non-identical repeats.
When aligning reads to this genome it can be the case that there is an ambiguity as to which repeat the read is derived.
There can be errors and biases in this assignment; particularly if sequencing errors conflate with the differences between the repeats.
If we apply variants to all reads which align to one copy of a repetitive element, we may in fact be applying variants to reads whose true identity is a second copy of the element.&lt;/p&gt;&lt;p&gt;Having introduced variants, this misalignment effect can work the other way round.
The modified reads may be significantly different from the original seed read, and may now more properly align to another locus of the genome entirely; however our synthetic dataset has the read aligned at the original location.
In a real sequencing dataset with the variant present, a read may align to the alternative location: our simulated read set does not now reflect a real set od aligned reads.
If we were to now benchmark a variant caller with the synthetic set we may obtain better results than with real reads (natively containing the variants).
Realigning the synthetic reads following artificial introduction of variants would better model a real experiment, and the resulting inaccuracies to be accounted for in subsequent benchmarking.&lt;/p&gt;&lt;p&gt;Finally, care should be taken when incorporating variants to ensure that other properties of the data are maintained.
For example: base qualities.
Suppose we insert two Gs before an existing pair of Gs.
In real data this would often result in low quality scores for the whole stretch of Gs due to it creating a 4-base homopolymer.
Similarly if a GGT trimer has an A inserted before the T, then in real data this should result in an increased quality score for the T. &lt;/p&gt;&lt;h3 id=&quot;how-to-get-hold-of-well-characterised-real-datasets&quot;&gt;How to get hold of well characterised real datasets&lt;/h3&gt;&lt;p&gt;So those are two different approaches used to simulate sequencing datasets.
You might now be asking, “how do I get hold of some thoroughly characterised real datasets?”&lt;/p&gt;&lt;p&gt;The best way to get a well characterised dataset again depends on your specific use case.
Previously characterised datasets are often available for common scenarios so it’s worth searching for one, or having a look at what’s been used in other simulation studies in your field.
For human data, consortia exist that provide extensively characterised samples, such as the &lt;a href=&quot;https://www.nist.gov/programs-projects/genome-bottle&quot;&gt;Genome in a Bottle Consortium&lt;/a&gt;.
Among other methods, the GIAB consortia employ son/father/mother trios to determine sets of high confidence variant calls.
Other options may be to generate data yourself in such a way that you are confident of the ground truths.
For example, sequencing pure isolates of species for use in simulating datasets for abundance testing.&lt;/p&gt;&lt;h2 id=&quot;which-approach-do-i-use&quot;&gt;Which approach do I use?&lt;/h2&gt;&lt;h3 id=&quot;i-need-to-test-how-my-pipeline-performs-with-lower-coverage&quot;&gt;I need to test how my pipeline performs with lower coverage&lt;/h3&gt;&lt;p&gt;Simple! Use the second approach by subsampling to reduce coverage of an existing dataset, either from FASTQ or BAM files:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;samtools view -bh -s {DOWNSAMPLE_FRACTION} {INPUT BAM} &amp;gt; {SUBSAMPLED_BAM}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Okay, that was a fairly obvious one to start with, but it does get more interesting!&lt;/p&gt;&lt;h3 id=&quot;i-need-to-test-my-metagenomics-abundance-estimation-pipeline&quot;&gt;I need to test my metagenomics abundance estimation pipeline&lt;/h3&gt;&lt;p&gt;The best option for this would likely be the second approach above, providing you have real datasets available for a sufficient variety of species, and importantly, of sufficient purity.
In metagenomic datasets we would hope that the relative abundance of reads should reflect the abundance of species in a sample.
However in reality, factors such as GC content and extraction and library preparation biases result in non-uniform sampling of genomes.
This should therefore be reflected in the simulated datasets.
You may also want to add in some reads from unknown microbes or from human contamination, to add to the authenticity of the simulated samples. &lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:28.421052631578945%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Metagenomics simulation&quot; title=&quot;How to simulate datasets for metagenomics abundance analysis.&quot; src=&quot;/static/f8fc62a5e97d8f09b16fad5ae4badffe/b5cea/metagenomics.png&quot; srcSet=&quot;/static/f8fc62a5e97d8f09b16fad5ae4badffe/0e2fe/metagenomics.png 285w,/static/f8fc62a5e97d8f09b16fad5ae4badffe/432e7/metagenomics.png 570w,/static/f8fc62a5e97d8f09b16fad5ae4badffe/b5cea/metagenomics.png 1140w,/static/f8fc62a5e97d8f09b16fad5ae4badffe/09ede/metagenomics.png 1710w,/static/f8fc62a5e97d8f09b16fad5ae4badffe/d50e7/metagenomics.png 2280w,/static/f8fc62a5e97d8f09b16fad5ae4badffe/836a3/metagenomics.png 2905w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;How to simulate datasets for metagenomics abundance analysis.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;If the above approach is not feasible due to lack of datasets, then another option is to throw in a bit of the first simulation approach! &lt;/p&gt;&lt;h3 id=&quot;i-need-to-test-how-my-pipeline-performs-with-lower-allele-frequency&quot;&gt;I need to test how my pipeline performs with lower allele frequency&lt;/h3&gt;&lt;p&gt;Use the second approach to mix reads from two different samples at various proportions, resulting in variants with altered allele frequencies.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:46.666666666666664%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Low allele frequency simulation&quot; title=&quot;How to simulate datasets to test performance with low allele frequency.&quot; src=&quot;/static/c917a4bf3c23f93e453ea08c45df5521/b5cea/lowAF.png&quot; srcSet=&quot;/static/c917a4bf3c23f93e453ea08c45df5521/0e2fe/lowAF.png 285w,/static/c917a4bf3c23f93e453ea08c45df5521/432e7/lowAF.png 570w,/static/c917a4bf3c23f93e453ea08c45df5521/b5cea/lowAF.png 1140w,/static/c917a4bf3c23f93e453ea08c45df5521/09ede/lowAF.png 1710w,/static/c917a4bf3c23f93e453ea08c45df5521/d50e7/lowAF.png 2280w,/static/c917a4bf3c23f93e453ea08c45df5521/4fe3b/lowAF.png 2901w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;How to simulate datasets to test performance with low allele frequency.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;h3 id=&quot;i-need-to-test-my-short-variant-caller-on-a-specific-set-of-novel-variants&quot;&gt;I need to test my short variant caller on a specific set of novel variants&lt;/h3&gt;&lt;p&gt;This requires simulating new variants through either of the approaches.
As we mentioned before, these each have their own drawbacks so it’s worth considering which suits your needs best before deciding on one.&lt;/p&gt;&lt;p&gt;When using the first approach you can simulate both the reference sequence and novel variants in proportions corresponding to any desired allele frequencies for the study.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:30.87719298245614%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Novel variant simulation&quot; title=&quot;How to simulate datasets to test variant calling on novel variants.&quot; src=&quot;/static/2521265c47a41482a6fcbc5ad49071ec/b5cea/novel.png&quot; srcSet=&quot;/static/2521265c47a41482a6fcbc5ad49071ec/0e2fe/novel.png 285w,/static/2521265c47a41482a6fcbc5ad49071ec/432e7/novel.png 570w,/static/2521265c47a41482a6fcbc5ad49071ec/b5cea/novel.png 1140w,/static/2521265c47a41482a6fcbc5ad49071ec/09ede/novel.png 1710w,/static/2521265c47a41482a6fcbc5ad49071ec/d50e7/novel.png 2280w,/static/2521265c47a41482a6fcbc5ad49071ec/0a61a/novel.png 3822w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;How to simulate datasets to test variant calling on novel variants.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;For the second approach, you’ll need to acquire real sequencing dataset for which all true variant positions are reliably known (such that these true variants don’t conflate the study).
You can then swap bases at the required positions in a portion of reads covering a simulated variant position. &lt;/p&gt;&lt;h3 id=&quot;i-need-to-test-the-performance-of-my-breakpoint-based-copy-number-variant-caller&quot;&gt;I need to test the performance of my breakpoint-based copy number variant caller&lt;/h3&gt;&lt;p&gt;Breakpoints and complex structural rearrangements cannot be modelled easily with the second approach.
It is possible in theory to do so by cutting and joining reads together, but this quickly becomes a headache in practice!
That leaves us with the first approach probably being our best option.
Copy number variants or translocations can be added into a genome sequence and processed to create reads that represent those structural rearrangements.&lt;/p&gt;&lt;h3 id=&quot;i-need-to-test-my-tumour-evolution-analysis-pipeline&quot;&gt;I need to test my tumour evolution analysis pipeline&lt;/h3&gt;&lt;p&gt;We’ve come to our last scenario!
Tumours are constantly evolving, with sub-populations of cells forming that contain distinct sets of genetic mutations.
Tumour evolution analysis covers a range of techniques that can be applied to tumour sequencing datasets, and which aim to characterise the distinct cell populations and uncover the evolutionary relationships between them. &lt;/p&gt;&lt;p&gt;But how to simulate suitable benchmarking datasets for them?&lt;/p&gt;&lt;p&gt;This requires generating datasets that represent a phylogenetic tree of evolving cell populations, all descended from the tumour’s cell of origin.
Simulating such a dataset can be achieved by using either of the approaches, and extending many of the above scenarios in such a way that once one cell population is simulated, subsequent populations are then simulated by acquiring all variants from their parent cell population and incorporating additional new variants. &lt;/p&gt;&lt;p&gt;You’ll probably want to first simulate a germline “cell population”, followed by a cell of origin “cell population”.
After that, divergence of cell populations can be achieved by adding distinct sets of variants to two populations originating from the same parent, and shorter or larger evolutionary distances can be reflected by altering the number of new variants added. &lt;/p&gt;&lt;p&gt;Once the reads for each cell population are created, they can be merged at required frequencies.
Different samples can be created that contain varying proportions of each cell population to reflect different tumour sampling regions or time points through therapy.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:44.56140350877193%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Evolution simulation&quot; title=&quot;How to simulate datasets to test tumour evolution analysis.&quot; src=&quot;/static/ba8a7f04711c4f5c288428c2e6077fe8/b5cea/evolution.png&quot; srcSet=&quot;/static/ba8a7f04711c4f5c288428c2e6077fe8/0e2fe/evolution.png 285w,/static/ba8a7f04711c4f5c288428c2e6077fe8/432e7/evolution.png 570w,/static/ba8a7f04711c4f5c288428c2e6077fe8/b5cea/evolution.png 1140w,/static/ba8a7f04711c4f5c288428c2e6077fe8/09ede/evolution.png 1710w,/static/ba8a7f04711c4f5c288428c2e6077fe8/d50e7/evolution.png 2280w,/static/ba8a7f04711c4f5c288428c2e6077fe8/4fe3b/evolution.png 2901w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;How to simulate datasets to test tumour evolution analysis.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;It is not necessary to merge, or even generate reads, from every stage of the phylogenetic tree.
In tumours, cell populations can die off from being outcompeted by others or as a result of treatments, or they might only be present in certain regions of the tumour.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;A similar approach to simulating tumours might be useful for simulating microbial evolution.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;One final point to mention about this scenario: an important feature of tumour evolution is that the genome can become very messy and complex.
For example, copy number variants can overlap other copy number variants, and short variants can occur on one or multiple copies of a multiplied region depending on whether the short variant occurred in time before or after the copy number variant.
These features can make it even more challenging to analyse tumour evolution.
So if the analysis you’re wanting to benchmark could be affected by this complexity then it’s important to include them in your simulated datasets.&lt;/p&gt;&lt;h2 id=&quot;simulation-tool-examples&quot;&gt;Simulation tool examples&lt;/h2&gt;&lt;p&gt;You’re hopefully now aware of how to simulate sequencing datasets in various different scenarios.
Many more applications will exist and by understanding the considerations needed for the above, you will have an idea of how to decide on the best approach for others.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Icarust - a complete MinKNOW simulator including squiggle and basecall simulation, &lt;a href=&quot;https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btae141/7628125&quot;&gt;Munro et al. 2024&lt;/a&gt;&lt;/li&gt;&lt;li&gt;NanoSIM - Simulates nanopore long-read sequencing data, &lt;a href=&quot;https://doi.org/10.1093/gigascience/gix010&quot;&gt;Yang et al. 2017&lt;/a&gt;&lt;/li&gt;&lt;li&gt;DeepSimulator1.5 - Simulates nanopore long-read sequencing data, via simulating the raw electrical signals &lt;a href=&quot;https://doi.org/10.1093/bioinformatics/btz963&quot;&gt;Wang et al. 2020&lt;/a&gt;&lt;/li&gt;&lt;li&gt;A recent review of other short-read simulators - &lt;a href=&quot;https://doi.org/10.1038/s41437-022-00577-3&quot;&gt;Milhaven and Pfeifer 2023&lt;/a&gt;&lt;/li&gt;&lt;li&gt;CAMISIM - Metagenomics simulator, incorporating various other short and long-read simulators, &lt;a href=&quot;https://doi.org/10.1186/s40168-019-0633-6&quot;&gt;Fritz 2019 et al.&lt;/a&gt;&lt;/li&gt;&lt;li&gt;HeteroGenesis - Simulates genome sequences for complex somatic samples, &lt;a href=&quot;https://doi.org/10.1093/bioinformatics/bty1063&quot;&gt;Tanner et al. 2019&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;what-next&quot;&gt;What next?&lt;/h2&gt;&lt;p&gt;For a guide on how to perform benchmarking once you have your simulated datasets, see our recent blog post on &lt;a href=&quot;https://labs.epi2me.io/benchmarking-performance/&quot;&gt;Benchmarking performance with sensitivity, specificity, precision and recall&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Happy simulating!&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/676ccc6f1894f739f61cc237322c970e/59ccf/polar_bear.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/676ccc6f1894f739f61cc237322c970e/polar_bear.jpeg</content:toenail></item><item><title><![CDATA[Updated Tumor Normal Pair Benchmark Dataset]]></title><description><![CDATA[Sequencing the COLO829/BL tumour/normal pair with Oxford Nanopore's latest Ligation Sequencing Kit]]></description><link>https://labs.epi2me.io/colo-2024.03</link><guid isPermaLink="false">https://labs.epi2me.io/colo-2024.03</guid><pubDate>Thu, 07 Mar 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/ea4cf8152c67fe63f76d56fcf7e917f1/59ccf/gobble-cancer.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;We are pleased to announce an updated data release for the
&lt;a href=&quot;https://www.atcc.org/products/crl-1974&quot;&gt;COLO829&lt;/a&gt;/&lt;a href=&quot;https://www.atcc.org/products/crl-1980&quot;&gt;COLO829BL&lt;/a&gt;
tumour/normal pair.
Sequencing was performed with the 5kHz sampling rate upgrade to the &lt;a href=&quot;https://store.nanoporetech.com/uk/ligation-sequencing-kit-v14.html&quot;&gt;Ligation Sequencing Kit 14&lt;/a&gt;.
The release compliments and supercedes our &lt;a href=&quot;/colo-2023.05&quot;&gt;previous&lt;/a&gt; release.&lt;/p&gt;&lt;p&gt;These reference samples were sequenced with four PromethION flow cells; two flowcells for the cancer sample, two for the normal.
They should provide a valuable resource for cancer researchers.&lt;/p&gt;&lt;h3 id=&quot;data-location&quot;&gt;Data location&lt;/h3&gt;&lt;p&gt;As with previous releases the new dataset is available for anonymous download from
an Amazon Web Services S3 bucket. The bucket is part of the &lt;a href=&quot;https://aws.amazon.com/opendata/&quot;&gt;Open Data on AWS&lt;/a&gt;
project enabling sharing and analysis of a wide range of data.&lt;/p&gt;&lt;p&gt;The data is located in the bucket at:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;s3://ont-open-data/colo829_2024.03/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;See the &lt;a href=&quot;/tutorials/&quot;&gt;tutorials&lt;/a&gt; page for information on downloading the dataset.&lt;/p&gt;&lt;h3 id=&quot;sample-preparation&quot;&gt;Sample preparation&lt;/h3&gt;&lt;p&gt;COLO829 melanoma fibroblasts (ATCC CRL-1974) and COLO829BL normal B lymphoblasts
(ATCC CRL-1980) were cultured for three days in RPMI-1640 medium with 10% fetal
bovine serum and 1% antibiotic-antimycotic incubated at 37°C with 5% CO2. Five
million cells were extracted with the DNeasy Blood and Tissue Kit (Qiagen, Cat.
No. 69504) following the manufacturer’s instructions. Sequencing libraries were prepared
following Oxford Nanopore Ligation Sequencing Kit instructions. Size selection was done
using Blue Pippin and 20 fmols were loaded onto R10.4.1 PromethION flow cells. Sequencing was
performed on a PromethION 24 instrument with the 23.11.5 MinKNOW software.&lt;/p&gt;&lt;h3 id=&quot;sequencing-outputs&quot;&gt;Sequencing Outputs&lt;/h3&gt;&lt;p&gt;Three flowcells were used to sequence the samples to high depth:&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Genome&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;th&gt;Preparation&lt;/th&gt;&lt;th&gt;Flowcell&lt;/th&gt;&lt;th&gt;Yield / Gbase&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;COLO829&lt;/td&gt;&lt;td&gt;Tumour&lt;/td&gt;&lt;td&gt;DNeasy&lt;/td&gt;&lt;td&gt;PAU61426&lt;/td&gt;&lt;td&gt;137&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;DNeasy&lt;/td&gt;&lt;td&gt;PAU59949&lt;/td&gt;&lt;td&gt;136&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;COLO829BL&lt;/td&gt;&lt;td&gt;Normal&lt;/td&gt;&lt;td&gt;DNeasy&lt;/td&gt;&lt;td&gt;PAU59807&lt;/td&gt;&lt;td&gt;167&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;DNeasy&lt;/td&gt;&lt;td&gt;PAU61427&lt;/td&gt;&lt;td&gt;131&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;For each flowcell used in the sequencing the primary sequencer outputs are available
as &lt;code&gt;.pod&lt;/code&gt; files. We also provide sequencing reads in BAM format produced by our
&lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling&quot;&gt;wf-basecalling&lt;/a&gt; workflow. Reads are
aligned to the GRCh38 human reference.&lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;coloPlot&quot; plotCaption=&quot;Sequencing summary metrics for the SUP COLO829/BL cancer/normal pair sequenced with Oxford Nanopore Technologies&amp;#x27; PromethION instrument.&quot;&gt;&lt;/div&gt;&lt;h3 id=&quot;analysis&quot;&gt;Analysis&lt;/h3&gt;&lt;p&gt;As with the previous release the analyses presented here were performed using the latest version of our workflows:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling&quot;&gt;wf-basecalling&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation&quot;&gt;wf-somatic-variation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The somatic variant calling workflow uses &lt;a href=&quot;https://github.com/HKU-BAL/ClairS&quot;&gt;ClairS&lt;/a&gt;
to create calls for the tumour sample by eliminating variants found
also in the paired-normal sample.&lt;/p&gt;&lt;p&gt;The variant calling workflow was run using data from both the COLO829 (tumour) flowcells
and the single COLO829BL (normal). The results of the workflow are present at:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;s3://ont-open-data/colo829_2024.03/analysis/wf_somatic_variation/
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;further-information&quot;&gt;Further information&lt;/h3&gt;&lt;p&gt;For additional information regarding these data please contact &lt;a href=&quot;mailto:support@nanoporetech.com&quot;&gt;support@nanoporetech.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;We hope that these data and analyses provide a useful resource to the community.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/ea4cf8152c67fe63f76d56fcf7e917f1/59ccf/gobble-cancer.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/ea4cf8152c67fe63f76d56fcf7e917f1/gobble-cancer.jpeg</content:toenail></item><item><title><![CDATA[EPI2ME 24.03-01 Release]]></title><description><![CDATA[A gargantuan release of workflow updates and new features.]]></description><link>https://labs.epi2me.io/epi2me-24.03-01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-24.03-01-release</guid><pubDate>Wed, 06 Mar 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/c888f333de52ecb0c35d367deca80f5d/59ccf/cheekier-monkey.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Dear Nanopore Community,&lt;/p&gt;&lt;p&gt;We are delighted to release a mammoth collection of updates and improvements to our EPI2ME workflows.
And a new dataset release to boot!&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-bacterial-genomes&quot;&gt;wf-bacterial genomes&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-bacterial-genomes/blob/master/CHANGELOG.md&quot; title=&quot;https://github.com/epi2me-labs/wf-bacterial-genomes/blob/master/CHANGELOG.md&quot;&gt;v1.2.0&lt;/a&gt;] &lt;ul&gt;&lt;li&gt;&lt;strong&gt;NEW&lt;/strong&gt;! &lt;em&gt;Salmonella&lt;/em&gt; serotyping is automatically performed when &lt;em&gt;Salmonella&lt;/em&gt; is detected with MLST analysis. This functionality is provided by the integration of &lt;a href=&quot;https://github.com/denglab/SeqSero2&quot; title=&quot;https://github.com/denglab/SeqSero2&quot;&gt;SeqSero2&lt;/a&gt; into the workflow.&lt;/li&gt;&lt;li&gt;Read length filtering of reads &amp;lt; 1kb is now performed by default. This can be adjusted using &lt;code&gt;--min_read_length&lt;/code&gt;.&lt;/li&gt;&lt;li&gt;A new &lt;code&gt;--client_fields&lt;/code&gt; parameter can be specified to add extra info to the report.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation&quot; title=&quot;https://github.com/epi2me-labs/wf-human-variation&quot;&gt;wf-human-variation&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation/blob/master/CHANGELOG.md&quot; title=&quot;https://github.com/epi2me-labs/wf-human-variation/blob/master/CHANGELOG.md&quot;&gt;v2.0.0&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;&lt;strong&gt;NEW&lt;/strong&gt;! Version 2.0.0 introduces Spectre (&lt;a href=&quot;https://github.com/fritzsedlazeck/Spectre&quot; title=&quot;https://github.com/fritzsedlazeck/Spectre&quot;&gt;https://github.com/fritzsedlazeck/Spectre&lt;/a&gt;) as the default CNV caller. Users can still use QDNASeq with the newly added “use_qdnaseq” option.&lt;/li&gt;&lt;li&gt;This update introduces performance improvements when CRAM is supplied as an input, and fixes a bug that prevented tandem repeat BED files from being selected in the EPI2ME Desktop Application.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation&quot; title=&quot;https://github.com/epi2me-labs/wf-clone-validation&quot;&gt;wf-clone-validation&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation/blob/master/CHANGELOG.md&quot; title=&quot;https://github.com/epi2me-labs/wf-clone-validation/blob/master/CHANGELOG.md&quot;&gt;v1.2.0&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;&lt;strong&gt;NEW&lt;/strong&gt;! Option to change the assembly tool to Canu with &lt;code&gt;--assembly_tool&lt;/code&gt;, flye still remains the default assembler.&lt;/li&gt;&lt;li&gt;The &lt;code&gt;--client_fields&lt;/code&gt; parameter can be specified to add extra info to the report.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell&quot; title=&quot;https://github.com/epi2me-labs/wf-single-cell&quot;&gt;wf-single-cell&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell/blob/master/CHANGELOG.md&quot; title=&quot;https://github.com/epi2me-labs/wf-single-cell/blob/master/CHANGELOG.md&quot;&gt;v1.1.0&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;Adapters and barcodes are now trimmed from reads before their alignment to the reference genome.&lt;/li&gt;&lt;li&gt;Added an option &lt;code&gt;--full_length_only&lt;/code&gt; (default true), to process only read segments with two flanking compatible adapters.&lt;/li&gt;&lt;li&gt;Fixed an issue where minimap2 was searching for splice sites on the incorrect strand for 3prime/multiome data.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes/&quot; title=&quot;https://github.com/epi2me-labs/wf-transcriptomes/&quot;&gt;wf-transcriptomes&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes/blob/master/CHANGELOG.md&quot; title=&quot;https://github.com/epi2me-labs/wf-transcriptomes/blob/master/CHANGELOG.md&quot;&gt;v1.1.1&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;Improved handling of different annotation file types (eg. &lt;code&gt;.gtf/.gff/.gff3&lt;/code&gt;) in &lt;code&gt;de_analysis&lt;/code&gt; mode and bug fix to accommodate 10 or more samples in &lt;code&gt;de_analysis&lt;/code&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-artic&quot; title=&quot;https://github.com/epi2me-labs/wf-artic&quot;&gt;wf-artic&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-artic/blob/master/CHANGELOG.md&quot; title=&quot;https://github.com/epi2me-labs/wf-artic/blob/master/CHANGELOG.md&quot;&gt;v1.1.0&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;To facilitate the identification of the latest clades and lineages of SARS-CoV-2 we have updated Nextclade to V3.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot; title=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot;&gt;wf-metagenomics&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics/releases&quot; title=&quot;https://github.com/epi2me-labs/wf-metagenomics/releases&quot;&gt;v2.9.1&lt;/a&gt;] / &lt;a href=&quot;https://github.com/epi2me-labs/wf-16s&quot; title=&quot;https://github.com/epi2me-labs/wf-16s&quot;&gt;wf-16s&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-16s/releases&quot; title=&quot;https://github.com/epi2me-labs/wf-16s/releases&quot;&gt;v1.1.1&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;Update to accommodate long argument list lengths when using glob patterns.&lt;/li&gt;&lt;li&gt;Fix to address a Kraken2 pipeline issue where incorrect numbers for unclassified counts were reported.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation&quot; title=&quot;https://github.com/epi2me-labs/wf-somatic-variation&quot;&gt;wf-somatic-variation&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation/releases/v1.1.0&quot; title=&quot;https://github.com/epi2me-labs/wf-somatic-variation/releases/v1.1.0&quot;&gt;v1.1.0&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;Updated ClairS to v0.1.7.&lt;/li&gt;&lt;li&gt;Support for the latest ClairS Dorado models.&lt;/li&gt;&lt;li&gt;Performance improvements that should appreciably reduce the run time of the workflow.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling/&quot; title=&quot;https://github.com/epi2me-labs/wf-basecalling/&quot;&gt;wf-basecalling&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling/blob/master/CHANGELOG.md&quot; title=&quot;https://github.com/epi2me-labs/wf-basecalling/blob/master/CHANGELOG.md&quot;&gt;v1.1.7&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;Bug fixed where incremental reports were not produced during live basecalling with the watchPath functionality.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Our collection of bioinformatics howto articles has also been growing. If you are unfamiliar with the &lt;a href=&quot;https://labs.epi2me.io/category/articles/&quot; title=&quot;https://labs.epi2me.io/category/articles/&quot;&gt;EPI2ME blogposts&lt;/a&gt;, recent articles include helpful guides on topics that include&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The usage of IGV to review and explore BAM files - such BAM files are a fundamental output from several workflows that include e.g. wf-alignment, wf-human-variation and wf-metagenomics (when run using the minimap2-based classifier).    &lt;/li&gt;&lt;li&gt;A quickstart guide on how to benchmark accuracy when assessing genetic variation. This article introduces some of the terms that are used and stresses the importance of appropriate truthsets.&lt;/li&gt;&lt;li&gt;How to for VCF format files that describe genetic variation. This article helps demystify this file format and helps with wrangling specific data facets.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Finally we have a updated data release for the COLO829/COL0829BL tumour/normal pair of cell lines. This dataset improves the single-molecule read accuracy and read length characteristics of our previous dataset and uses the new 5kHz sampling frequency in line with current default sequencing device setup. Download links and instructions are available on the &lt;a href=&quot;/colo829-2024.03&quot;&gt;data release page&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;We welcome any feedback and would love to hear your recommendations for future workflows, new features or additional functionality.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/c888f333de52ecb0c35d367deca80f5d/59ccf/cheekier-monkey.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/c888f333de52ecb0c35d367deca80f5d/cheekier-monkey.jpeg</content:toenail></item><item><title><![CDATA[A guide to reviewing BAM files]]></title><description><![CDATA[Reviewing BAM files]]></description><link>https://labs.epi2me.io/reviewing-bam</link><guid isPermaLink="false">https://labs.epi2me.io/reviewing-bam</guid><pubDate>Mon, 04 Mar 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/c426bec37ac8d73d6afd9dd078bd16dc/59ccf/review-bam-thumbnail.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;In the world of sequencing data analysis, automation reigns supreme.
Standardised workflows, such as those &lt;a href=&quot;https://labs.epi2me.io/wfindex/&quot;&gt;published&lt;/a&gt; by the EPI2ME team, efficiently transform raw sequence reads into variant calls and other outputs, a process which is vital for research endevours as well as clinical applications.
However, there usually comes a point in many projects where manual review becomes necessary to gain deeper insights.
For example in the clinical assessment of sequence variants, particularly in human genomics when evaluating pathogenicity, discerning between authentic variants and those which are artifacts is imperative for accurate prioritisation.
This means that when reviewing the results of an analysis workflow, understanding variant calls in the context of corresponding BAM alignments is an important process.
In this blog post, we will provide a quick refresher on the file formats involved, discuss some of the different types of variants which can be viewed in the Integrative Genomics Viewer, or IGV, and explore how visualisation can enhance our understanding of genomic variation. &lt;/p&gt;&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;&lt;p&gt;In previous posts we have discussed &lt;a href=&quot;https://labs.epi2me.io/how-to-align/&quot;&gt;generating alignments&lt;/a&gt; and recently, querying and manipulating &lt;a href=&quot;https://labs.epi2me.io/querying-vcf-files/&quot;&gt;VCF files&lt;/a&gt;.
As a reminder, here is a quick refresher on some of the relevant file formats involved:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The &lt;code&gt;SAM&lt;/code&gt; (Sequence Alignment/Map) format is a text-based format for storing alignment data, commonly generated by tools such as &lt;a href=&quot;https://github.com/lh3/minimap2&quot;&gt;minimap2&lt;/a&gt; and &lt;a href=&quot;https://github.com/lh3/bwa&quot;&gt;BWA&lt;/a&gt;. It includes details about the alignment of sequencing reads to a reference genome, mapping qualities, and alignment positions. SAM files can be large and can’t be indexed, making them inefficient for large-scale analyses.&lt;/li&gt;&lt;li&gt;&lt;code&gt;BAM&lt;/code&gt; (Binary Alignment/Map) is the binary version of the SAM format. BAM files store the same alignment information as SAM, but in a compressed binary format, resulting in smaller file sizes and faster data access. Additionally, BAM files can be indexed, giving efficient random access to specific regions. (Occassionally unaligned reads are also stored in BAM, in which case the file is referred to as a &lt;code&gt;uBAM&lt;/code&gt;, &lt;code&gt;u&lt;/code&gt; for unaligned).&lt;/li&gt;&lt;li&gt;&lt;code&gt;CRAM&lt;/code&gt; (Compressed Alignment Map) is another alignment format designed to address the storage and access issues of SAM and BAM. Like BAM, CRAM files are highly compressed and support indexing for fast random access. CRAM achieves higher compression ratios than BAM mainly through using reference-based compressions. This makes it especially useful for storing large-scale sequencing data. However it also means the a reference genome file is required for decompression and interpretation, which may not always be available (or convenient to carry around).&lt;/li&gt;&lt;li&gt;&lt;code&gt;VCF&lt;/code&gt; (Variant Call Format) is a standard text file format used to represent the genetic variation called in a BAM or CRAM file.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In addition, there are other alignment formats such as &lt;code&gt;PAF&lt;/code&gt; (Pairwise mApping Format) and &lt;code&gt;MAF&lt;/code&gt; (Multiple Alignment Format); BAM files remain the most commonly used in genomics research.
BAM files are not human-friendly due to their binary format, but fortunately there are a number of tools that enable us to extract insights from these files.
Among them is the popular Integrative Genomics Viewer, &lt;a href=&quot;https://www.igv.org&quot;&gt;IGV&lt;/a&gt;, which is the software that will be used to navigate alignments in this post.
Other well-known genome browsers are available such as &lt;a href=&quot;https://genome.ucsc.edu&quot;&gt;UCSC&lt;/a&gt;, &lt;a href=&quot;http://www.ensembl.org/&quot;&gt;Ensembl&lt;/a&gt;, and &lt;a href=&quot;https://jbrowse.org/jb2/&quot;&gt;Jbrowse&lt;/a&gt;, each with their own advantages.&lt;/p&gt;&lt;p&gt;You may be wondering why we need to view BAM files in this way, when after all, the VCF contains a list of all the variants detected, and important metadata about them.
The answer lies in the nuance of variant assessment: while VCFs offer a condensed summary of variants, BAM files hold a wealth of contextual information crucial for distinguishing true variants from artifacts.
By loading BAM files into IGV, we can gain access to a dynamic workspace where variants can be viewed in their genomic context along with useful metadata and additional annotation tracks.
This visual representation enchances our understanding, revealing details which may not be obvious from a cursory examination of the VCF alone.&lt;/p&gt;&lt;h2 id=&quot;exploring-an-alignment-in-igv&quot;&gt;Exploring an alignment in IGV&lt;/h2&gt;&lt;p&gt;IGV can be run in a browser or is available to &lt;a href=&quot;https://igv.org&quot;&gt;download&lt;/a&gt; as a desktop application for a range of platforms.
Before you can explore an alignment, it is first necessary to load the correct reference genome.
The default reference genome loaded is human genome build &lt;code&gt;hg38&lt;/code&gt;, however it is possible to load alternative versions of the human genome, or indeed the reference of an entirely different organism, by selecting the appropriate option from the &lt;code&gt;Genomes&lt;/code&gt; drop-down menu.
You can select a pre-loaded genome, or upload your own reference FASTA file - handy if you work with less commonly studied organisms such as the mighty Axolotl.
Once the appropriate genome is selected, the next step is to load the alignment.&lt;/p&gt;&lt;p&gt;Key points to remember before you load a BAM file:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The reads in the BAM are aligned to a specific reference genome. If you load the BAM file with a different reference genome selected in IGV, the alignment positions of the reads may not match the co-ordinates of the new genome, which could result in misinterpretation, misaligned visualisations, and missing features, and may cause the application to crash.&lt;/li&gt;&lt;li&gt;The BAM file should be sorted and indexed before being loaded into IGV. Please refer to our &lt;a href=&quot;https://labs.epi2me.io/how-to-align/#how-to-post-process-the-alignment&quot;&gt;previous post&lt;/a&gt; on alignment for more information on how to do this. &lt;ul&gt;&lt;li&gt;While IGV can read an unsorted BAM, it may not display the reads in the correct order, potentially affecting visualisation and analysis.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Once you are satisfied that your BAM is ready, you can load it into IGV using the &lt;code&gt;File&lt;/code&gt; drop-down menu.
As well as BAM files, A number of other file types can be loaded into IGV.
Among the most important file formats supported, there are:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;VCF format, particularly useful if you are trying to understand more about specific variants in your results.&lt;/li&gt;&lt;li&gt;BED format and its variants, useful to see values in genomic ranges e.g. methylation rates in bedMethyl format, and signal intensity data from experiments such as ChIP-seq, in bedGraph format.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Let’s start with a really simple example.
Here we’ve loaded the &lt;a href=&quot;https://ont-exd-int-s3-euwst1-epi2me-labs.s3.amazonaws.com/wf-human-variation/wf-human-variation-demo.tar.gz&quot;&gt;demo reference FASTA and BAM&lt;/a&gt; for the &lt;code&gt;snp&lt;/code&gt; sub-workflow of wf-human-variation, along with with the VCF generated by the workflow:&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:52.28070175438596%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABiklEQVR42p1Sa0vDQBDM//9Jgogf1EJM7nKtGiGvWigtyeXyaNo8CuPutVaxfrIw7HYnOzu3d07TNBiGAdM4oqlrtG1rUVNeX/7v0DQGVWVszrWqqiy/27U4HDrkeQ5jDBytNTabzUWEBzCYZNR1Qw0tlNoiywp0XWv5smTxGuu1IU7TgIJqJRxu4ml931PjwUYGu2ZwPo49St2TENf78zcD1QcS7VEUvXXJhhyeNtJxj8ejxX9/rGEF2R2j6zqsViskSYI0TX8gQbb8QLwQSN5fkWbLb26ZIQkjJN6bzQ2t4SK43+8RhiFc14Xv+/A874xnCF/ifnaLR/eBOHHhfOFh9uTj7kZCSM/u1WGbLMbHjeMYUkpasrrCQs0xJ/yuz+cKLwuJIFCnW/5yOE0ToiiCEILI4Ao8SAbyTy4Qp7oV5EvhW7MOo5gEySFNYwTnaMF1Sc3qiwssH9AQxSbU2eHprdGx6VJKo2kPBFOcYlkgp6iprinnaCgySl2i0PRWKVb0/op8i8oYfAL+AuNX06DMgAAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Viewing a simple alignment&quot; title=&quot;Simple alignment generated by snp demo data from wf-human-variation&quot; src=&quot;/static/ae2689b93e53d7795bdc2c1207c8a77b/b5cea/simple_bam.png&quot; srcSet=&quot;/static/ae2689b93e53d7795bdc2c1207c8a77b/0e2fe/simple_bam.png 285w,/static/ae2689b93e53d7795bdc2c1207c8a77b/432e7/simple_bam.png 570w,/static/ae2689b93e53d7795bdc2c1207c8a77b/b5cea/simple_bam.png 1140w,/static/ae2689b93e53d7795bdc2c1207c8a77b/c1b63/simple_bam.png 1200w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Simple alignment generated by snp demo data from wf-human-variation&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;At the top of the screen there are various ways to access positions in the selected reference, either by selecting a chromosome, entering some co-ordinates, and also zooming in and out with the slider at the far right.
At the bottom of the screen is the reference sequence, and in this example there are two positions which are different from the reference: both heterozygous variants, highlighted in the red boxes.
The first is an &lt;code&gt;A&lt;/code&gt; to &lt;code&gt;T&lt;/code&gt; substitution, and the second is a &lt;code&gt;C&lt;/code&gt; to &lt;code&gt;T&lt;/code&gt; substitution, and both have been recorded as variants in the VCF, as the blue boxes above each one denote.
Clicking on the coloured boxes on the &lt;code&gt;chr6_chr20.bam Coverage&lt;/code&gt; track will bring up information about the coverage at that position, and the ratio of observed to reference alleles.
It is also possible to view information about individual reads by clicking on them; this will bring up a host of metadata such as read name, length, and mapping quality.
Examples of both of these features can be seen in the image below:&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:62.456140350877185%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Viewing a simple alignment in detail&quot; title=&quot;Viewing more detail in the alignment. The left hand box is the result of clicking on the coverage track and shows the total depth and number of reads containing each base, at a particular position. The right hand box is the result of clicking on an individual read, and displays metadata such as read name, length, and mapping quality.&quot; src=&quot;/static/8f3acb698cdc49f7f8cea81b0cbf9e02/b5cea/simple_bam_info.png&quot; srcSet=&quot;/static/8f3acb698cdc49f7f8cea81b0cbf9e02/0e2fe/simple_bam_info.png 285w,/static/8f3acb698cdc49f7f8cea81b0cbf9e02/432e7/simple_bam_info.png 570w,/static/8f3acb698cdc49f7f8cea81b0cbf9e02/b5cea/simple_bam_info.png 1140w,/static/8f3acb698cdc49f7f8cea81b0cbf9e02/09ede/simple_bam_info.png 1710w,/static/8f3acb698cdc49f7f8cea81b0cbf9e02/d50e7/simple_bam_info.png 2280w,/static/8f3acb698cdc49f7f8cea81b0cbf9e02/948e7/simple_bam_info.png 2339w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Viewing more detail in the alignment. The left hand box is the result of clicking on the coverage track and shows the total depth and number of reads containing each base, at a particular position. The right hand box is the result of clicking on an individual read, and displays metadata such as read name, length, and mapping quality.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;h2 id=&quot;mapq-scores&quot;&gt;MAPQ scores&lt;/h2&gt;&lt;p&gt;One metric that is helpful when assessing whether an alignment supports a variant call made is the mapping quality, or &lt;code&gt;MAPQ&lt;/code&gt; score.
This is a score assigned to each read alignment in the BAM.
It represents the confidence or reliability of the alignment of a read to a specific location in the reference genome, with scores ranging from 0 to 255, where higher values indicate a greater confidence in the alignment.
In the toy example given above, the MAPQ scores for the reads supporting the two variants displayed range from 60, all the way down to 1.
If this were a real dataset, it may be worth performing some filtering on the data as the range of relatively low MAPQ scores indicates that there is very little confidence in the alignment of some reads, which could indicate that the variant calls made are also not reliable.&lt;/p&gt;&lt;h2 id=&quot;visualising-complex-variation&quot;&gt;Visualising complex variation&lt;/h2&gt;&lt;p&gt;The data generated by sequencing experiments, and especially long-read sequencing from Oxford Nanopore, enables us to discover a bigger range of variant type than ever before.
For example split reads, which are sequence reads which do not align continuously to the reference genome but instead partially align to different regions, indicate potential support for candidate structural variations, or other complex genomic events.
Visualising structural variants such as insertions, deletions, inversions, or translocations, allows us to directly observe the breakpoints, or boundaries, of structural variants.
With long reads we can identify events several thousands of kilobases in size, and so visualising them can be helpful to determine whether they are real or artifacts.
Consider this 6kb deletion a human reference sample from Coriell, &lt;a href=&quot;https://catalog.coriell.org/0/Sections/Search/Sample_Detail.aspx?Ref=NA02533&amp;amp;Product=DNA&quot;&gt;NA02533&lt;/a&gt;, called by &lt;a href=&quot;https://github.com/fritzsedlazeck/Sniffles&quot;&gt;Sniffles2&lt;/a&gt;, which is a structural variant caller that is included within &lt;code&gt;wf-human-variation&lt;/code&gt;.
By selecting the option to sort alignments by read length, we can clearly see where this large deletion event, highlighted in the red box, has disrupted the normal genomic sequence, resulting in some reads aligning partially to the deleted region, and partially to the flanking sequences on either side of it.
In this example, we have also loaded a VCF of annotations from &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/clinvar/&quot;&gt;ClinVar&lt;/a&gt;, which is a database of human clinical variation, and are rendered as blue bars in the track above the alignment.
This allows us to see that this particular structural variant is recorded in ClinVar, and gives us access to further information about the variant.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:69.47368421052632%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Viewing a large deletion&quot; title=&quot;Viewing a large deletion in NA02533&quot; src=&quot;/static/8af236e12393de82897c550e6fa8a755/b5cea/big_sv.png&quot; srcSet=&quot;/static/8af236e12393de82897c550e6fa8a755/0e2fe/big_sv.png 285w,/static/8af236e12393de82897c550e6fa8a755/432e7/big_sv.png 570w,/static/8af236e12393de82897c550e6fa8a755/b5cea/big_sv.png 1140w,/static/8af236e12393de82897c550e6fa8a755/09ede/big_sv.png 1710w,/static/8af236e12393de82897c550e6fa8a755/d50e7/big_sv.png 2280w,/static/8af236e12393de82897c550e6fa8a755/948e7/big_sv.png 2339w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Viewing a large deletion in NA02533&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;IGV can also be used to colour and group bases to identify other patterns which can be useful for interpretation.
Haplotypes, which are combinations of alleles or variants located on a single chromosome, offer valuable insights into genetic inheritance.
Through the process of phasing, we can ascertain which variants co-occur on the same chromosome, thereby unravelling the parental origin of genetic material within an individual’s genome.
Variant phasing involves determining precisely whether specific genetic variants, such as single nucleotide variants (SNVs) or insertions/deletions (indels), reside on the chromosome inherited from one parent or the other.
Haplotagged reads, such as these generated within &lt;code&gt;wf-human-variation&lt;/code&gt; by software such as &lt;a href=&quot;https://whatshap.readthedocs.io/&quot;&gt;whatshap&lt;/a&gt; and &lt;a href=&quot;https://github.com/twolinin/longphase&quot;&gt;longphase&lt;/a&gt;, assign tags that link the reads or contigs to the haplotypes they to which they belong.
Some of our workflows use software which performs this functionality, and the resulting BAM file can give us greater insight when evaluating certain variants.
We can use IGV to visualise this, as in the example below:&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:63.859649122807014%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Viewing a short tandem repeat&quot; title=&quot;Viewing an STR in NA07063&quot; src=&quot;/static/b2db925e3195dc0b3da7b043279dec3e/b5cea/str_haplotag.png&quot; srcSet=&quot;/static/b2db925e3195dc0b3da7b043279dec3e/0e2fe/str_haplotag.png 285w,/static/b2db925e3195dc0b3da7b043279dec3e/432e7/str_haplotag.png 570w,/static/b2db925e3195dc0b3da7b043279dec3e/b5cea/str_haplotag.png 1140w,/static/b2db925e3195dc0b3da7b043279dec3e/09ede/str_haplotag.png 1710w,/static/b2db925e3195dc0b3da7b043279dec3e/d50e7/str_haplotag.png 2280w,/static/b2db925e3195dc0b3da7b043279dec3e/948e7/str_haplotag.png 2339w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Viewing an STR in NA07063&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;To visualise the haplotypes, we have coloured and sorted each read based on the value of the &lt;code&gt;HP&lt;/code&gt; tag.
This helps us to clearly see the difference in the two alleles: while both contain insertions, the size of the insertion in one allele is much greater than the other, highlighted by the two black boxes.
The grey reads indicate that no haplotype could be assigned.
In fact this is a human Coriell reference sample, &lt;a href=&quot;https://www.coriell.org/0/sections/Search/Sample_Detail.aspx?Ref=NA07063&amp;amp;product=DNA&quot;&gt;NA07063&lt;/a&gt;, that harbours a short tandem repeat expansion in the &lt;em&gt;FMR1&lt;/em&gt; gene, and by organising the reads in this way, it is possible to view the inserted sequences in the context of their haplotypes, giving us more context about the variant.&lt;/p&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;In this blog post we have looked at the file formats relating to alignment, and how visualising alignments in IGV can be used to help assess whether or not a variant is real, which is a crucial part of the investigation process.
We have seen some examples of visualising some more complex variants and changed some basic settings in IGV to produce a more informative visualisation, and we have also loaded other types of data to provide additional annotations to help understand the variants in the BAM file.
As a reminder, all EPI2ME workflows produce BAM files which are aligned and sorted, and therefore compatible with IGV.
By delving further into BAM files using platforms such as IGV, it’s possible to customise an analysis and add annotation layers, and tailor the analysis to answer specific research questions.&lt;/p&gt;&lt;h2 id=&quot;further-information&quot;&gt;Further information&lt;/h2&gt;&lt;p&gt;For more information on using the IGV desktop application, there is extensive &lt;a href=&quot;https://igv.org/doc/desktop/&quot;&gt;documentation&lt;/a&gt; available, along with some excellent &lt;a href=&quot;https://igv.org/doc/desktop/#TutorialVideos/&quot;&gt;video tutorials&lt;/a&gt;.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/c426bec37ac8d73d6afd9dd078bd16dc/59ccf/review-bam-thumbnail.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/c426bec37ac8d73d6afd9dd078bd16dc/review-bam-thumbnail.jpeg</content:toenail></item><item><title><![CDATA[Singularity for bioinformatics]]></title><description><![CDATA[An introduction to containerised bioinformatics applications with Singularity.]]></description><link>https://labs.epi2me.io/singularity</link><guid isPermaLink="false">https://labs.epi2me.io/singularity</guid><pubDate>Mon, 26 Feb 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/11e9dc1621a534cffdc50b69ba10cd26/59ccf/space_-_mars_and_sun.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Have you ever tried downloading a new shiny software tool from GitHub, keen to run your test dataset through it, only to find that the application fails with a missing library error, or even worse it produces different results to those you were expecting?
Well most applications require a whole host of other software libraries in order to function.
Each of these libraries can exist as different versions that may or may not produce different results.&lt;/p&gt;&lt;p&gt;One increasingly common solution to this issue is to bundle-up your software into a &lt;em&gt;container&lt;/em&gt;.
This blog post introduces the use of containers for distributing bioinformatics applications, focusing on the &lt;a href=&quot;https://apptainer.org/&quot;&gt;Singularity&lt;/a&gt; container engine which, as will be demonstrated, is very well suited to bioinformatics applications. &lt;/p&gt;&lt;p&gt;This post serves as a guide to Linux users developing bioinformatics application that are unfamiliar with using containers, or would like an introduction to Singularity.
We will discuss also how and why we use containers in EPI2ME workflows. &lt;/p&gt;&lt;h2 id=&quot;containers-for-reproducible-research&quot;&gt;Containers for reproducible research&lt;/h2&gt;&lt;p&gt;For reproducible research we need a way to ensure all the software dependencies of an application are the same no matter where or when the software is run.
One way to do this is to post your laptop by DHL to your collaborators who can then run the same analyses as you.
If this seems a bit cumbersome, we should consider some alternatives. &lt;/p&gt;&lt;p&gt;Virtual machines (VMs) are a type of software that runs on your computer and fully emulates a second computer.
So you could have a linux VM running on a Windows host machine with complete isolation, and it can be identical to a copy of the same VM running elsewhere.
However, VMs can consume a lot of the host system’s resources (even if they don’t actually use it), and the files used to define and share VMs are large. &lt;/p&gt;&lt;p&gt;An alternative approach is to use a &lt;a href=&quot;https://en.wikipedia.org/wiki/Containerization_(computing)&quot;&gt;container&lt;/a&gt; to bundle-up your application along with all the dependencies it needs
to run reproducibly.
Containers, unlike VMs, run as a normal process on the host machine.
A container contains all the programs, system settings and other dependencies that are needed to reproducibly run your application.
But they directly use the resources of the underlying operating system and so use much fewer resources and are easier to distribute than VMs.
Containerised apps have now largely superseded the use of VMs in the distribution of bionformatics applications.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;A note on terminology:&lt;br/&gt;
&lt;strong&gt;images&lt;/strong&gt;: layers of data, which record the state of a computer’s file system.&lt;br/&gt;
&lt;strong&gt;container&lt;/strong&gt;: a running instance of an image.  &lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id=&quot;docker-vs-singularityapptainer&quot;&gt;Docker vs Singularity/Apptainer&lt;/h2&gt;&lt;p&gt;In the sea of container engine players, two stand out as particularly popular in the bioinformatics community:
&lt;a href=&quot;https://www.docker.com&quot;&gt;Docker&lt;/a&gt; and &lt;a href=&quot;https://apptainer.org/&quot;&gt;Singularity&lt;/a&gt; (or equivalently Apptainer, see &lt;a href=&quot;https://en.wikipedia.org/wiki/Singularity_(software)&quot;&gt;History&lt;/a&gt;).
Docker has been around longer and was originally designed with the deployment of web applications in mind.
The company behind docker host a distribution platform called &lt;a href=&quot;https://hub.docker.com/&quot;&gt;dockerhub&lt;/a&gt; that enables the storing and sharing of docker images. &lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Singularity? Apptainer? What’s the difference? The answers to this questions is long. If you are interested read more in &lt;a href=&quot;https://apptainer.org/news/community-announcement-20211130/&quot;&gt;this post&lt;/a&gt;.
We will use singularity and apptainer interchangeably in this post.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Docker is often run with administrator access as it has many powerful functions that need escalated privileges.
On multi-user high performance computing systems, giving every Tom, Dick, and Harry admin permissions isn’t such a great idea.
That’s where Singularity comes in. When a Singularity container executes it runs as the same user within the container, and has identical permissions. This model side-steps many of the security issues associated with Docker.
Singularity can seamlessly run docker container images, including directly from dockerhub, as well as being able to use its own container format. &lt;/p&gt;&lt;h2 id=&quot;getting-started-with-singularity-well-actually-apptainer&quot;&gt;Getting started with Singularity (well actually apptainer)&lt;/h2&gt;&lt;p&gt;Singularity is geared for running on Linux systems and this is where you most likely encounter it in the wild.
Installation instructions differ for different operating systems.
To see detailed instructions including for different Linux flavours, and for Mac see &lt;a href=&quot;https://apptainer.org/docs/admin/main/installation.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h3 id=&quot;using-publicly-available-images-with-singularity&quot;&gt;Using publicly available images with Singularity&lt;/h3&gt;&lt;p&gt;As we noted above, Singularity is able to use transparently the Docker image format. There are a multitude Docker images to be found on &lt;a href=&quot;https://hub.docker.com/&quot;&gt;Docker Hub&lt;/a&gt; with a good chance that your favourite applications have already been packaged into an image and are available there.
The &lt;a href=&quot;https://biocontainers.pro/&quot;&gt;biocontainers&lt;/a&gt; project is a community-led project to create bioinformatics containers backed largely by bioconda packages.
Their &lt;a href=&quot;https://biocontainers.pro/registry&quot;&gt;registry&lt;/a&gt; hosts many useful container images in the native singularity format.&lt;/p&gt;&lt;p&gt;To show how we can use a container from Docker Hub, we’ll take a look at the EPI2ME &lt;a href=&quot;https://hub.docker.com/r/ontresearch/wf-alignment/&quot;&gt;wf-alignment&lt;/a&gt; workflow image as an example by running the following command.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-commandline&quot;&gt;singularity shell docker://ontresearch/wf-alignment:shaa9faef16822c5aa48366a4c45b401c9233a6c0f7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will instantiate a wf-alignment container and drop into a shell within it. Let’s break down this command. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;singularity&lt;/code&gt; - the main command&lt;/li&gt;&lt;li&gt;&lt;code&gt;shell&lt;/code&gt; - this subcommand tells Singularity that we want to run a shell within a container&lt;/li&gt;&lt;li&gt;&lt;code&gt;docker://&lt;/code&gt; - tells Singularity to get images from Docker Hub. To use images from Singularity Library, prefix the path with &lt;code&gt;library://&lt;/code&gt; instead&lt;/li&gt;&lt;li&gt;&lt;code&gt;ontresearch/&lt;/code&gt; - is Oxford Nanopore Technologies’ Docker Hub namespace&lt;/li&gt;&lt;li&gt;&lt;code&gt;wf-alignment&lt;/code&gt; - specifies the project&lt;/li&gt;&lt;li&gt;&lt;code&gt;:shaa9faef16822c5aa48366a4c45b401c9233a6c0f7&lt;/code&gt; - everything after the &lt;code&gt;:&lt;/code&gt; is the tag. This is the version of the container.
In EPI2ME workflows, this tag is what associates an image version with a workflow version and is specified in the workflow config (see this &lt;a href=&quot;https://github.com/epi2me-labs/wf-alignment/blob/v1.1.1/nextflow.config#L42&quot;&gt;example&lt;/a&gt;).&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Running the command above drops us into a shell in the container. We can run applications that are installed in the container, for example &lt;code&gt;seqkit&lt;/code&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-commandline&quot;&gt;Singularity&amp;gt; seqkit version
seqkit v2.6.
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;accessing-host-directories&quot;&gt;Accessing host directories&lt;/h3&gt;&lt;p&gt;When a container is running in Singularity, each process will have a work directory on the host system, which you’ll likely be reading data from and writing results to.
For security purposes not all the locations on the host system are available from within the container (see &lt;a href=&quot;https://apptainer.org/docs/user/latest/bind_paths_and_mounts.html&quot;&gt;here&lt;/a&gt;), although there are some default locations that are mounted, which include: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;$HOME&lt;/code&gt; mounted within the container at  &lt;code&gt;$HOME&lt;/code&gt; &lt;/li&gt;&lt;li&gt;&lt;code&gt;/tmp&lt;/code&gt; mounted within the container at &lt;code&gt;/tmp&lt;/code&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If you want other host directories to be available in your container, for example a network share folder, use Singularity’s &lt;code&gt;--bind&lt;/code&gt; option.
The following command opens a shell in a container with the host folder &lt;code&gt;/mnt/share&lt;/code&gt; available in the container at &lt;code&gt;/data&lt;/code&gt;.
Note that the &lt;code&gt;bind&lt;/code&gt; option must come before the image path. &lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-commandline&quot;&gt;singularity shell --bind /data/share:/data docker://ontresearch/wf-alignment:shaa9faef16822c5aa48366a4c45b401c9233a6c0f7
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;creating-singularity-images&quot;&gt;Creating Singularity images&lt;/h3&gt;&lt;p&gt;To create a Singularity image, we first need to create a definition file.
There is a good tutorial available &lt;a href=&quot;https://singularity-tutorial.github.io/03-building/&quot;&gt;here&lt;/a&gt;, but we’ll just create a very simple definition file that defines a container that has a single application; &lt;code&gt;samtools&lt;/code&gt; (let’s call it samtools.def)&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Bootstrap: docker
From: ubuntu:22.04

%post
    apt -y update
    apt install samtools
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first line here tells Singularity that we will be using an image from Docker Hub as our starting point.
The second line specifies that our starting image will be Ubuntu version 22.04.
Changes that we specify later in the file, will be applied to this Ubuntu image.&lt;/p&gt;&lt;p&gt;The commands in the &lt;code&gt;%post&lt;/code&gt; section are run, and the results are added on to the base Ubuntu image to create your new Ubuntu + samtools image.&lt;/p&gt;&lt;p&gt;To go ahead and build the image, run the following:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;singularity build samtools.sif samtools.def
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can fire up our newly-minted image into a running container using the following command (remembering to bind our data folder directory):&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-commandline&quot;&gt;singularity shell --bind /home/me/data:/data samtools.sif
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And run a command in the running container to view a BAM file.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-commandline&quot;&gt;Singularity&amp;gt; samtools view /data/chr19.bam | less
d42c2f04-3ad0-47a7-9d85-c2adf95f3ec1_0  16  chr19   58498337    60  80S35M2I121M1I198M93N18M1D111M207N104M1D4M1D15M30S  *   0   0   TCCTACGACGCT.....
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;containers-and-nextflow&quot;&gt;Containers and Nextflow&lt;/h2&gt;&lt;p&gt;Nextflow is a domain-specific language that is used to create bioinformatics workflows; it’s used to power all our &lt;a href=&quot;https://labs.epi2me.io/wfindex/&quot;&gt;EPI2ME workflows&lt;/a&gt;.
A Nextflow workflow consists of individual processes that run custom scripts and commands. Each of these processes is able to run these commands in its own container separate from all other processes and from the host system.&lt;/p&gt;&lt;p&gt;Nextflow natively supports both Singularity and Docker (as do our workflows) as well as some lesser-known container engines.
See the nextflow &lt;a href=&quot;https://www.nextflow.io/docs/latest/container.html&quot;&gt;documentation&lt;/a&gt; for more information on these.&lt;/p&gt;&lt;p&gt;If your workflow is fairly simple with few dependencies, you might want to use the same container for each process.
To do that, use the &lt;code&gt;-with-singularity&lt;/code&gt; option e.g:  &lt;code&gt;nextflow run &amp;lt;your script&amp;gt; -with-singularity [singularity image file]&lt;/code&gt;.
Using this, each process will run using the specified container image for each process and run all the process commands in it.
If you don’t want to add this to the command line every time, it can be supplied in the Nextflow config like so:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;process.container = &amp;#x27;/path/to/singularity.sif&amp;#x27;
singularity.enabled = true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is great if your workflow utilizes a single container, but if your workflow is complex or if you have a common
container that is shared across workflows, you may want to apply specific containers to each process.
This is what we do in many EPI2ME workflows; there will be at least one workflow-specific container and another common container, which supplies functionality shared across all workflows.&lt;/p&gt;&lt;p&gt;Nextflow allows each process to be assigned its own container using either the process name or the process label.
Using the process label as below, it’s possible to apply the same container to multiple processes with the same label, simplifying the configuration.
The container can be a locally stored image or exist on the internet.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;process {
  withLabel:process1 {
    # Path to a local image
    container = &amp;quot;path/to/container1.sif&amp;quot;
  }
  withLabel:process2 {
    # A Docker Hub image
    container = &amp;quot;docker://ontresearch/wf-common&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;the-use-of-containers-with-epi2me-workflows&quot;&gt;The use of containers with EPI2ME workflows&lt;/h3&gt;&lt;p&gt;EPI2ME workflows are only supported when run with our supplied images.
Each of our workflows are associated with containers, that are mostly built by our internal continuous integration (CI) systems.
&lt;a href=&quot;https://docs.conda.io/projects/conda/en/stable/&quot;&gt;Conda&lt;/a&gt; packages for each container are defined in &lt;a href=&quot;https://docs.docker.com/reference/dockerfile/&quot;&gt;Dockerfiles&lt;/a&gt;, with some of the conda packages created by our team. The process is quite similar to that adopted by the biocontainers project though we lean more toward building containers containing multiple tools.
Our internal CI system then builds a Docker image from this file and deposits it to Docker Hub with a unique tag.
This tagged image is then associated with one or more versions of the workflow. &lt;/p&gt;&lt;p&gt;To use Singularity when running an EPI2ME workflow, it’s as easy as supplying the profile option in your command &lt;/p&gt;&lt;pre&gt;&lt;code&gt;nextflow run epi2me-labs/wf-basecalling -profile singularity
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Using this command, the following profile defined in the workflow config is used. &lt;/p&gt;&lt;pre&gt;&lt;code&gt;profiles {
  singularity {
    enabled = true
    autoMounts = true
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This profile enables the use of Singularity.
It also sets &lt;code&gt;autoMounts&lt;/code&gt; to true, which instructs Singularity to mount automatically the host paths that are used in the workflow.&lt;/p&gt;&lt;h2 id=&quot;common-issues&quot;&gt;Common issues&lt;/h2&gt;&lt;p&gt;Common issues encountered when using Singularity are often due to host paths not being available within the container.
The following describes two such issues and how to workaround them.&lt;/p&gt;&lt;h3 id=&quot;relative-paths&quot;&gt;Relative paths&lt;/h3&gt;&lt;p&gt;Sometimes the use of relative paths can result in problems when using nextflow with Singularity.
Therefore, it’s advisable to use absolute paths in the command.
So do this: &lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;nextflow run /home/git/wf-basecalling —input /home/data/fastq&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;rather than this:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;nextflow run ~/wf-basecalling —input ~/data/fastq&lt;/p&gt;&lt;/blockquote&gt;&lt;h3 id=&quot;temporary-directories-becoming-full&quot;&gt;Temporary directories becoming full&lt;/h3&gt;&lt;p&gt;Singularity needs a place to store temporary files during the building of containers.
By default, this is set to &lt;code&gt;/tmp&lt;/code&gt;, but this can result in &lt;code&gt;/tmp&lt;/code&gt; getting full
especially on shared computing systems, and you might encounter a message such as &lt;/p&gt;&lt;pre&gt;&lt;code&gt;FATAL:   While making image from oci registry .... short write: write /tmp/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To prevent this, set the Singularity temp directory by placing the following in
your &lt;code&gt;~/.bashrc&lt;/code&gt; file. &lt;/p&gt;&lt;pre&gt;&lt;code&gt;export SINGULARITY_TMPDIR=/path/to/you_tmp_dir
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;This short guide was an introduction to containers and, in particular, Singularity (Apptainer) and why and how it can be used for bioinformatics applications.
We briefly touched on what containers are, how to create and run Singularity images and containers and highlighted a couple of common issues you may encounter while using Singularity.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/11e9dc1621a534cffdc50b69ba10cd26/59ccf/space_-_mars_and_sun.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/11e9dc1621a534cffdc50b69ba10cd26/space_-_mars_and_sun.jpeg</content:toenail></item><item><title><![CDATA[Benchmarking performance with sensitivity, specificity, precision and recall]]></title><description><![CDATA[Benchmarking performance]]></description><link>https://labs.epi2me.io/benchmarking-performance</link><guid isPermaLink="false">https://labs.epi2me.io/benchmarking-performance</guid><pubDate>Mon, 19 Feb 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/c726a5a55e1561da478b200471025bb8/59ccf/cultured.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;div&gt;&lt;/div&gt;&lt;p&gt;You just finished reading the paper of the latest and greatest new bioinformatic tool.
It promises to solve all your problems (maybe not all of them) and how it left every alternative tool available biting the dust.
So you rush to start replacing your dull old tool with this fancy new one.
But maybe, just maybe, you’re getting ahead of yourself and should probably test it out first.&lt;/p&gt;&lt;p&gt;It’s happened to all of us, the rush to stay on top with the newest model or fastest piece of software.
The paper will have likely posted some benchmarking statistics of their own, which is the first step for us to feel confident it may work for us.
But every use case is different, so it’s important to assess the performance for your own particular needs.&lt;/p&gt;&lt;p&gt;Benchmarking is an important process in software tool development, enabling us to quantify performance and help optimise parameters to eke out the best possible results from our tools.
There are many performance metrics which we have at our disposal to assess our tools.
Four popular metrics are &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;sensitivity and specificity&lt;/a&gt; as well as their their closely related friends &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;&gt;precision and recall&lt;/a&gt;.
Often used in many papers, they can quickly give readers vital information on the performance of a test or tool, but they can sometimes be misunderstood if used in the wrong scenario.
Although not shown in any of our workflow outputs, these metrics have been extensively used during their development process.
In this blog post we hope to help clarify what they are showing and what to consider when benchmarking tools using one or the other.&lt;/p&gt;&lt;h2 id=&quot;definitions&quot;&gt;Definitions&lt;/h2&gt;&lt;p&gt;Many times, a tool will yield a result in a sample we feel unsure about, so how can we be confident it is correct?
Before we run a multitude of samples with mysterious and unknown findings to be discovered, we want to test our tool against a dataset where we know the expected result.
These expected results from an understood case are referred to as a “truth set” or “ground truth”.
Using the results of our tool on this dataset and comparing them to known results, we can create a confusion matrix of all the possible outcomes (in the world of confusion matrices, this one is actually rather simple).&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:718px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:90.52631578947368%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Confusion matrix&quot; title=&quot;Figure 1. A confusion matrix comparing expected and predicted test results.&quot; src=&quot;/static/321fcc027cca80f4cdbb9140baf90960/57dc1/confusion_matrix.png&quot; srcSet=&quot;/static/321fcc027cca80f4cdbb9140baf90960/0e2fe/confusion_matrix.png 285w,/static/321fcc027cca80f4cdbb9140baf90960/432e7/confusion_matrix.png 570w,/static/321fcc027cca80f4cdbb9140baf90960/57dc1/confusion_matrix.png 718w&quot; sizes=&quot;(max-width: 718px) 100vw, 718px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 1. A confusion matrix comparing expected and predicted test results.&lt;/figcaption&gt;
  &lt;/figure&gt;
Simply put:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;True positive (TP) - Both the tool and the truth produced a positive result.&lt;/li&gt;&lt;li&gt;True negative (TN) - Both the tool and the truth produced a negative result.&lt;/li&gt;&lt;li&gt;False positive (FP) - The tool produced a positive results whilst the truth produced a negative result.&lt;/li&gt;&lt;li&gt;False negative (FN) - The tool produced a negative result whilst the truth produced a positive result.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;With the values within this confusion matrix, we can make the following definitions:&lt;/p&gt;&lt;p&gt;$$ Sensitivity = \frac{TP}{TP + FN} $$
$$ Specificity = \frac{TN}{TN + FP} $$
$$ Precision = \frac{TP}{TP + FP} $$
$$ Recall = \frac{TP}{TP + FN} $$&lt;/p&gt;&lt;p&gt;These can be simplified to:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Sensitivity - Out of all the positive outcomes in the truth set, how many got a positive test result?&lt;/li&gt;&lt;li&gt;Specificity - Out of all the negative outcomes in the truth set, how many got a negative test result?&lt;/li&gt;&lt;li&gt;Precision - Out of all the positive test results, how many were positive in the truth set?&lt;/li&gt;&lt;li&gt;Recall - Out of all the positive outcomes in the truth set, how many got a positive test result?&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;From these definitions, we can see sensitivity and recall are the same thing with a different name!
Many other popular metrics are aliases or easily derived from these definitions, such as positive predictive value (precision), true positive rate (sensitivity or recall), true negative rate (specificity) and false positive rate (1 - specificity).
In all such cases the four basic statistics are the rates of true and false, negatives and positives.&lt;/p&gt;&lt;h2 id=&quot;which-one-to-choose&quot;&gt;Which one to choose?&lt;/h2&gt;&lt;p&gt;Both sensitivity and specificity, and precision and recall aim to do similar things.
Depending on the type of data you have one may be more informative than the other.&lt;/p&gt;&lt;p&gt;The combination of sensitivity and specificity means that every state within the confusion matrix is considered.
This is useful in fields such as medical diagnostics, as medical professionals need to know a negative result just as much as a positive result, so including metrics which measures the rate of true positives (sensitivity) and the rate of true negatives (specificity) makes sense.&lt;/p&gt;&lt;p&gt;In other examples as we shall see below, recall and precision will be preferred as the go to metrics to understand the performance of an analysis tool&lt;/p&gt;&lt;h3 id=&quot;when-to-use-sensitivity-and-specificity&quot;&gt;When to use sensitivity and specificity&lt;/h3&gt;&lt;p&gt;These quantities are most useful when the rates of true positives and negatives are balanced, or when the analysis should consider equally the classes.&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Truth set Positive&lt;/th&gt;&lt;th&gt;Truth set Negative&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;100&lt;/td&gt;&lt;td&gt;100&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Imagine we have a truth set with the above number of classifications.
It has an even split of positive and negative results.
We run the data through our tool and get the following results:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TP - 86&lt;/li&gt;&lt;li&gt;FN - 14&lt;/li&gt;&lt;li&gt;FP - 20&lt;/li&gt;&lt;li&gt;TN - 80&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;With this truth set, we calculate a sensitivity of 0.86 and specificity of 0.8 - not too bad.
In other words, out of 100 actual positives, we correctly classified 86 (and 80 out of 100 actual negatives).&lt;/p&gt;&lt;h3 id=&quot;when-precision-and-recall-become-useful&quot;&gt;When precision and recall become useful&lt;/h3&gt;&lt;p&gt;Above we considered a balanced dataset, one with equal numbers of true positive and negative results.
But what if our truth set had a large imbalance in classifications? For example:&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Truth set Positive&lt;/th&gt;&lt;th&gt;Truth set Negative&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;100&lt;/td&gt;&lt;td&gt;1000&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This truth set has a much larger proportion of negative results than positive ones.
For example, this could happen when screening a population for a rare disease.
We run the data through our tool, compare against the known results and get the following:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TP - 86&lt;/li&gt;&lt;li&gt;FN - 14&lt;/li&gt;&lt;li&gt;FP - 200&lt;/li&gt;&lt;li&gt;TN - 800&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In this analysis we still get a sensitivity of 0.86 and specificity of 0.8, but intuitively we can see that the results are not as good as the first.
Our tool produced 286 positive calls, but 200 of these are wrong!
By using sensitivity and specificity as our performance metrics we’ve obscured this glaring problem.&lt;/p&gt;&lt;p&gt;In this application, we don’t really care so much about the negative results, but instead want to know about the positive calls and how much we can trust them.
This is where the precision-recall approach provides more insightful information: it focuses on the positive calls.
You may have noticed from the above definitions, but neither precision nor recall use true negative in their calculations.&lt;/p&gt;&lt;p&gt;What happens if we calculate the precision and recall for the imbalanced truth set?
Recall and sensitivity are the same thing so that stays at 0.86, but the precision is calculated at 0.301. The poor performance of the tool has now appeared.
If a variant caller identified 286 variants in a 1100bp amplicon, but 200 of them were false alarms, you wouldn’t go near it.&lt;/p&gt;&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;&lt;p&gt;In reality you are likely to use both sensitivity and specificity and precision and recall at various different stages of development.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Imbalanced binary classes are the classic case where precision and recall are used over sensitivity and specificity.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Depending on the situation and what facets of the data are most important, one pair of metrics will be more useful than the other.
For many bioinformatics applications our datasets will be imbalanced, even more than in this example.&lt;/p&gt;&lt;p&gt;A very common example of this is that of variant calling across a genome.
The number of variant sites in a genome is typically very low compared to the total size of a genome.
There are therefore many many more true negative sites compared to true positives.&lt;/p&gt;&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;&lt;p&gt;Let’s imagine we are developing a metagenomics assay targeted at patients with inflammation in the lungs.
We want to determine when this may be caused by an infection.
We aim for the tool to correctly identify the presence (positive) or absence (negative) of a pathogen in a patient sample every time. &lt;/p&gt;&lt;p&gt;Our test can be split into 2 simple steps:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Classify the taxa/organisms within the sample.&lt;/li&gt;&lt;li&gt;Determine if there is a causative pathogen within the sample.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;So the first thing we need to do is correctly classify the organisms in the sample through taxonomic classification.
Taxonomic classifiers can have huge databases of information to search across and many taxa won’t be classified or present in the sample.
These unclassified taxa (negatives) would take an overwhelming proportion of the data, so it will be imbalanced.
Additionally we only really care about what is called by our classifier, so precision-recall seem like the right metrics to use.&lt;/p&gt;&lt;p&gt;Now that we have determined the microbial composition of the sample, we can determine if there is a causative pathogen present.
The classifications can be well defined: either there is a pathogen present in the patient’s sample, or there is not.
Our truth set here could be several patients with lung tissue inflammation, some of which are caused by pathogenic infection.
The truth set should be fairly well balanced as pathogenic infection is a common cause of lung inflammation, but not the only cause.
Information on the rate of true negative calls is also important, so sensitivity-specificity seems the appropriate metrics to use.&lt;/p&gt;&lt;p&gt;So in the development of this assay, we can use both sensitivity-specificity and precision-recall in assessing the performance.&lt;/p&gt;&lt;h2 id=&quot;optimisation&quot;&gt;Optimisation&lt;/h2&gt;&lt;p&gt;As well as being useful for benchmarking the performance of the tool, both metrics can also be used to optimise parameters of an analysis.
Many tools will output some form of probability, likelihood, or classification score; not a binary classification directly.
In order to then transform these numbers to a binary classification we can, for examples, set a threshold cut-off value which determines the classification in a binary fashion.
So-called receiver operating characteristic (ROC) curves and precision-recall curves allow us to visualise the effect of changing such threshold values.
They additionally can provide a general idea on the goodness of the model’s fit.&lt;/p&gt;&lt;p&gt;For our metagenomics assay, we may implement an abundance threshold in which pathogens under this are not considered, as they are deemed too low to be the likely causative pathogen.
An ROC curve plots true positive rate (sensitivity) against false positive rate (1-specificity) for many different threshold values.
By changing the value of the threshold we can trace out a curve showing the effect the parameter.&lt;/p&gt;&lt;p&gt;Similarly within the taxonomic classification step we may expect a small subset of our data to be incorrectly classified.
We may therefore want to apply a cut-off to exclude lowly abundant taxa from any downstream analysis.
By testing a variety of different cut-offs, we can visualise their effects on performance with a precision-recall curve.&lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;curves&quot; plotCaption=&quot;Figure 2. left: Receiver operating characteristic (ROC) curve, right: precision-recall curve.&quot;&gt;&lt;/div&gt;&lt;p&gt;It is typical to observe a trade-off between sensitivity and specificity, or between precision and recall.
This occurs naturally from the fact that algorithms are not perfect.
Take a thought experiment where we randomly flip some proportion of the classifications produced by an algorithm from positive to negative, and vice-versa.
The only situation in which both sensitivity and specificity will both increase is if all of these changes were from an being an incorrect classification to a correct one.
(Such a situation can only occur if the algorithm is 100% incorrect - it systematically switches positive and negative classifications).
It is not a stretch to see that in other situations such random perturbations can lead only to an improvement of one metric at the expense of the other.
So in this sense sensitivity and specificity (and precision and recall) are anti-correlated.&lt;/p&gt;&lt;p&gt;Optimising for both sensitivity-specificity or precision-recall can therefore be tricky.
Two derived metrics that can help in this optimisation are &lt;a href=&quot;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&quot;&gt;Youden’s J&lt;/a&gt; (sensitivity-specificity) and &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;&gt;F1-score&lt;/a&gt; (precision-recall).
These can be calculated as follows:&lt;/p&gt;&lt;p&gt;$$ Youden’s\ J = sensitivity + specificity - 1 $$
$$ F1-score = \frac{2\ x\ precision\ x\ recall}{precision + recall} $$&lt;/p&gt;&lt;p&gt;Youden’s J values range from 0 to 1, where 1 indicates a perfect prediction model and 0 indicates an even proportion of false negative and false positive calls.
Negative values can occur but indicate that the class labels have been switched.
The calculation assumes equal weighting between false positive and false negative calls, or that they are both equally important.
The F1-score value ranges from 0-1 (0-100%) and aims to maximise precision and recall.
It uses the harmonic mean of precision and recall.
The &lt;code&gt;1&lt;/code&gt; in F1 indicates that precision and recall are given equal weight in this mean.&lt;/p&gt;&lt;p&gt;In our example metagenomics assay, for the overall test it can be argued that it’s more important to prioritise sensitivity over specificity.
A false negative call results in a patient with an infection not receiving antibiotic treatment, which could have severe consequences.
But a false positive call results in a patient with no infection receiving antibiotics, which is not as bad (if we disregard antimicrobial stewardship at least).
The extreme end of this would be to prioritise sensitivity completely, by give everyone a positive call (i.e every patient receives antibiotics), thus eliminating any false negatives.
This approach may be beneficial to patients in the short term, but there could be long term consequences such as an increase in prevalance of antimicrobal resistant pathogens.
Careful consideration should be taken into priortising one metric over the other and the potential risks should be outlined clearly.&lt;/p&gt;&lt;h2 id=&quot;truth-sets&quot;&gt;Truth sets&lt;/h2&gt;&lt;p&gt;Regardless of the exact performance metric you decide to use, the need for a truth set when benchmarking an algorithm is paramount.
The truth set can be from various sources such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The current gold-standard test for whatever your assay targets, such as 16S rRNA for microbial identification or culture-based techniques.&lt;/li&gt;&lt;li&gt;A reference set with known results, such as microbial standards with a defined set of organisms in a sample.&lt;/li&gt;&lt;li&gt;Simulated datasets, which can be particularly helpful when there are no other appropriate sources.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Each of the above have their pros and cons, and perhaps more than one will need to be used.
By using the gold standard as the truth set the benchmarking will show exactly how well your tool performs over the industry standard.
But it may have known limitations such as the difficulty of delineating species with a high homology in the 16S rRNA, or the biases of growing colonies in culture.
These limitations may create misleading benchmarking results such as a false positive or a false negative call with a high level of uncertainty.
Reference sets such as microbial standards are good because there is little ambiguity in the truth set classification.
But they may not encapsulate the challenges the test will face such as identifying species within the same genera.
Simulated datasets allow you to tailor your test to the exact challenges you may come up against and you can be sure of the classification.
They may not however represent the quality or composition of data you expect to encounter.
The choice can be tricky: choosing one over another can result in misleading or poor benchmarking results.&lt;/p&gt;&lt;p&gt;For clinical applications, various regions provide programs that offer datasets for validation and benchmarking
These aim to standardise procedures across multiple laboratories.
A notable example is &lt;a href=&quot;https://ukneqas.org.uk/&quot;&gt;NEQAS&lt;/a&gt;: a UK-based organisation that welcomes participation not only from private and commercial laboratories but also from international entities, while also extending support to NHS genomics facilities.&lt;/p&gt;&lt;p&gt;Some commonly used truth sets we use in our work are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.nist.gov/programs-projects/genome-bottle&quot;&gt;Genome in a bottle&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.nist.gov/programs-projects/genome-bottle&quot;&gt;Zymo microbial community standards&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;summary-1&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;Hopefully we have been able to shed some light on benchmarking metrics and our fictiously example metagenomic test has shown you where and when to use them.
As a quick round-up:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Sensitivity and specificity, and precision and recall are useful tools to benchmark performance.&lt;/li&gt;&lt;li&gt;Sensitivity and specificity should be used on balanced datasets, or where a negative classification is important.&lt;/li&gt;&lt;li&gt;Precision and recall are particularly useful on unbalanced datasets, for example in the detection of rare events. &lt;/li&gt;&lt;li&gt;During the optimisation of sensitivity and specificity (and precision-recall), these are typically anti-correlated.&lt;/li&gt;&lt;li&gt;Between applications the relative importance of the quantities may vary.&lt;/li&gt;&lt;li&gt;Your choice of truth set is almost as important as your test and will influence your benchmarking results.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Happy benchmarking!&lt;/p&gt;&lt;h2 id=&quot;further-information&quot;&gt;Further information&lt;/h2&gt;&lt;p&gt;We had variant calling as an example, and luckily there’s a tool available for benchmarking already. Check out &lt;a href=&quot;https://github.com/Illumina/hap.py&quot;&gt;hap.py&lt;/a&gt; and &lt;a href=&quot;https://github.com/RealTimeGenomics/rtg-tools&quot;&gt;rtg-tools&lt;/a&gt; for comparing variant calls against a truth set. &lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/c726a5a55e1561da478b200471025bb8/59ccf/cultured.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/c726a5a55e1561da478b200471025bb8/cultured.jpeg</content:toenail></item><item><title><![CDATA[Querying a VCF file]]></title><description><![CDATA[A hitchhiker's guide to VCF querying.]]></description><link>https://labs.epi2me.io/querying-vcf-files</link><guid isPermaLink="false">https://labs.epi2me.io/querying-vcf-files</guid><pubDate>Mon, 12 Feb 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/860ffd3335dca00c8ea96887832bc607/59ccf/drain-the-swamp.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Anyone working with genomic data from next or third generation sequencing studies is bound to work with VCF files.
No, I’m not talking of the VCard format, but of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Variant_Call_Format&quot;&gt;Variant Call Format&lt;/a&gt;!
Bad jokes aside, VCF files are currently the standard for storing variation data, and come with a number of &lt;a href=&quot;https://samtools.github.io/hts-specs/VCFv4.4.pdf&quot;&gt;specifications&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;VCF files are very common in genomic studies as they are the standard output of variant callers, such as &lt;a href=&quot;https://samtools.github.io/bcftools/bcftools.html#mpileup&quot;&gt;bcftools mpileup&lt;/a&gt;, &lt;a href=&quot;https://github.com/HKU-BAL/Clair3&quot;&gt;Clair3&lt;/a&gt; and &lt;a href=&quot;https://github.com/fritzsedlazeck/Sniffles&quot;&gt;Sniffles2&lt;/a&gt;.
They are also the primary outputs of several EPI2ME workflows, including &lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation&quot;&gt;wf-human-variation&lt;/a&gt;, &lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation&quot;&gt;wf-somatic-variation&lt;/a&gt; and &lt;a href=&quot;https://github.com/epi2me-labs/wf-amplicon&quot;&gt;wf-amplicon&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Depending on the variant caller and the variant type, your VCF will report different pieces of information for each variant site.
You might want to extract these values from your file in order to perform different types of operations and postprocessing manually.
For instance, you might want to count how many structural variants (SVs) of each type your favourite SV caller has found; this information is normally encoded with the &lt;code&gt;SVTYPE&lt;/code&gt; tag.
Or again, you might want to extract the depth of sequencing at each site, a value commonly reported in the &lt;code&gt;DP&lt;/code&gt; field, in order to help you visualize their distribution in a plot.&lt;/p&gt;&lt;p&gt;VCF files have become very popular thanks to their flexible structure, which lets developers encode many different metrics and values for each variant called.
However, this can make VCFs very verbose and complex to query for individual fields or values.
This article will guide you through the first steps of applying filters and queries to a VCF file, enabling you to extract those pieces of information you are really interested in.
So, without further ado, let’s have a look at how to query a VCF file!&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;This post is a companion to our previous IPython notebook &lt;a href=&quot;https://labs.epi2me.io/notebooks/Introduction_to_Variant_Call_Format_(vcf)_files.html&quot;&gt;Introduction to Variant Call Format files&lt;/a&gt;.
Readers unfamiliar with the basics of VCF files may wish to read through the notebook before reading this post.&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id=&quot;the-toolkit-and-bcftools&quot;&gt;The toolkit and bcftools&lt;/h2&gt;&lt;p&gt;Thanks to its popularity, the VCF file has an impressive array of software and libraries developed to achieve the most diverse set of tasks.
Some popular software solutions for working with VCF files are:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://vcftools.github.io/&quot;&gt;vcftools&lt;/a&gt; - the eponymous (but likely not best these days) tool.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://pcingola.github.io/SnpEff/#snpsift&quot;&gt;SnpSift&lt;/a&gt; - from the authors of the &lt;a href=&quot;https://pcingola.github.io/SnpEff/snpeff/introduction/&quot;&gt;SnpEff&lt;/a&gt; tool.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.htslib.org/doc/bcftools.html&quot;&gt;bcftools&lt;/a&gt; - part of the &lt;a href=&quot;https://www.htslib.org/download/&quot;&gt;htslib&lt;/a&gt; family of tools.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The evergreen &lt;code&gt;vcftools&lt;/code&gt; comes with a lot of analysis options and functionalities, but it can be a little slow when it comes to querying and filtering large files.
&lt;code&gt;SnpSift&lt;/code&gt; boasts impressive capabilities, but as it is geared towards the outputs of its sibling annotation tool &lt;a href=&quot;https://pcingola.github.io/SnpEff/&quot;&gt;snpEff&lt;/a&gt;, this might limit applicability for general filtering tasks.
In many cases, &lt;code&gt;bcftools&lt;/code&gt; is the best tool for the job and this is why we are going to feature it in the tutorial below.&lt;/p&gt;&lt;p&gt;One more thing: while all these tools are command-line interface (CLI) based, their developers did a great job and simplifying their usage as much as possible.
So don’t be discouraged by the black screen and give them a go!&lt;/p&gt;&lt;p&gt;&lt;code&gt;bcftools&lt;/code&gt; is a set of utilities for variant calling and manipulating VCFs and BCFs (the more efficient, binary version of VCF).
It is a valuable member of the bioinformatician’s toolkit, since it combines high speed with diverse features and great flexibility in complex situations.
Among the many available, some tools of note are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;bcftools view&lt;/code&gt; - view, subset, filter and convert VCF or BCF files.&lt;/li&gt;&lt;li&gt;&lt;code&gt;bcftools filter&lt;/code&gt; - apply filters to files.&lt;/li&gt;&lt;li&gt;&lt;code&gt;bcftools norm&lt;/code&gt; - normalize sites, split multiallelic sites, check alleles against the reference, and left-align indels.&lt;/li&gt;&lt;li&gt;&lt;code&gt;bcftools annotate&lt;/code&gt; - add or remove annotations to/from the INFO field.&lt;/li&gt;&lt;li&gt;&lt;code&gt;bcftools query&lt;/code&gt; - Query fields and write the output in a user-defined format.
The built-in functionalities of &lt;code&gt;bcftools&lt;/code&gt; are further expanded through the use of &lt;a href=&quot;https://samtools.github.io/bcftools/howtos/plugins.html&quot;&gt;plugins&lt;/a&gt;.
Now, going through all its goodies would make for a very long blog post, so let’s just focus on two very common &lt;code&gt;bcftools&lt;/code&gt; commands: &lt;code&gt;view&lt;/code&gt; and &lt;code&gt;query&lt;/code&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;visualize-and-filter-a-vcf-file&quot;&gt;Visualize and filter a VCF file&lt;/h2&gt;&lt;p&gt;For this blog post, we are using a small toy VCF, that you can easly download from the website from &lt;a href=&quot;/d4d939924781def7e274b49028227c36/myfile.vcf&quot;&gt;here&lt;/a&gt;.
Once you have downloaded the file, you will be able to reproduce every command in this post and its outputs by simply copying and pasting them in you command line.&lt;/p&gt;&lt;p&gt;If you want to have a quick look at your VCF file you can simply use &lt;code&gt;bcftools view&lt;/code&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bcftools view myfile.vcf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will print the contents of the VCF file:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;##fileformat=VCFv4.3
##fileDate=20090805
##source=myImputationProgramV3.1
##reference=file:///seq/references/1000GenomesPilot-NCBI36.fasta
##contig=&amp;lt;ID=20,length=62435964,assembly=B36,md5=f126cdf8a6e0c7f379d618ff66beb2da,species=&amp;quot;Homo sapiens&amp;quot;,taxonomy=x&amp;gt;
##phasing=partial
##INFO=&amp;lt;ID=NS,Number=1,Type=Integer,Description=&amp;quot;Number of Samples With Data&amp;quot;&amp;gt;
##INFO=&amp;lt;ID=DP,Number=1,Type=Integer,Description=&amp;quot;Total Depth&amp;quot;&amp;gt;
##INFO=&amp;lt;ID=AF,Number=A,Type=Float,Description=&amp;quot;Allele Frequency&amp;quot;&amp;gt;
##INFO=&amp;lt;ID=AA,Number=1,Type=String,Description=&amp;quot;Ancestral Allele&amp;quot;&amp;gt;
##INFO=&amp;lt;ID=DB,Number=0,Type=Flag,Description=&amp;quot;dbSNP membership, build 129&amp;quot;&amp;gt;
##INFO=&amp;lt;ID=H2,Number=0,Type=Flag,Description=&amp;quot;HapMap2 membership&amp;quot;&amp;gt;
##INFO=&amp;lt;ID=SVTYPE,Number=1,Type=String,Description=&amp;quot;Type of structural variant&amp;quot;&amp;gt;
##INFO=&amp;lt;ID=SVLEN,Number=1,Type=Integer,Description=&amp;quot;Difference in length between REF and ALT alleles&amp;quot;&amp;gt;
##INFO=&amp;lt;ID=END,Number=1,Type=Integer,Description=&amp;quot;End position of the variant described in this record&amp;quot;&amp;gt;
##ALT=&amp;lt;ID=INS,Description=&amp;quot;Insertion&amp;quot;&amp;gt;
##FILTER=&amp;lt;ID=q10,Description=&amp;quot;Quality below 10&amp;quot;&amp;gt;
##FILTER=&amp;lt;ID=s50,Description=&amp;quot;Less than 50% of samples have data&amp;quot;&amp;gt;
##FORMAT=&amp;lt;ID=GT,Number=1,Type=String,Description=&amp;quot;Genotype&amp;quot;&amp;gt;
##FORMAT=&amp;lt;ID=GQ,Number=1,Type=Integer,Description=&amp;quot;Genotype Quality&amp;quot;&amp;gt;
##FORMAT=&amp;lt;ID=DP,Number=1,Type=Integer,Description=&amp;quot;Read Depth&amp;quot;&amp;gt;
##FORMAT=&amp;lt;ID=HQ,Number=2,Type=Integer,Description=&amp;quot;Haplotype Quality&amp;quot;&amp;gt;
#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  NA00001 NA00002 NA00003
20  14370   rs6054257   G   A   29  PASS    NS=3;DP=14;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:51,51  1|0:48:8:51,51  1/1:43:5:.,.
20  17330   .   T   A   3   q10 NS=3;DP=11;AF=0.017 GT:GQ:DP:HQ 0|0:49:3:58,50  0|1:3:5:65,3    0/0:41:3
20  1110696 rs6040355   A   G,T 67  PASS    NS=2;DP=10;AF=0.333,0.667;AA=T;DB   GT:GQ:DP:HQ 1|2:21:6:23,27  2|1:2:0:18,2    2/2:35:4
20  1230237 .   T   .   47  PASS    NS=3;DP=13;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60  0|0:48:4:51,51  0/0:61:2
20  1234567 microsat1   GTC G,GTCT  50  PASS    NS=3;DP=9;AA=G  GT:GQ:DP    0/1:35:4    0/2:17:2    1/1:40:3
20  29444234    .   A   ACGTTCAGAGA .   PASS    END=29444245;SVTYPE=INS;SVLEN=10    GT:DP   0/1:8   0/1:5   0/0:2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You might want to pipe the command into a &lt;code&gt;less&lt;/code&gt; command in order to scroll up and down the file:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bcftools view myfile.vcf | less
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;bcftools view&lt;/code&gt; makes it easy to select a specific class of variants through the &lt;code&gt;--types&lt;/code&gt; option.
The variant classes available to choose from are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;snps&lt;/code&gt;: single nucleotide polymorphisms&lt;/li&gt;&lt;li&gt;&lt;code&gt;indels&lt;/code&gt;: insertions and deletions&lt;/li&gt;&lt;li&gt;&lt;code&gt;mnps&lt;/code&gt;: multi-nucleotide polymorphisms&lt;/li&gt;&lt;li&gt;&lt;code&gt;ref&lt;/code&gt;: reference variants&lt;/li&gt;&lt;li&gt;&lt;code&gt;bnd&lt;/code&gt;: breakend (i.e. start or end point of a translocation)&lt;/li&gt;&lt;li&gt;&lt;code&gt;others&lt;/code&gt;: any other type of variants&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Therefore, if you want to extract only the indels from our test VCF file, you can simply do:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# Use tail -5 to show only the last five lines
bcftools view --types indels myfile.vcf | tail -5
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will return the following:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;##FORMAT=&amp;lt;ID=HQ,Number=2,Type=Integer,Description=&amp;quot;Haplotype Quality&amp;quot;&amp;gt;
##bcftools_viewVersion=1.13+htslib-1.13+ds
##bcftools_viewCommand=view -v indels myfile.vcf; Date=Mon Jan 29 11:16:16 2024
#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  NA00001 NA00002 NA00003
20      1234567 microsat1       GTC     G,GTCT  50      PASS    NS=3;DP=9;AA=G  GT:GQ:DP        0/1:35:4        0/2:17:2        1/1:40:3
20      29444234        .       A       ACGTTCAGAGA     .       PASS    END=29444245;SVTYPE=INS;SVLEN=10        GT:DP   0/1:8   0/1:5   0/0:2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you want to skip the header entirely, you can easily discard it with the &lt;code&gt;-H&lt;/code&gt; option.&lt;/p&gt;&lt;p&gt;Other convenient filtering options available with the &lt;code&gt;view&lt;/code&gt; command include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;filter variants that have at least (&lt;code&gt;-m&lt;/code&gt;) or at most (&lt;code&gt;-M&lt;/code&gt;) a given number of alleles,&lt;/li&gt;&lt;li&gt;extract or exclude variants in regions defined in a &lt;a href=&quot;https://genome.ucsc.edu/FAQ/FAQformat.html#format1&quot;&gt;BED&lt;/a&gt; file, provided with &lt;code&gt;-T&lt;/code&gt;,&lt;/li&gt;&lt;li&gt;compress the output with using gzip compressiong with &lt;code&gt;-O z&lt;/code&gt;, or use the more efficient BCF format by giving &lt;code&gt;-O b&lt;/code&gt;.
You can see all the options for &lt;code&gt;view&lt;/code&gt; on the &lt;code&gt;bcftools&lt;/code&gt; &lt;a href=&quot;https://www.htslib.org/doc/bcftools.html#view&quot;&gt;website&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;querying-a-vcf-file&quot;&gt;Querying a VCF file&lt;/h2&gt;&lt;p&gt;While filtering variants is useful when visualising some of the outputs, it doesn’t solve a major problem of the VCF format: it can be very verbose and detailed.
The &lt;code&gt;INFO&lt;/code&gt; field in particular can get confusingly large very easily as many different metrics are stored here, ranging from sequencing statistics to functional annotations.
Parsing these fields can be tricky due to the structure of the file format, but &lt;code&gt;bcftools query&lt;/code&gt; gives us a powerful, yet simple, way to extract the pieces of information that we want.&lt;/p&gt;&lt;p&gt;Let’s try with a hands on example. To only show a subset of some fields, we can use the &lt;code&gt;--format&lt;/code&gt; option.
The option can alternatively be provided as &lt;code&gt;-f&lt;/code&gt;; we will use &lt;code&gt;--format&lt;/code&gt; in the examples below for clarity.
We may wish to extract only the variants’ positions and alleles (both reference and alternate) and separate them by a single whitespace.
We can do this with the command:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bcftools query --format &amp;#x27;%CHROM %POS %REF %ALT\n&amp;#x27; myfile.vcf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We refer to a field by prefixing its name with the &lt;code&gt;%&lt;/code&gt; special character. The field names and descriptions can be found in the last line of the VCF header, in this example we have:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;CHROM - the chromosome.&lt;/li&gt;&lt;li&gt;POS - the position on the chromosome where the variant occurs.&lt;/li&gt;&lt;li&gt;REF - the allele contained in the reference sequence.&lt;/li&gt;&lt;li&gt;ALT - alternative allele(s) which may occur in a sample.
Notice that we add a &lt;code&gt;\n&lt;/code&gt; (new-line) character at the end of the line: if we do not add it the software will save everything as a single, very long line.
So don’t forget to add it!
The command will return the following output:&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code&gt;20 14370 G A
20 17330 T A
20 1110696 A G,T
20 1230237 T .
20 1234567 GTC G,GTCT
20 29444234 A ACGTTCAGAGA
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can add a header simply by adding the &lt;code&gt;--print-header&lt;/code&gt; option (or &lt;code&gt;-H&lt;/code&gt; for short), facilitating the interpretation of the fields:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bcftools query --print-header --format &amp;#x27;%CHROM %POS %REF %ALT\n&amp;#x27; myfile.vcf
# [1]CHROM [2]POS [3]REF [4]ALT
20 14370 G A
20 17330 T A
20 1110696 A G,T
20 1230237 T .
20 1234567 GTC G,GTCT
20 29444234 A ACGTTCAGAGA
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This output is space-separated, but can be changed to be tab-separated by changing the command line to separate the individual fields with &lt;code&gt;\t&lt;/code&gt;, instead of &lt;code&gt; &lt;/code&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bcftools query --print-header --format &amp;#x27;%CHROM\t%POS\t%REF\t%ALT\n&amp;#x27; myfile.vcf
# [1]CHROM  [2]POS  [3]REF  [4]ALT
20      14370   G       A
20      17330   T       A
20      1110696 A       G,T
20      1230237 T       .
20      1234567 GTC     G,GTCT
20      29444234        A       ACGTTCAGAGA
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;extracting-fields-from-the-info-field&quot;&gt;Extracting fields from the INFO field&lt;/h2&gt;&lt;p&gt;The &lt;code&gt;INFO&lt;/code&gt; field of VCF files, can contain arbitrary user defined information.
It contains information in &lt;code&gt;key=value&lt;/code&gt; pairs; the information keys are usually detailed in the header section of the file.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;INFO&lt;/code&gt; field can be queried just like any other field by using the &lt;code&gt;%&lt;/code&gt; special character:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bcftools query --format &amp;#x27;%CHROM\t%POS\t%REF\t%ALT\t%INFO\n&amp;#x27; myfile.vcf
20      14370   G       A       NS=3;DP=14;AF=0.5;DB;H2
20      17330   T       A       NS=3;DP=11;AF=0.017
20      1110696 A       G,T     NS=2;DP=10;AF=0.333,0.667;AA=T;DB
20      1230237 T       .       NS=3;DP=13;AA=T
20      1234567 GTC     G,GTCT  NS=3;DP=9;AA=G
20      29444234        A       ACGTTCAGAGA     END=29444245;SVTYPE=INS;SVLEN=10
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;While this is useful, it displays the entirety of the &lt;code&gt;INFO&lt;/code&gt; field.
If we want to extract the depth of sequencing of each site in the VCF, we first need to check the file’s header lines (starting with &lt;code&gt;##&lt;/code&gt;) to identify the corresponding INFO flag.
Let’s have a look at the right line:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;##INFO=&amp;lt;ID=DP,Number=1,Type=Integer,Description=&amp;quot;Total Depth&amp;quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This header line tells us that within the &lt;code&gt;INFO&lt;/code&gt; field:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;there is a variable named &lt;code&gt;DP&lt;/code&gt; (&lt;code&gt;ID=DP&lt;/code&gt;),&lt;/li&gt;&lt;li&gt;that can have a single value (&lt;code&gt;Number=1&lt;/code&gt;),&lt;/li&gt;&lt;li&gt;which is an integer (&lt;code&gt;Type=Integer&lt;/code&gt;), and&lt;/li&gt;&lt;li&gt;refers to the “Total Depth” (&lt;code&gt;Description=&amp;quot;Total Depth&amp;quot;&lt;/code&gt;).
Now that we have identified the field we are interested in, we can retrieve it for each site in our VCF file by simply referring to it as &lt;code&gt;%INFO/DP&lt;/code&gt; (which can be read: extract &lt;code&gt;DP&lt;/code&gt; within &lt;code&gt;INFO&lt;/code&gt;):&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code&gt;bcftools query --format &amp;#x27;%INFO/DP\n&amp;#x27; myfile.vcf
14
11
10
13
9
.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Each line will represent a different depth for a different entry in the VCF file, returing &lt;code&gt;.&lt;/code&gt; when a value is missing. Easy, right?
This can help you to retrieve the fields you’re interested in from INFO, creating tables that can be used in downstream analyses and processes.&lt;/p&gt;&lt;h2 id=&quot;extracting-fields-for-each-sample&quot;&gt;Extracting fields for each sample&lt;/h2&gt;&lt;p&gt;VCF files can contain information for more than one biological sample.
The first eight columns of a VCF file refer to the genomic site, meaning that the fields are not referring to any sample specifically.
The sample-level information is stored in the subsequent columns:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;The &lt;code&gt;FORMAT&lt;/code&gt; column (column 9) provides the meta information detailing the values presented for each sample.&lt;/li&gt;&lt;li&gt;Column 10 onwards contains the per-sample information, one column per sample.
Each sample in the VCF file is named with a unique identifier, reported in the last line of the header.
For instance, our VCF file contains three separate individuals named &lt;code&gt;NA00001&lt;/code&gt;, &lt;code&gt;NA00002&lt;/code&gt; and &lt;code&gt;NA00003&lt;/code&gt;:&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code&gt;bcftools view -h Test/myfile.vcf | tail -1
#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  NA00001 NA00002 NA00003
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can look at the FORMAT and sample fields by querying them directly:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bcftools query -f &amp;#x27;%FORMAT\t[]\n&amp;#x27; myfile.vcf
GT:GQ:DP:HQ     0|0:48:1:51,51  1|0:48:8:51,51  1/1:43:5:.,.
GT:GQ:DP:HQ     0|0:49:3:58,50  0|1:3:5:65,3    0/0:41:3:.
GT:GQ:DP:HQ     1|2:21:6:23,27  2|1:2:0:18,2    2/2:35:4:.
GT:GQ:DP:HQ     0|0:54:7:56,60  0|0:48:4:51,51  0/0:61:2:.
GT:GQ:DP        0/1:35:4        0/2:17:2        1/1:40:3
GT:DP   0/1:8   0/1:5   0/0:2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first column in the above reports the order of the data in each sample field, each field is separated by a &lt;code&gt;:&lt;/code&gt;.
Each of these values is detailed in the header:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;##FORMAT=&amp;lt;ID=GT,Number=1,Type=String,Description=&amp;quot;Genotype&amp;quot;&amp;gt;
##FORMAT=&amp;lt;ID=GQ,Number=1,Type=Integer,Description=&amp;quot;Genotype Quality&amp;quot;&amp;gt;
##FORMAT=&amp;lt;ID=DP,Number=1,Type=Integer,Description=&amp;quot;Read Depth&amp;quot;&amp;gt;
##FORMAT=&amp;lt;ID=HQ,Number=2,Type=Integer,Description=&amp;quot;Haplotype Quality&amp;quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For instance, the first sample (column 10) has the values &lt;code&gt;GT=0|0&lt;/code&gt;, &lt;code&gt;GQ=48&lt;/code&gt;, &lt;code&gt;DP=1&lt;/code&gt; and &lt;code&gt;HQ=51,51&lt;/code&gt; for the first sample.&lt;/p&gt;&lt;p&gt;Values for one or more samples can be extracted by using their identifiers.
For example, we can extract &lt;code&gt;FORMAT&lt;/code&gt; and the fields for &lt;code&gt;NA00001&lt;/code&gt; and &lt;code&gt;NA00003&lt;/code&gt; samples by using the &lt;code&gt;--samples&lt;/code&gt; option (&lt;code&gt;-s&lt;/code&gt; for short) followed by the comma-separated list of sample identifiers:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bcftools query --samples NA00001,NA00003 --format &amp;#x27;%FORMAT\t[]\n&amp;#x27; myfile.vcf
GT:GQ:DP:HQ     0|0:48:1:51,51  1/1:43:5:.,.
GT:GQ:DP:HQ     0|0:49:3:58,50  0/0:41:3:.
GT:GQ:DP:HQ     1|2:21:6:23,27  2/2:35:4:.
GT:GQ:DP:HQ     0|0:54:7:56,60  0/0:61:2:.
GT:GQ:DP        0/1:35:4        1/1:40:3
GT:DP   0/1:8   0/0:2
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: the &lt;code&gt;-s&lt;/code&gt; option works also for &lt;code&gt;bcftools view&lt;/code&gt;, and can be replaced with &lt;code&gt;-S&lt;/code&gt; if you want to provide a text file of the list of individuals to consider. &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;You might have noticed the empty brackets (&lt;code&gt;[]&lt;/code&gt;) in the command.
This is an instruction for &lt;code&gt;bcftools&lt;/code&gt; to iterate across each sample (subject to sample selection with &lt;code&gt;-s&lt;/code&gt;) and return the requested values.
In the previous example, we did not specify any field, and that led &lt;code&gt;bcftools&lt;/code&gt; to return everything for each sample.&lt;/p&gt;&lt;p&gt;Now if we want to extract only the genotype (&lt;code&gt;GT&lt;/code&gt;) value for each sample, we can provide this &lt;strong&gt;within&lt;/strong&gt; the &lt;code&gt;[]&lt;/code&gt; (remember to also provide a delimiter within the brackets, or everything will be concatenated!):&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bcftools query --format &amp;#x27;[%GT\t]\n&amp;#x27; myfile.vcf
0|0     1|0     1/1
0|0     0|1     0/0
1|2     2|1     2/2
0|0     0|0     0/0
0/1     0/2     1/1
0/1     0/1     0/0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This command will return simply the genotype (&lt;code&gt;GT&lt;/code&gt; field) for each of the samples in the VCF file.
To access multiple fields, for example both genotype (&lt;code&gt;GT&lt;/code&gt;) and depth (&lt;code&gt;DP&lt;/code&gt;) for each sample as a set of comma-separated fields (again with &lt;code&gt;\t&lt;/code&gt; to show how we want the output to display):&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bcftools query -f &amp;#x27;[%GT,%DP\t]\n&amp;#x27; myfile.vcf
0|0,1   1|0,8   1/1,5
0|0,3   0|1,5   0/0,3
1|2,6   2|1,0   2/2,4
0|0,7   0|0,4   0/0,2
0/1,4   0/2,2   1/1,3
0/1,8   0/1,5   0/0,2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This command extracts all the data for each sample as a single column.
Alternatively it is possible to structure the output with columns containing each requested data field for all samples:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bcftools query -f &amp;#x27;[%GT,]\t[%DP,]\n&amp;#x27; myfile.vcf
0|0,1|0,1/1,    1,8,5,
0|0,0|1,0/0,    3,5,3,
1|2,2|1,2/2,    6,0,4,
0|0,0|0,0/0,    7,4,2,
0/1,0/2,1/1,    4,2,3,
0/1,0/1,0/0,    8,5,2,
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first column contains the comma-separated list of genotypes for each sample, whilst the second contains the comma-separated list of sequencing depths for each sample.&lt;/p&gt;&lt;h2 id=&quot;piping-commands&quot;&gt;Piping commands&lt;/h2&gt;&lt;p&gt;One of the main advantages of &lt;code&gt;bcftools&lt;/code&gt; is the possibility of combining multiple commands to achieve great flexibility.
This might involve combining some filtering with &lt;code&gt;bcftools view&lt;/code&gt; followed by &lt;code&gt;bcftools query&lt;/code&gt;.
For instance, if we want to extract chromosome, position, and alleles for the indels in our VCF, we can write:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bcftools view --types indels myfile.vcf | bcftools query --format &amp;#x27;%CHROM\t%POS\t%REF\t%ALT\n&amp;#x27;
20      1234567 GTC     G,GTCT
20      29444234        A       ACGTTCAGAGA
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;code&gt;view&lt;/code&gt; command here first extracts indels (i.e. insertions and deletions) from the file, before the &lt;code&gt;query&lt;/code&gt; command produces formatted output.&lt;/p&gt;&lt;h2 id=&quot;bcftools-plugins&quot;&gt;bcftools plugins&lt;/h2&gt;&lt;p&gt;VCF annotations can be very complex and it is not unusual for an &lt;code&gt;INFO&lt;/code&gt; value to include multiple subvalues.
An example of this is the functional annotation produced by tools such as the Variant Effect Predictor (&lt;a href=&quot;https://www.ensembl.org/info/docs/tools/vep/index.html&quot;&gt;VEP&lt;/a&gt;).
For these specific cases you might want to check if there is a plugin that can help you deal with these special use cases.
For instance, the annotations from VEP can be queried by using the &lt;a href=&quot;https://samtools.github.io/bcftools/howtos/plugin.split-vep.html&quot;&gt;split-vep plugin&lt;/a&gt;.&lt;/p&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;We hope the above was helpful and clarified how to query your VCF file. To recap:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;bcftools view&lt;/code&gt; can be used to view, convert and filter your VCF file.&lt;/li&gt;&lt;li&gt;&lt;code&gt;bcftools query&lt;/code&gt; can be used to extract and format desired data:&lt;ul&gt;&lt;li&gt;use the &lt;code&gt;%&lt;/code&gt; special character to target a field&lt;/li&gt;&lt;li&gt;use &lt;code&gt;/&lt;/code&gt; to target items in &lt;code&gt;INFO&lt;/code&gt; columns, for example &lt;code&gt;INFO/DP&lt;/code&gt;&lt;/li&gt;&lt;li&gt;use &lt;code&gt;[...]&lt;/code&gt; to retrieve per-sample metrics, for example &lt;code&gt;[%GT\t]&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;further-information&quot;&gt;Further information&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://labs.epi2me.io/notebooks/Introduction_to_Variant_Call_Format_(vcf)_files.html&quot;&gt;An introduction to VCF files&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://samtools.github.io/hts-specs/VCFv4.4.pdf&quot;&gt;VCF format&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.htslib.org/doc/bcftools.html&quot;&gt;bcftools&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.htslib.org/doc/bcftools.html#view&quot;&gt;bcftools view&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.htslib.org/doc/bcftools.html#query&quot;&gt;bcftools query&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://samtools.github.io/bcftools/howtos/plugins.html&quot;&gt;bcftools plugins&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/860ffd3335dca00c8ea96887832bc607/59ccf/drain-the-swamp.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/860ffd3335dca00c8ea96887832bc607/drain-the-swamp.jpeg</content:toenail></item><item><title><![CDATA[EPI2ME 24.02-01 Release]]></title><description><![CDATA[A bumper release of workflow updates and usability improvements.]]></description><link>https://labs.epi2me.io/epi2me-24.02-01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-24.02-01-release</guid><pubDate>Wed, 07 Feb 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/1ca711f974606a03fddcf0d68c80799f/59ccf/cheeky-monkey.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Dear Nanopore Community,&lt;/p&gt;&lt;p&gt;We are delighted to release a magnificent collection of updates and improvements to our EPI2ME workflows. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation&quot;&gt;wf-clone-validation&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation/blob/master/CHANGELOG.md&quot;&gt;v1.1.0&lt;/a&gt;] - introduces a new &lt;code&gt;--large_construct&lt;/code&gt; parameter to enable the assembly of larger plasmids and Bacterial Artificial Chromosomes (50,000-300,000bps).&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-pore-c&quot;&gt;wf-pore-c&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-pore-c/blob/master/CHANGELOG.md&quot;&gt;v1.1.0&lt;/a&gt;] &lt;ul&gt;&lt;li&gt;The newly added &lt;code&gt;--bed&lt;/code&gt; parameter will prepare a BED output file compatible with downstream tools including the &lt;a href=&quot;https://github.com/c-zhou/yahs&quot;&gt;Yahs&lt;/a&gt; scaffolder.&lt;/li&gt;&lt;li&gt;Introduces compute resource management improvements including a &lt;code&gt;--max_monomers&lt;/code&gt; parameter to filter out reads which contain more than this number of monomers, see the Changelog for more details. &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes/&quot;&gt;wf-transcriptomes&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes/blob/master/CHANGELOG.md&quot;&gt;v1.1.0&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;Output improvements for differential gene expression analyses that include&lt;ul&gt;&lt;li&gt;adding a &lt;code&gt;gene_name&lt;/code&gt; column to &lt;code&gt;de_analysis&lt;/code&gt; count TSV files.&lt;/li&gt;&lt;li&gt;additional read count output files including gene count per million reads and unfiltered transcript counts mapped with gene names.    &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;The workflow will now run to completion when no reference annotation is provided; the output transcriptome will however be unannotated.&lt;/li&gt;&lt;li&gt;The mapping processes are now multi-threaded.&lt;/li&gt;&lt;li&gt;Increased memory allocation for JAFFAL process to prevent out of memory errors.&lt;/li&gt;&lt;li&gt;Transcript summary tables are now also published into the output directory.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-aav-qc&quot;&gt;wf-aav-qc&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-aav-qc/blob/master/CHANGELOG.md&quot;&gt;v1.0.3&lt;/a&gt;] - Improvements include better datatype handling.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell&quot;&gt;wf-single-cell&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell/blob/master/CHANGELOG.md&quot;&gt;v1.0.3&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;The transcriptome sequence and annotation files prepared during the run are now published into the output directory.&lt;/li&gt;&lt;li&gt;Improved datatype handling during the tagging of BAM files.  &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot;&gt;wf-metagenomics&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics/blob/master/CHANGELOG.md&quot;&gt;v2.9.0&lt;/a&gt;] / &lt;a href=&quot;https://github.com/epi2me-labs/wf-16s&quot;&gt;wf-16S&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-16s/blob/master/CHANGELOG.md&quot;&gt;v1.1.0&lt;/a&gt;]:&lt;ul&gt;&lt;li&gt;Swaps the plotting backend for the barplot in the report from eCharts to Bokeh.&lt;/li&gt;&lt;li&gt;Default for &lt;code&gt;--n_taxa_barplot&lt;/code&gt; increased from 8 to 9.&lt;/li&gt;&lt;li&gt;The &lt;code&gt;--database_set&lt;/code&gt; parameter is now &lt;code&gt;Standard-8&lt;/code&gt; when the &lt;code&gt;--classifier&lt;/code&gt; parameter is set to &lt;code&gt;kraken2&lt;/code&gt;.&lt;/li&gt;&lt;li&gt;Bug fix with custom databases in the kraken2 and real-time pipelines.&lt;/li&gt;&lt;li&gt;Updated memory resources.&lt;/li&gt;&lt;li&gt;The workflow now accepts BAM files (as produced by &lt;a href=&quot;https://github.com/nanoporetech/dorado&quot;&gt;Dorado&lt;/a&gt;) as input.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation&quot;&gt;wf-human-variation&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation/blob/master/CHANGELOG.md&quot;&gt;v1.11.0&lt;/a&gt;] updates Straglr to fix some inconsistencies in the output VCF and appease VCF validator software. The latest version also ships with several performance improvements which will noticeably reduce workflow run time. &lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation&quot;&gt;wf-somatic-variation&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation/blob/master/CHANGELOG.md&quot;&gt;v1.0.0&lt;/a&gt;] adds a tumor-only mode for the base workflow, better resource directives to improve workflow robustness and several quality-of-life fixes.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-bacterial-genomes&quot;&gt;wf-bacterial-genomes&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-bacterial-genomes/blob/master/CHANGELOG.md&quot;&gt;v1.1.0&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;Workflow now uses Flye’s &lt;code&gt;--nano-hq&lt;/code&gt; parameter for the &lt;em&gt;de novo&lt;/em&gt; assembly. Additionally, Flye’s &lt;code&gt;--genome-size&lt;/code&gt; and &lt;code&gt;--asm-coverage&lt;/code&gt; parameters are available in the workflow as &lt;code&gt;--flye_genome_size&lt;/code&gt; and &lt;code&gt;--flye_asm_coverage&lt;/code&gt;, respectively.&lt;/li&gt;&lt;li&gt;Improved memory allocation.&lt;/li&gt;&lt;li&gt;A summary of results for all samples are now collected in JSON format in file &lt;code&gt;results.json&lt;/code&gt;.&lt;/li&gt;&lt;li&gt;Bug fixes to generate reports when the &lt;em&gt;de novo&lt;/em&gt; assembly fails.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-flu&quot;&gt;wf-flu&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-flu/blob/master/CHANGELOG.md&quot;&gt;v1.1.0&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;Downsampling is now turned off by default.&lt;/li&gt;&lt;li&gt;Added a new parameter (&lt;code&gt;--rbk&lt;/code&gt;) to improve downsampling of RBK data.&lt;/li&gt;&lt;li&gt;Bug fixes to generate reports when no samples undergo nextclade analysis. &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-alignment&quot;&gt;wf-alignment&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-alignment/blob/master/CHANGELOG.md&quot;&gt;v1.1.0&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;Dropped limit of 20 for &lt;code&gt;--thread&lt;/code&gt; parameter.&lt;/li&gt;&lt;li&gt;BAM tags in uBAM input files are now carried over to the output.&lt;/li&gt;&lt;li&gt;Fixed depth plots in report appearing mangled in some situations.&lt;/li&gt;&lt;li&gt;Now produces an MMI index file that can be re-used to reduce execution time when running the workflow again on the same reference.&lt;/li&gt;&lt;li&gt;&lt;code&gt;--reference_mmi_file&lt;/code&gt; option to use a pre-generated MMI index file as reference.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-amplicon&quot;&gt;wf-amplicon&lt;/a&gt; [&lt;a href=&quot;https://github.com/epi2me-labs/wf-amplicon/blob/master/CHANGELOG.md&quot;&gt;v1.0.3&lt;/a&gt;]&lt;ul&gt;&lt;li&gt;Better handling of whitespace / special characters in input file names.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The EPI2ME desktop application is now available tagged at version v5.1.9. The latest release includes improvements to stability and installation and is available from the project &lt;a href=&quot;https://labs.epi2me.io/downloads/&quot;&gt;downloads page&lt;/a&gt;.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The minimum allowed version of WSL required for workflow execution on Windows devices has been increased to 2.0.14. You should be prompted to update within the setup wizard, if your WSL version is not already up to date.&lt;/li&gt;&lt;li&gt;Login to the app is now available for early access users of our new EPI2ME cloud platform. We greatly welcome your feedback! There is more information on this upcoming product and release timelines are described in this &lt;a href=&quot;https://community.nanoporetech.com/posts/epi2me-unified-experience-13724&quot;&gt;earlier note&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We welcome any feedback and would love to hear your recommendations for future workflows, new features or additional functionality.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/1ca711f974606a03fddcf0d68c80799f/59ccf/cheeky-monkey.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/1ca711f974606a03fddcf0d68c80799f/cheeky-monkey.jpeg</content:toenail></item><item><title><![CDATA[Comparing sequences with dot plots]]></title><description><![CDATA[How to create dot plots and interpret dot plots for the comparison of sequences.]]></description><link>https://labs.epi2me.io/dot-plots</link><guid isPermaLink="false">https://labs.epi2me.io/dot-plots</guid><pubDate>Mon, 05 Feb 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/d11285dfd7ac66c5f39663bad620d733/59ccf/spicy-dots.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;If you had to make a list of the most valuable and insightful ways of visualising sequence data, would the humble but powerful dot plot be on that list?
Dot plots are a popular tool with which you might already be familiar. We use them in several of our workflows.
In this blog post we’ll look at when to use dot plots, and how to create and interpret them.&lt;/p&gt;&lt;p&gt;A dot plot is one way of comparing two sequences to find similarities and differences between them. It can be one sequence with itself (self comparison), or comparing two different sequences (cross comparison). A dot plot can be used for comparing any type of sequence including DNA, RNA, and amino acids. They allow you to quickly identify regions of interest in a sequence that may require further investigation.
Other use cases of dot plots include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Obtaining a global perspective of differences between two sequences.&lt;/li&gt;&lt;li&gt;Detection of repetitive elements such as tandem repeats or transposons within one sequence.&lt;/li&gt;&lt;li&gt;Finding homologous regions between two sequences.&lt;/li&gt;&lt;li&gt;Quality control of &lt;em&gt;de-novo&lt;/em&gt; sequence assembly to assess continuity and completeness of assembled sequences.&lt;/li&gt;&lt;li&gt;Visualising mutations such as insertions and deletions between a reference and sample sequence, as well as inversions, translocations and more complex rearrangements.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For instance, we use dot plots in the context of plasmid sequencing (with &lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation&quot;&gt;wf-clone-validation&lt;/a&gt;) to perform a self comparison of the &lt;em&gt;de-novo&lt;/em&gt; assembled plasmid sequence to help identify expected or unexpected repetitive elements.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:711px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:90.87719298245614%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAABYlAAAWJQFJUiTwAAABkUlEQVR42oWUCY7CMAxFe/87IlWtUAtUdINuLBmepZ9xMxphKYpJnBd/xzQLHxvHMZzP57AsS5jneTdr8Huapt1v+ff7PXRdF16vV8gA4gzDELZtM//9fofn82kzA2NmT+PxeFgMPsDb7WZrBsS4UTBdIqCHCowBIGvW1nX9BXJD0zS2QZb+IEEynzlz3/e2T9wOiMOmIB6Y+oC4lMzwpYZaGhByKtnL1iEZh4hVppIfM0QqBeWVJZFAzdysQ9fr1YbAHhqBp9PJDvHKXp6y1UFk0l5S5B+MQdYG5EYOS7KCfasAkUz/QN5iDeu6tuyQLqDPkEAewPdeGrOrITCC2raNclQ/fiNTANa1p3L8eRSlTC+qB9kkK+a0ff4bUTIwQFVVWS+ycTgcwuVyiVnI9Ff7CmSBPzfyyAw4vgd6mYL7y3ZAjFoC0RfEg9I28U2vkkSgAvI8t8xoD16edsJnVsH1qaJE+sKo3ijkgpghi2oLwbisLEsbKCiKIhyPR1v3rQacmmNZ+GLKimyA4KdN7e0H8ESFkVkOKSoAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Emboss dot plot&quot; title=&quot;Figure 1. A dot plot produce by the Emboss program.&quot; src=&quot;/static/6a2ca2d4dc071903a5b32647811d8b75/a8e5b/emboss_dotplot.png&quot; srcSet=&quot;/static/6a2ca2d4dc071903a5b32647811d8b75/0e2fe/emboss_dotplot.png 285w,/static/6a2ca2d4dc071903a5b32647811d8b75/432e7/emboss_dotplot.png 570w,/static/6a2ca2d4dc071903a5b32647811d8b75/a8e5b/emboss_dotplot.png 711w&quot; sizes=&quot;(max-width: 711px) 100vw, 711px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 1. A dot plot produce by the Emboss program.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;&lt;p&gt;To create a dot plot, we need to associate each of the two sequences with an axis of a 2D scatter plot.
The plot depicts regions of the two sequences which are similar, often referred to as homologous regions.
Marking a point on the scatter plot for every base pair that is the same between two sequences would lead to a very noisy dot plot.
For example, it would be silly to mark a dot at every possible co-ordinate where the two sequences have an ‘A’.
Instead similarity is only considered worthy of being marked when it occurs over longer distances.
We mark only subsequence matches of a predetermined length.
Note that the definition of “matches” in the context of sequence alignment may include approximate matches; some variation between two subsequences of the two sequences is allowed.&lt;/p&gt;&lt;p&gt;So we see that when creating dot plots, it is useful to understand a little about the parameters involved in sequence alignment.
The extent to which short subsequences of two longer sequences are similar, but not necessarily identical, is often parameterised by three parameters.
Alignments are “scored” according to the so-called match, mismatch, and gap penalties:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The match score is positive and awarded for identical base matches between a sequence.&lt;/li&gt;&lt;li&gt;The mismatch score subtracts from the overall score as a penalty for base differences.&lt;/li&gt;&lt;li&gt;The gap penalty also subtracts and is given when one sequence has a missing base relative to the other sequence. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;More elaborate scoring schemes exist (particularly for protein alignments), but it suffices to say that subsequence alignments can be scored to provide a quantitative measure of their similarity.&lt;/p&gt;&lt;p&gt;In order to create a dot plot, we must decide which subsequence alignments are similar enough to be plotted.
A simple and common method is to filter alignments by their length and their score.
Decreasing the subsequence length or alignment score can reduce stringency to reveal more subtle structural relationships, but can also increase background noise.
Raising the stringency of the filtering will produce cleaner plots at the expense of missing information.&lt;/p&gt;&lt;p&gt;Various online dot plot tools may implement different scoring systems and thresholds and therefore produce qualitatively different looking plots.&lt;/p&gt;&lt;h2 id=&quot;creating-plots&quot;&gt;Creating plots&lt;/h2&gt;&lt;p&gt;We’ve discussed a little about the background theory of how data underlying a dot plot can be created.
How though do we go about creating one?
Many tools and packages are available for sequence alignment. Several such tools exist with special considerations for creating dot plots, such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://en.vectorbuilder.com/tool/sequence-dot-plot.html&quot;&gt;Vectorbuilder dot plot&lt;/a&gt; is a simple online tool for comparing smaller sequences.&lt;/li&gt;&lt;li&gt;The &lt;a href=&quot;https://genomevolution.org/coge/SynMap.pl&quot;&gt;Genome Evolutions Syn Map&lt;/a&gt; online programme is great for exploring homology between larger sequences. It can retrieve the FASTA sequences required and create a dot plot which you can use to select regions of interest to inspect more closely. &lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mummer4.github.io/tutorial/tutorial.html&quot;&gt;Mummer4&lt;/a&gt; is a great offline option which has a nice tutorial. &lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/thackl/minidot&quot;&gt;Minidot&lt;/a&gt; is an R package for visualising dot plots of &lt;a href=&quot;https://github.com/lh3/minimap2&quot;&gt;minimap&lt;/a&gt; mappings.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In our clone validation workflow we use the &lt;a href=&quot;https://gitlab.com/mcfrith/last/&quot;&gt;Last&lt;/a&gt; tool to create self alignments and plot the outputs using &lt;a href=&quot;https://docs.bokeh.org/en/latest/index.html&quot;&gt;Bokeh&lt;/a&gt;.
We first create an alignment database using the &lt;code&gt;lastdb&lt;/code&gt; command-line tool, and then proceed to perform alignments using the &lt;code&gt;lastal&lt;/code&gt; command.&lt;/p&gt;&lt;p&gt;Using, as an example, the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/gene?Db=gene&amp;amp;Cmd=DetailsSearch&amp;amp;Term=4150&quot;&gt;DNA sequence&lt;/a&gt; of the MYC associated zinc finger protein (MAZ) we can run: &lt;/p&gt;&lt;pre&gt;&lt;code&gt;lastdb db.lastdb maz.fasta
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;to create an alignment database.
The database provides data structures which allow computationally efficient sequence similarity searches to be made.
We then use the &lt;code&gt;lastal&lt;/code&gt; command with a query sequence.
As this is a self comparison we use the same FASTA file again.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;lastal -m 1000 -w 10 db.lastdb maz.fasta &amp;gt; maz.maf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This outputs a &lt;a href=&quot;https://genome.ucsc.edu/FAQ/FAQformat.html#format5&quot;&gt;MAF file&lt;/a&gt; which includes a list of the alignments found.
We have increased the maximum initial matches per query position (&lt;code&gt;-m&lt;/code&gt;) parameter from a default of 10 to 1000, to increase sensitivity which for this short sequence comes at a minimal computational cost.
We also reduced the offset distance for suppressing repeats inside exact matches (&lt;code&gt;-w&lt;/code&gt;) from a default of 1000 to 10, again increasing sensitivity for this small sequence allowing us to visualise the self comparison in finer detail.&lt;/p&gt;&lt;p&gt;Then we used the following python function to create the Bokeh plot from the MAF file.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Import bokeh plotting libraries
from bokeh.models import ColumnDataSource, Segment
from bokeh.plotting import figure
import pandas as pd

def dot_plot_assembly(maf_assembly):
    &amp;quot;&amp;quot;&amp;quot;dot plot of assembly using a .maf format file.&amp;quot;&amp;quot;&amp;quot;
    # Create a bokeh plot
    plot = figure(
        width=400, height=400,
        min_border=0, x_axis_label=&amp;quot;First sequence position&amp;quot;,
        y_axis_label=&amp;quot;Second sequence position&amp;quot;, title=&amp;quot;Dot plot&amp;quot;)
    plot.toolbar_location = None
    # Iterate through maf file to create a dataframe
    records = list()
    with open(maf_assembly) as maf:
        while True:
            line = maf.readline()
            if line.startswith(&amp;#x27;#&amp;#x27;):
                # get read length available in the MAF generated with LAST
                if &amp;#x27;letters&amp;#x27; in line:
                    read_length = int(line.split(&amp;#x27;letters=&amp;#x27;)[1])
                    continue
                else:
                    continue
            # a is for each alignment
            elif line.startswith(&amp;#x27;a&amp;#x27;):
                # take successive &amp;#x27;s&amp;#x27; lines
                r1 = maf.readline().split()[1:5]
                r2 = maf.readline().split()[1:5]
                maf.readline()
                records.append(r1 + r2)
            elif line == &amp;quot;&amp;quot;:
                break
            else:
                raise IOError(&amp;quot;Cannot read alignment file&amp;quot;)
    # take reference start, length and orientation
    # and query start, length and orientation
    names = [
        &amp;#x27;ref&amp;#x27;, &amp;#x27;rstart&amp;#x27;, &amp;#x27;rlen&amp;#x27;, &amp;#x27;rorient&amp;#x27;,
        &amp;#x27;query&amp;#x27;, &amp;#x27;qstart&amp;#x27;, &amp;#x27;qlen&amp;#x27;, &amp;#x27;qorient&amp;#x27;]
    df_all = pd.DataFrame(records, columns=names)
    df_all = df_all.astype({&amp;#x27;qstart&amp;#x27;: int, &amp;#x27;qlen&amp;#x27;: int, &amp;#x27;rstart&amp;#x27;: int, &amp;#x27;rlen&amp;#x27;: int})
    # If query orientation is +
    df = df_all[df_all.qorient.isin([&amp;#x27;+&amp;#x27;])]
    # create query and ref end columns by adding length to start
    df[&amp;#x27;qend&amp;#x27;] = df[&amp;#x27;qstart&amp;#x27;] + df[&amp;#x27;qlen&amp;#x27;]
    df.loc[df[&amp;#x27;rorient&amp;#x27;] == &amp;#x27;+&amp;#x27;, &amp;#x27;rend&amp;#x27;] = df[&amp;#x27;rstart&amp;#x27;] + df[&amp;#x27;rlen&amp;#x27;]
    # If reference orientation is negative switch reference start and end
    df.loc[df[&amp;#x27;rorient&amp;#x27;] == &amp;#x27;-&amp;#x27;, &amp;#x27;rend&amp;#x27;] = df[&amp;#x27;rstart&amp;#x27;]
    df.loc[df[&amp;#x27;rorient&amp;#x27;] == &amp;#x27;-&amp;#x27;, &amp;#x27;rstart&amp;#x27;] = df[&amp;#x27;rstart&amp;#x27;] - df[&amp;#x27;rlen&amp;#x27;]
    # Add forward lines to plot
    source = ColumnDataSource(df)
    glyph = Segment(x0=&amp;#x27;rstart&amp;#x27;, y0=&amp;#x27;qstart&amp;#x27;, x1=&amp;#x27;rend&amp;#x27;, y1=&amp;#x27;qend&amp;#x27;, line_color=&amp;quot;black&amp;quot;)
    plot.add_glyph(source, glyph)
    # if query orientation is -
    df = df_all[df_all.qorient.isin([&amp;#x27;-&amp;#x27;])]
    # If the orientation is &amp;quot;-&amp;quot;, start coordinate is in the reverse strand (maf docs)
    # Therefore as plot will be + vs + query start needs to be flipped
    df[&amp;#x27;qstart&amp;#x27;] = read_length - df[&amp;#x27;qstart&amp;#x27;]
    df[&amp;#x27;qend&amp;#x27;] = df[&amp;#x27;qstart&amp;#x27;] - df[&amp;#x27;qlen&amp;#x27;]
    df[&amp;#x27;rend&amp;#x27;] = df[&amp;#x27;rstart&amp;#x27;] + df[&amp;#x27;rlen&amp;#x27;]
    # Add reverse complement lines to plot
    source = ColumnDataSource(df)
    glyph = Segment(x0=&amp;#x27;rstart&amp;#x27;, y0=&amp;#x27;qstart&amp;#x27;, x1=&amp;#x27;rend&amp;#x27;, y1=&amp;#x27;qend&amp;#x27;, line_color=&amp;quot;red&amp;quot;)
    plot.add_glyph(source, glyph)
    return plot
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This creates the following plot which as you can see consists of red and black diagonal lines in varying orientations.
The x-axis is showing the base pairs of the (first) reference sequence and the y-axis is the base pairs of the (second) query sequence.
So what does all this mean?
It may look complicated at first, but read on to find out more.&lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;maz&quot; plotCaption=&quot;Figure 2. Dot plot illustrating the self-similarity of the DNA sequence encoding the Zinc finger transcription factor.&quot;&gt;&lt;/div&gt;&lt;h2 id=&quot;interpreting-dot-plots&quot;&gt;Interpreting dot plots&lt;/h2&gt;&lt;p&gt;In the previous section we discussed how to create dot plots by aligning sequences using &lt;code&gt;lastal&lt;/code&gt; and plotting its outputs using Python.
In this section we will discuss what can be discerned from such plots.&lt;/p&gt;&lt;h3 id=&quot;self-comparison-of-a-sequence&quot;&gt;Self comparison of a sequence&lt;/h3&gt;&lt;p&gt;If two sequences are exactly the same we expect to see a diagonal line.
This represents the fact that the every position in the first sequence matches exactly the equivalent position in the second. &lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;selfComparison&quot; plotCaption=&quot;Figure 3. Self-comparison of a sequence. A diagonal line is observed showing how each position in the &amp;#x27;first&amp;#x27; sequence matches the equivalent position in the &amp;#x27;second&amp;#x27; (identical) sequence.&quot;&gt;&lt;/div&gt;&lt;h3 id=&quot;a-sequence-showing-a-duplication&quot;&gt;A sequence showing a duplication&lt;/h3&gt;&lt;p&gt;If there are duplications within the sequence we would see parallel diagonal lines.
In the example below the 10 kb input sequence consists of a duplicated element of 5 kb length.
We see three parallel lines. The central diagonal line along y=x depicts regions aligning to themselves.
The lines above and below the diagnonal indicate distinct homologous copies of sequence aligning to each other.
The upper line is the &lt;em&gt;second&lt;/em&gt; copy of the repeat in the reference sequence aligning to the &lt;em&gt;first&lt;/em&gt; copy in the query sequence.
The lower diagonal is the &lt;em&gt;first&lt;/em&gt; copy in the reference sequence aligning to the &lt;em&gt;second&lt;/em&gt; copy in the query sequence.
If there were five lines this would indicate three duplicate copies, seven lines would indicate three copies, and so on.&lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;fullRepeat&quot; plotCaption=&quot;Figure 4. A sequence duplication represented in a dot plot. Parallel lines are observed illustrating the first copy aligning to the second and vice-versa.&quot;&gt;&lt;/div&gt;&lt;h3 id=&quot;a-highly-repetitive-sequence&quot;&gt;A Highly Repetitive sequence&lt;/h3&gt;&lt;p&gt;In an extreme case of the above, highly repetitive regions will appear as many small parallel lines which resemble a square:&lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;repetitiveRegion&quot; plotCaption=&quot;Figure 5. The classic tell-tale sign of a highly repetitive region: many parallel lines forming a hatched square shape.&quot;&gt;&lt;/div&gt;&lt;h3 id=&quot;insertions-and-deletions-between-two-sequences&quot;&gt;Insertions and deletions between two sequences&lt;/h3&gt;&lt;p&gt;Insertion and deletion are terms used to denote additional or missing stretches in one sequence with respect to another.
Subsequences in the second sequence that are not present in the first sequence are termed insertions.
These additional sequences have no match in the first sequence; they cause a break in the main diagonal line and a jump up slightly.
Deletions are the corollary of this: they are missing subsequence in the second sequence relative to the first.
(Alternatively they can be described as an insertion in the first sequence relative to the second).
A deletion appears in a dot plot as a jump sideways in the main diagonal line.&lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;insertionDeletion&quot; plotCaption=&quot;Figure 6. Structural diffences between two sequences. A insertion (at position 500) does not have a corresponding match in the first sequence. A jump upwards results. A deletion (at position 4000) results in a sideways step.&quot;&gt;&lt;/div&gt;&lt;h3 id=&quot;inversions&quot;&gt;Inversions&lt;/h3&gt;&lt;p&gt;Inversions are subsequences which occur in the opposite direction in one sequence relative to another.
Inversions and inverted repeats appear in dot plots as lines perpendicular to, and intersecting, the main diagonal.
Many tools for creating dot plots will colour these lines differently to help distinguish when they are overlapping with forward repeats.
In our workflows inversions are coloured red.&lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;inversion&quot; plotCaption=&quot;Figure 7. An inversion is represented in a dot plot as a trailing diagonal line.&quot;&gt;&lt;/div&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;The assemblies and resulting dot plots in the clone validation workflow are usually small and simple. However, you can also create dot plots for large scale genomes with multiple contigs, which can reveal interesting patterns. Sometimes you will need to tweak available parameters to fine tune the sensitivity to make the plot meaningful.&lt;/p&gt;&lt;p&gt;We hope this post showed how useful dot plots are for comparing two sequences and how easy they are to interpret once you recognise the common patterns.
Why not have a go at creating your own with some sequences of interest using one of the offline or online tools listed above?&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/d11285dfd7ac66c5f39663bad620d733/59ccf/spicy-dots.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/d11285dfd7ac66c5f39663bad620d733/spicy-dots.jpeg</content:toenail></item><item><title><![CDATA[Input directory structure for EPI2ME workflows]]></title><description><![CDATA[A brief guide on the input data structure expected by EPI2ME workflows.]]></description><link>https://labs.epi2me.io/input-data</link><guid isPermaLink="false">https://labs.epi2me.io/input-data</guid><pubDate>Mon, 22 Jan 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/d342c23470cec4c33dc4816f334c1a14/59ccf/crabby-inputs.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;The EPI2ME team strives to make bioinformatics analysis as easy to use and intuitive as possible.
Our &lt;a href=&quot;https://www.nextflow.io/&quot;&gt;nextflow&lt;/a&gt; workflows are designed to seamlessly run with Oxford Nanopore Technologies’ device outputs.
They accept sequencing data in the format output by the sequencer, as well as forms into which users may have pre-processed their data.
In this blog post we will explore the options provided and note a few minor exceptions to the rule.&lt;/p&gt;&lt;p&gt;So, without further ado, let’s dive into the dos and don’ts of passing data to one of our workflows.&lt;/p&gt;&lt;h2 id=&quot;input-file-types&quot;&gt;Input file types&lt;/h2&gt;&lt;p&gt;All EPI2ME workflows accept &lt;a href=&quot;https://en.wikipedia.org/wiki/FASTQ_format&quot;&gt;FASTQ&lt;/a&gt; files and an increasing number can also be used with &lt;a href=&quot;https://en.wikipedia.org/wiki/Binary_Alignment_Map&quot;&gt;BAM&lt;/a&gt;.
On the command line, you would pass FASTQ input with the &lt;code&gt;--fastq&lt;/code&gt; flag and BAM with &lt;code&gt;--bam&lt;/code&gt;.
In EPI2ME Desktop, the corresponding options are in the Input Options panel when launching a new workflow (see Figure below).
The expected file extensions are &lt;code&gt;[.fastq, .fastq.gz, .fq, .fq.gz]&lt;/code&gt; and &lt;code&gt;[.bam, .ubam]&lt;/code&gt;, respectively.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:86.66666666666667%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAABYlAAAWJQFJUiTwAAACXElEQVR42o1T7YoUMRCcdxVPOf8Igq/hCaJwKsqBiM+lIufufGUm38m0VT0zxwh6mqXJZpJUV6q6mwdX13Lx8p08fIH56q08eX0jl4d4ev1Jnn/4ovHs/We5fHNzF49ffZSL7R7jEaLpnZNUFhkHI+3pLDln+d+x1Cq1FKm1bHOV5tttKzNAT20vX7//kHGaJaQsPiaNiAQJh1Mu+F90LyJCShKwF/fAenZeGjNbcS6I6YzcnjsZjRUzOwDjewhivReLgxnZ0wYeyWxZ1qiLYJKEJG3bSRNC0oyzdzKMRnyI+mxmLLgYsR66QbyFND5gHSSAeQVbPpmDZwO+d10vjfcRlJPY5BVwBuMMFoWaIPVSKm+AQl4DBLhe+I2APIPI2NsAkwK6FGQyMxgGZcgnUGQ+10ISDyAHtpw1No2paUFSD/bntl0ZppLFxSADnHYAIBAZMjOFpmGDmWSCtpM9BBJZJJmt170zNdwZ2uhhyKSAZJdzUcETLkQYlHEp4xL/pz3AMOXVdQfzNsCotvvETA6CR2W2D4J7dZp7AfW21h5fUPQlVc8nGNN2O8OSxHgL2wfp+wGORQXioKYtxGb0w6gSGLpNdimpw6o51ncur8+reoiby+acdgPn5d9dk0v+HfBvg13UgRnr023B/3TYxawdlPFsd3R5Bzwy24fFwbYfhR2lzu5O4+mTi1rkdJkaj8asnXIfw8jSAGAYJ3U4ojUpTUxrN2lfx7VuybLpe3MvYN1YL1vfauzf8IsAY+GzAVg6zenUKW1m+GPkw5wP661WDVjf/mzldO5V219i1Rhv57qXHQAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Desktop-Input-Options&quot; title=&quot;Input Options panel in EPI2ME Desktop&quot; src=&quot;/static/be6ed26231ec354ce087295860aa3220/b5cea/desktop-app-input-options.png&quot; srcSet=&quot;/static/be6ed26231ec354ce087295860aa3220/0e2fe/desktop-app-input-options.png 285w,/static/be6ed26231ec354ce087295860aa3220/432e7/desktop-app-input-options.png 570w,/static/be6ed26231ec354ce087295860aa3220/b5cea/desktop-app-input-options.png 1140w,/static/be6ed26231ec354ce087295860aa3220/c1b63/desktop-app-input-options.png 1200w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Input Options panel in EPI2ME Desktop&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;h2 id=&quot;input-structure&quot;&gt;Input structure&lt;/h2&gt;&lt;p&gt;Whether in FASTQ or BAM format, input data for our workflows should be structured in one of these three ways:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;a single file,&lt;/li&gt;&lt;li&gt;a single directory containing several target (e.g. FASTQ) files, or&lt;/li&gt;&lt;li&gt;a directory containing sub-directories (usually barcodes) which in turn contain the target files.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Some exceptions to these options are noted below.&lt;/p&gt;&lt;p&gt;Example file trees for the three cases might look as follows:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# single file           # single directory              # barcoded directories
.                       .                               .
└── reads.fq.gz         └── my-input-dir                └── my-input-dir
                            ├── reads1.fq.gz                ├── barcode01
                            ├── reads2.fq.gz                │   ├── reads1.fq.gz
                            └── reads3.fq.gz                │   ├── reads2.fq.gz
                                                            │   └── reads3.fq.gz
                                                            ├── barcode02
                                                            │   ├── reads1.fq.gz
                                                            │   ├── reads2.fq.gz
                                                            │   └── reads3.fq.gz
                                                            ├── barcode03
                                                            │   ├── reads1.fq.gz
                                                            │   ├── reads2.fq.gz
                                                            │   └── reads3.fq.gz
                                                            ├── ...
                                                            │
                                                            ...
                                                            │
                                                            └── unclassified
                                                                ├── reads1.fq.gz
                                                                ├── reads2.fq.gz
                                                                └── reads3.fq.gz
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the third case, the sub-directories can only contain target files “one level deep”.
In other words, the following is not allowed:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# this will fail input validation
.
├── barcode01
│   ├── dir-within-barcode
│   │   └── reads.fq.gz
│   ├── reads1.fq.gz
│   ├── reads2.fq.gz
│   └── reads3.fq.gz
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Similarly, “mixtures” of the second and third case (i.e. target files in the top-level directory and in barcodes) will also fail:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# this will fail input validation
.
├── reads.fq.gz
├── barcode01
│   ├── reads1.fq.gz
│   ├── reads2.fq.gz
│   └── reads3.fq.gz
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The workflows are inflexible in this way such that they are able to correctly interpret the users
intentions as to which files should be used for analysis.&lt;/p&gt;&lt;p&gt;Non-target files and directories containing only such files will be ignored.
The below is therefore allowed:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# this will ignore non-accepted input files
.
├── some-other-file.txt
├── barcode01
│   ├── reads1.fq.gz
│   ├── reads2.fq.gz
│   ├── reads3.fq.gz
│   └── yet-another-file.csv
...
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;exceptions&quot;&gt;Exceptions&lt;/h3&gt;&lt;p&gt;Most of our workflows behave precisely as above.
Some, however, deviate and are more strict or accept only a subset of the options.
The workflows &lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation#inputs&quot;&gt;wf-human-variation&lt;/a&gt; and &lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation#inputs&quot;&gt;wf-somatic-variation&lt;/a&gt; handle a single sample only; the options of directories of samples are therefore not relevant.
By contrast, &lt;a href=&quot;https://github.com/epi2me-labs/wf-tb-amr#inputs&quot;&gt;wf-tb-amr&lt;/a&gt; relies on the use of control samples and therefore explicitly requires multiple samples and a sample sheet (see below).
Please refer to the respective READMEs for more details regarding their input data requirements.&lt;/p&gt;&lt;h3 id=&quot;unclassified-reads&quot;&gt;Unclassified reads&lt;/h3&gt;&lt;p&gt;The &lt;code&gt;unclassified&lt;/code&gt; directory contains reads for which no barcode could be assigned during demultiplexing and it is ignored by default.
It can be included in the analysis with the &lt;code&gt;--analyse_unclassified&lt;/code&gt; flag.
In EPI2ME Desktop, this parameter is usually found in the Input Options panel (just like the options for &lt;code&gt;--fastq&lt;/code&gt; and &lt;code&gt;--bam&lt;/code&gt;).&lt;/p&gt;&lt;h3 id=&quot;sample-sheets&quot;&gt;Sample sheets&lt;/h3&gt;&lt;p&gt;When using barcoded directories, a sample sheet with metadata for the individual barcodes can be provided.
This needs to be a CSV file with at least the following two columns (extra columns are allowed):&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;alias&lt;/code&gt;: The alias (or sample name) for a barcode / directory.&lt;/li&gt;&lt;li&gt;&lt;code&gt;barcode&lt;/code&gt;: The barcode.
This needs to be in the format &lt;code&gt;barcode(X)YZ&lt;/code&gt; with &lt;code&gt;(X)YZ&lt;/code&gt; denoting either two or three integers (e.g. &lt;code&gt;barcode01&lt;/code&gt; or &lt;code&gt;barcode001&lt;/code&gt;).
All values in this column need to be of the same length (i.e. using &lt;code&gt;barcode001&lt;/code&gt; as well as &lt;code&gt;barcode02&lt;/code&gt; in the same sample sheet is not allowed).&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Some workflows require additional columns.
&lt;a href=&quot;https://github.com/epi2me-labs/wf-tb-amr/&quot;&gt;wf-tb-amr&lt;/a&gt;, for example, also expects a &lt;code&gt;type&lt;/code&gt; column. (accepted values are &lt;code&gt;test_sample&lt;/code&gt;, &lt;code&gt;positive_control&lt;/code&gt;, &lt;code&gt;negative_control&lt;/code&gt;, and &lt;code&gt;no_template_control]&lt;/code&gt;).&lt;/p&gt;&lt;p&gt;A sample sheet with four samples might look like this:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;barcode,alias,type
barcode01,sample1,test_sample
barcode02,sample2,test_sample
barcode03,positive,positive_control
barcode04,negative,negative_control
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;&lt;p&gt;Note: The above follows the MinKNOW sample sheet specifications and thus a valid MinKNOW sample sheet will also work with EPI2ME workflows.&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id=&quot;special-characters&quot;&gt;Special characters&lt;/h2&gt;&lt;p&gt;Whitespace and special characters in sample names, file paths, or reference sequence IDs can cause certain bioinformatics tools to fail.
EPI2ME workflows contain precautions to prevent this from happening, but we still recommend to only use alphanumerical characters (i.e. numbers and lower- or upper-case letters from ‘a’ to ‘z’) and underscores.
For example, &lt;code&gt;sample_01&lt;/code&gt; is fine, but &lt;code&gt;sample_A*2:01/10&lt;/code&gt; is not.&lt;/p&gt;&lt;h2 id=&quot;real-time-analysis-with---watch_path&quot;&gt;Real-time analysis with &lt;code&gt;--watch_path&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;At the time of writing, &lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics#311-running-wf-metagenomics-in-real-time&quot;&gt;wf-metagenomics&lt;/a&gt; and &lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling#5-real-time-analysis&quot;&gt;wf-basecalling&lt;/a&gt; can be run in real time.
To enable this, the flag &lt;code&gt;--watch_path&lt;/code&gt; needs to be provided and the input cannot be a single FASTQ / BAM file.
&lt;a href=&quot;https://www.nextflow.io/&quot;&gt;Nextflow&lt;/a&gt; will then &lt;a href=&quot;https://www.nextflow.io/docs/latest/channel.html#watchpath&quot;&gt;watch the input directory&lt;/a&gt; for new files with the correct extensions to appear and the pipeline will analyse them individually whenever they become available.&lt;/p&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;We hope the above was helpful and clarified how to provide input data to EPI2ME workflows. To recap:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;EPI2ME workflows accept either FASTQ and / or BAM input files which can be structured in one of three ways (a single target file, a single top-level directory containing target files, a directory containing barcoded sub-directories with target files).&lt;/li&gt;&lt;li&gt;“Mixing” of these structures is not allowed (i.e. you cannot use a directory that contains barcodes as well as target files).&lt;/li&gt;&lt;li&gt;Unclassified reads are ignored by default, but this behaviour can be overridden.&lt;/li&gt;&lt;li&gt;Additional metadata (like sample names) can be provided with a sample sheet.&lt;/li&gt;&lt;li&gt;Some workflows allow analysis of input files in real time (i.e. as they are being created by the sequencing device).&lt;/li&gt;&lt;li&gt;Special characters should be avoided as much as possible.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;When keeping these points in mind, analysing your data should be as easy as selecting the path to the inputs and pressing a button.&lt;/p&gt;&lt;p&gt;If you have further questions or run into issues, please &lt;a href=&quot;/contactus&quot;&gt;let us know&lt;/a&gt;.&lt;/p&gt;&lt;h2 id=&quot;further-information&quot;&gt;Further information&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://labs.epi2me.io/notebooks/Introduction_to_SAM_and_BAM_files.html&quot;&gt;An introduction to SAM and BAM files&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/FASTQ_format&quot;&gt;FASTQ format&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/SAM_(file_format)&quot;&gt;SAM format&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Binary_Alignment_Map&quot;&gt;BAM format&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://labs.epi2me.io/quickstart/&quot;&gt;EPI2ME Desktop quickstart&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://labs.epi2me.io/agent-transition/&quot;&gt;Transition from EPI2ME Agent to EPI2ME Desktop&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/d342c23470cec4c33dc4816f334c1a14/59ccf/crabby-inputs.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/d342c23470cec4c33dc4816f334c1a14/crabby-inputs.jpeg</content:toenail></item><item><title><![CDATA[Bed bugs]]></title><description><![CDATA[Introducing BED Bugs: an online tool for validating adaptive sampling inputs.]]></description><link>https://labs.epi2me.io/bed-bugs-intro</link><guid isPermaLink="false">https://labs.epi2me.io/bed-bugs-intro</guid><pubDate>Fri, 19 Jan 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/e5312e44ad2c4508791b18a2f4b00178/59ccf/coconut-rhinoceros-beetle.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;&lt;em&gt;(Yes, I know that’s not a bed bug; its a coconut rhinoceros beetle.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;The EPI2ME team is happy to release &lt;a href=&quot;https://labs.epi2me.io/bed-bugs&quot;&gt;Bed Bugs&lt;/a&gt;, an online
tool for validating input files for &lt;a href=&quot;https://nanoporetech.com/resource-centre/adaptive-sampling-oxford-nanopore&quot;&gt;adaptive sampling&lt;/a&gt;
experiments with MinKNOW.
This was a little bit of a fun side project for us using some technologies we’ve
previously experimented with but never found a good use for.&lt;/p&gt;&lt;p&gt;Adaptive sampling is a feature unique to Oxford Nanopore Technologies’ sequencing systems.
The technique enables a large number of applications to be carried out with very basic sample preparation,
leaving the complex target selection to be performed by the sequencer itself.
Adaptive sampling can be used to enrich for strands that contain a target region of interest,
thereby significantly decreasing the costs while preserving all the benefits of long-read sequencing.
Users can also use it to reject strands from an organism which is of no interest.
For example in the case of microbiome applications this could provide a simpler workflow,
negating any need to deplete the host during sample preparation.&lt;/p&gt;&lt;p&gt;Another use for adaptive sampling is to balance coverage of barcodes, amplicons or regions of a genome
ensuring target depths are achieved uniformly for the regions of interest.&lt;/p&gt;&lt;h2 id=&quot;introducing-bed-bugs&quot;&gt;Introducing Bed Bugs&lt;/h2&gt;&lt;p&gt;Central to the implementation of adaptive sampling within the MinKNOW instrument software is that
users must provide reference sequences and a so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/BED_(file_format)&quot;&gt;BED file&lt;/a&gt;
denoting regions of interest.
Currently MinKNOW provides little up-front validation of these inputs,
which can leave users confused when their adaptive sampling experiment hasn’t achieved what they
set out to do.
BED Bugs aims to fill this gap by providing some simple formatting checks and validations on these files
including cross-checks between the files.
It provides the user with feedback as to potential issues with their files.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1044px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:97.89473684210527%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAADZklEQVR42p2UXY/cNBSGc4HgYtWWq3KFVIpUCggkJH4cN3ADFImV+GhLKyH+C/cU2N12dnYyncxndpKZySSxHdtJnMzLsWdnmEWAEJGeOMfHPjmvj23v6O338caHH+P2B8R7H+HGnXs4uvMObr51H6/ffRe37t7HEX2/dvtNeK8cwXv1xj9wc8sX3z/G8dMf8TXx1eOn+PLhD3jgeIIHj57A+q39+XeP8OnxN/js+Nu/xfosXl1VkIWA1gpt0+D/PA+fjfHJzz5+Op3Bk0ojyTnBwAqFXEgUuoKuaihCXrXXKZFTEkJLlKZGJAQmNH+SpfCYlPit08VZ13f8cvIcw9klVN1gGi3RH88wDCOEiwQrJpBLjazgOO120AteYhLNEK/mmC9C+kkGL+MCp50LmhwjmIYYEpPLiLKkP6/WmMVLLNLcEa8zZEKBK4kgGmOazBGnS0TrBeZJDKEKeKZtULUtzGbjqAlrS1Ohvup3fZvW2bKmNTclWC0dZtOgAfmolUbDK2gdgumYmGCxXoJJTtLWyDRDnCwxmIwRUZtLRpKISiCj7/4wQD8IMJ2HmK9irHhKPsqQK4GzwTnOhz2cj3wM5iNEJENVGhNalxfBBfX3cDHuozv2scwTcF3gxH+B33tnGIQjzKLQ9ZdtDa+iV24EmCmQ1wJpxVAYhbKpIBp13VfSTqAMNfl4Q5KbAqotSXbrgummhKfrEst07VizHJrWR1TSSbN2kqdYEUW5/QkjWbniGM2mCOOIloJT1RnWIkUiM7ttOH7tnuHU72AwHbmK2crZLILLCZ51TvDcP0d/NIA/eknBt5K7gY8LojvoOazPBvc0VXOpKDuSoyg7i63iNhuBlUrBa5JGFSyoqlaBrDVCFiPMY1egrMghSkm7w9C2oePWOKj0xjiMaWD7HWRXdY26Ng5z5W9oCx2yG+uJQoKLLQWdAqnKvc14gRmdmuFwjOFogkEwQpoxGlPtxxwi6Oh69mWxwRjjjv1fKZMsy6HovG9oc2/oItBaI00zp2Q3zt4HuzgHAakVBcqy3N8idimSJHFSdo9SigKm126braq/BLRICgrKw2VD2IBZlu3tXUDOufve9f9LQLmfaJ+W5NhsDvusZMbYf8/QVnuHraqdbMzWbtuNW8+c7r7DcVL+uYZ/APr+sEJlu/MGAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Validation report&quot; title=&quot;Validation report from BedBugs. The report checks various properties of the input files to check that they are well-formed and mutually compatible.&quot; src=&quot;/static/4c6c39c1fcda53f70d31494d4d9aeb80/00172/bed-check.png&quot; srcSet=&quot;/static/4c6c39c1fcda53f70d31494d4d9aeb80/0e2fe/bed-check.png 285w,/static/4c6c39c1fcda53f70d31494d4d9aeb80/432e7/bed-check.png 570w,/static/4c6c39c1fcda53f70d31494d4d9aeb80/00172/bed-check.png 1044w&quot; sizes=&quot;(max-width: 1044px) 100vw, 1044px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Validation report from BedBugs. The report checks various properties of the input files to check that they are well-formed and mutually compatible.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;For readers who just want to check their adaptive sampling inputs, read no further.
You can start using &lt;a href=&quot;https://labs.epi2me.io/bed-bugs&quot;&gt;Bed Bugs now&lt;/a&gt;!&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The core technology used within Bed Bugs is &lt;a href=&quot;https://en.wikipedia.org/wiki/WebAssembly&quot;&gt;WebAssembly&lt;/a&gt;,
Wasm for short.
Wasm is a binary instruction format designed as a portable compilation
target for programming languages.
It allows code written in familiar languages such as C to be run in web browsers.
WebAssembly aims to execute code at native speed by taking advantage of hardware capabilities
of modern processors.
By running in a web browser, Wasm allows developers to reach a large audience with their
code without worrying about different operating systems and processor types.&lt;/p&gt;&lt;p&gt;Bed Bugs contains WebAssembly components created from bioinformatics codebases
written in C and C++, along with more standard Javascript code,
As BED files are at their heart simply tab-delimited text files, Bed Bugs
uses the robust and popular text-file parser &lt;a href=&quot;https://www.papaparse.com/&quot;&gt;PapaParse&lt;/a&gt;
to perform basic checks on the literal file contents.
In order to check the format of the user provided reference files we use
C code including the venerable &lt;code&gt;kseq.h&lt;/code&gt; header to create a WebAssembly &lt;code&gt;.fasta&lt;/code&gt;
parser.
By successfully parsing the users reference with this library we can assert that
the file is valid.
For the case of the user providing their reference as a minimap2 index (&lt;code&gt;.mmi&lt;/code&gt;)
we have a WebAssembly module compiled from custom C code that reads the header
sections of the file.
Both the &lt;code&gt;.fasta&lt;/code&gt; and &lt;code&gt;.mmi&lt;/code&gt; parsers extract the names and lengths of sequences stored within the file.&lt;/p&gt;&lt;p&gt;Having performed basic checks on the BED and reference file provided by the user,
BED Bugs applies additional contextual validations using &lt;a href=&quot;https://bedtools.readthedocs.io/en/latest/&quot;&gt;bedtools&lt;/a&gt;.
Using Bedtools to parse the BED file provides a secondary test on the file’s basic
formatting as well as checking the file is logically consistent for the purposes
of adaptive sampling.
BED Bugs checks that no two intervals in the file intersect; a circumstance which
although technically not an error in all circumstances could be not what the user
intended.
The tool also checks that all the intervals in the BED file correctly represent
valid regions of of the provided reference sequences, again using &lt;code&gt;bedtools&lt;/code&gt;.
This check intersects the user’s BED file with a BED file constructed from the &lt;code&gt;.fasta&lt;/code&gt; and &lt;code&gt;.mmi&lt;/code&gt;
WebAssembly parsers.&lt;/p&gt;&lt;h3 id=&quot;why-not-use-wasm-more&quot;&gt;Why not use Wasm more?&lt;/h3&gt;&lt;p&gt;As part of our mission to simplify bioinformatics, the EPI2ME team has previously
dabbled with Wasm to provide bioinformatics tools to our users.
We experimented with providing our bioinformatics tutorials as
WebAssembly-based web pages, before the most excellent &lt;a href=&quot;https://sandbox.bio/&quot;&gt;sandbox.bio&lt;/a&gt;
project did a better job of this than we ever could.
As part of this effort we infact contributed tools to the &lt;a href=&quot;https://github.com/biowasm/biowasm&quot;&gt;biowasm&lt;/a&gt; project.&lt;/p&gt;&lt;p&gt;Larger scale use of WebAssembly is hampered by one key issue: memory use.
The WebAssembly runtime is limited to using 4GB of memory,
this is prohibitively small for a variety of bioinformatics tasks such as
genome assembly and even alignment of reads to large-ish genomes.
Solely for this reason we have put Wasm to the side as being a curiosity,
worthy of investigation but not ultimately useful to build bioinformatics
tools for end-users.&lt;/p&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;Bed Bugs is a tools for validating user provided adaptive sampling input files for
the MinKNOW device software.
It uses technologies that allow the delivery of bioinformatics tools to users without
installation or use of a command-line environment.
We hope users find the tools useful whilst functionality is being implemented within
MinKNOW itself.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/e5312e44ad2c4508791b18a2f4b00178/59ccf/coconut-rhinoceros-beetle.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/e5312e44ad2c4508791b18a2f4b00178/coconut-rhinoceros-beetle.jpeg</content:toenail></item><item><title><![CDATA[Transition from EPI2ME Agent to EPI2ME Desktop]]></title><description><![CDATA[A guide for transitioning from the legacy EPI2ME Agent to the new EPI2ME Desktop application.]]></description><link>https://labs.epi2me.io/agent-transition</link><guid isPermaLink="false">https://labs.epi2me.io/agent-transition</guid><pubDate>Wed, 03 Jan 2024 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/aff3ca8e61cbf11493232f125a5b4a2c/59ccf/look-to-the-cloud.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;If you are a user of the legacy EPI2ME Agent for accessing EPI2ME cloud computing functionality, you will have seen the banner stating that:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;EPI2ME Agent and Portal will be replaced by a new EPI2ME local and cloud solution.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;This blog post aims to provide users with some additional explanation to help with the transition to the new platform.
The main differences are: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;a new unified application with an intuitive interface,&lt;/li&gt;&lt;li&gt;an option to run bioinformatics workflows on your own hardware or in the cloud, and&lt;/li&gt;&lt;li&gt;updated and improved workflows that use the latest analysis tools and are actively maintained.  &lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;getting-started&quot;&gt;Getting started&lt;/h2&gt;&lt;p&gt;The EPI2ME Desktop application can be freely downloaded from our &lt;a href=&quot;/downloads/&quot;&gt;Downloads&lt;/a&gt; page.
The application is rooted in providing a seemless bioinformatics experience on the desktop.
It can also branch out and run workflows using our second generation EPI2ME cloud compute environment.&lt;/p&gt;&lt;p&gt;If you need any guidance on the installation, see our &lt;a href=&quot;/installation/&quot;&gt;installation guide&lt;/a&gt;. &lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Login to the new application is recommended, and it is required for running workflows in the cloud. &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;After logging in to the application with your Oxford Nanopore Community credentials, you will be guided through any additional setup required.
Any time there is an update to the application, you will be prompted to install it within the application itself.
On the homepage you will see the dashboard with two top buttons: &lt;em&gt;View workflows&lt;/em&gt; and &lt;em&gt;Track analysis&lt;/em&gt;.
The &lt;em&gt;View workflows&lt;/em&gt; button opens a tab listing all installed and available workflows.
The &lt;em&gt;Track analyses&lt;/em&gt; button opens a tab that contains a history of your workflow runs, similar to the legacy EPI2ME Agent.
Below these buttons, you’ll find the &lt;em&gt;Latest updates&lt;/em&gt; section, with links to articles from the EPI2ME team including blog posts and how to guides.
Differences in how to run a workflow are described below. Whilst the application is designed to be intuitive, if you need any additional guidance on how to install and run a workflow see &lt;a href=&quot;/quickstart/&quot;&gt;this tutorial&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/3fde89f3be5824fe1c77b745e0d0fed9/dashboard.png&quot; alt=&quot;Dashboard&quot; title=&quot;Figure 1 - Dashboard.&quot;/&gt;&lt;/p&gt;&lt;h2 id=&quot;deprecation-of-epi2me-agent-and-legacy-workflows&quot;&gt;Deprecation of EPI2ME Agent and legacy workflows&lt;/h2&gt;&lt;p&gt;Although the majority of the pipelines available in the legacy EPI2ME Agent will no longer be supported, they have been superseded by our &lt;a href=&quot;/wfindex/&quot;&gt;best-practice workflows&lt;/a&gt; which cover the same use cases.
These workflows use up-to-date analysis tools, offer comprehensive results and are already being used extensively in the community.
They are actively maintained and continuously improved.
The &lt;a href=&quot;/wfindex&quot;&gt;new set of workflows&lt;/a&gt; make use of the &lt;a href=&quot;https://www.nextflow.io/&quot;&gt;Nextflow workflow manager&lt;/a&gt; to improve efficiency and scalability of compute both on local hardware and in the cloud.
They employ containerisation technologies such as &lt;a href=&quot;https://www.docker.com&quot;&gt;Docker&lt;/a&gt; and &lt;a href=&quot;https://docs.sylabs.io/guides/3.0/user-guide/index.html&quot;&gt;Singularity&lt;/a&gt; to allow users to run bioinformatics analyses of &lt;em&gt;anything, by anyone, anywhere&lt;/em&gt;.
All major computing platforms are supported in addition to our own cloud offering.&lt;/p&gt;&lt;h2 id=&quot;running-workflows-in-the-new-epi2me-desktop-application&quot;&gt;Running workflows in the new EPI2ME Desktop Application&lt;/h2&gt;&lt;p&gt;Each workflow available in the EPI2ME Desktop environment has documentation which provides a detailed explanation of the workflow’s capabilities, how it works and which tools it uses.
The documention is available within the application from the &lt;em&gt;Readme&lt;/em&gt; tab after opening a workflow.
There is also a &lt;em&gt;Use demo data&lt;/em&gt; button that downloads an example dataset and runs the workflow in your environment.
This is a good way to check that the setup worked and to see what to expect from the results.
The demos can be run both on your computer and in the cloud.&lt;/p&gt;&lt;p&gt;After clicking &lt;em&gt;Run this workflow&lt;/em&gt; you will be asked if you want to run it locally or in the cloud.
Minimum and recommended compute requirements for running the analysis locally (as well as approximate run times) can be found in the &lt;em&gt;Readme&lt;/em&gt; tab.
If running in the cloud, you will be prompted to confirm you are not uploading human sequence data which is not yet supported. &lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/9177675ca14ab6be6c6827c0d36ae47c/cloud_local.png&quot; alt=&quot;cloud_local&quot; title=&quot;Figure 2 - Run workflow in Cloud or locally.&quot;/&gt;&lt;/p&gt;&lt;p&gt;Once you have chosen a workflow to run, you will be guided through selecting input data and relevant parameters, with each having a short description and a help text explaining its function in more detail.  &lt;/p&gt;&lt;p&gt;Many of the new workflows have more options available than their legacy equivalents.
Most of these options are for advanced customisation of the analysis pipeline; the workflows are parameterised with default options that are suitable for standard use cases.
In the majority of cases, users need only to provide the required arguments and can leave unchanged all optional parameters.
For all required inputs you will be prompted to input a value.  &lt;/p&gt;&lt;p&gt;After launching the workflow, you will be able to visualise the progress in the &lt;em&gt;Overview&lt;/em&gt; tab.
You can also open the &lt;em&gt;Logs&lt;/em&gt; tab for further information: it displays the full output from Nextflow. &lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/b958c3e62949166164037249bafda56a/progress.png&quot; alt=&quot;Progress&quot; title=&quot;Figure 3 - Progress.&quot;/&gt;&lt;/p&gt;&lt;p&gt;Once the workflow finishes the generated output files are stored locally automatically even when the workflow is run in the cloud.
No separate download step is necessary as was sometimes the case in the legacy platform.
You can use the &lt;em&gt;Open folder&lt;/em&gt; button to open the output folder or select individual files from the &lt;em&gt;Output&lt;/em&gt; listing.
The interactive HTML report can be viewed under the &lt;em&gt;Report&lt;/em&gt; tab, in addition to being listed as one of the outputs.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/b87c1eaa2c575ed7c26abcbd67ea86d9/output.png&quot; alt=&quot;Outputs&quot; title=&quot;Figure 4 - Outputs.&quot;/&gt;&lt;/p&gt;&lt;h2 id=&quot;wimp-and-16s&quot;&gt;WIMP and 16s&lt;/h2&gt;&lt;p&gt;For users of the popular What’s In My Pot (WIMP) and 16S workflows, users can transition to the wf-metagenomics and wf-16S workflows respectively.
With these workflows you will be able to classify reads and assess species abundances.
The main difference is that whilst the previous workflows used the tools &lt;a href=&quot;https://github.com/DaehwanKimLab/centrifuge&quot;&gt;Centrifuge&lt;/a&gt; and &lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/Blast.cgi&quot;&gt;BLAST&lt;/a&gt; respectively, the replacement wf-metagenomics and wf-16S workflows both use the more efficient and accurate &lt;a href=&quot;https://ccb.jhu.edu/software/kraken2/&quot;&gt;Kraken2&lt;/a&gt; tool by default.
A &lt;a href=&quot;https://github.com/lh3/minimap2&quot;&gt;Minimap2&lt;/a&gt; option available is available also, but is not recommended for use as standard.
The workflows can also be used in real-time to classify reads as they become available.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;During the EPI2ME Desktop cloud early access period, wf-metagenomics and wf-16S cannot be used in their real-time mode.
This caveat does not apply when running the workflows on your own computer.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Unlike the legacy EPI2ME cloud offering, both workflows allow users a choice of databases. They even allow users to provide their own database for custom analyses!
For further information on selecting metagenomic databases see our guide &lt;a href=&quot;/metagenomic-databases&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;As mentioned above, in addition to the other output files the wf-metagenomics workflow also creates an HTML report containing an interactive Sankey plot and table with taxonomic classifications, as well as a sunburst plot and additional sections on diversity and antimicrobial resistance genes.
Please see the &lt;a href=&quot;/workflows/wf-metagenomics/&quot;&gt;documentation&lt;/a&gt; for further information. &lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/13f08434847de184d33ccdbe99b6988c/sankey.png&quot; alt=&quot;Sankey&quot; title=&quot;Figure 5 - wf-metagenomics sankey plot.&quot;/&gt;&lt;/p&gt;&lt;p&gt;The plots and stats related to read lengths and qualities in the QC tab of the EPI2ME agent can now be found in the read summary section of the reports produced by each workflow. &lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/9e78366735ce8ddfae555248fb58f7aa/read_summary.png&quot; alt=&quot;read_summary&quot; title=&quot;Figure 6 - Read summary.&quot;/&gt;&lt;/p&gt;&lt;h2 id=&quot;legacy-workflow-alternatives&quot;&gt;Legacy workflow alternatives&lt;/h2&gt;&lt;p&gt;For all workflows available through the EPI2ME Agent we have developed new alternatives that are fully supported and in active development.
Each workflow has associated documentation to help you learn how the workflows functions and what to expect from the outputs.
The table below acts as a guide to users to highlight which workflows available in the EPI2ME Desktop product provide equivalent functionality to the legacy cloud product.&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;EPI2ME Agent workflow&lt;/th&gt;&lt;th&gt;EPI2ME Workflow&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Fastq WIMP&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-metagenomics/&quot;&gt;wf-metagenomics&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Fastq 16s&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-16s/&quot;&gt;wf-16S&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Dorado basecalling&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-basecalling/&quot;&gt;wf-basecalling&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Fastq SV Caller for human&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-human-variation/&quot;&gt;wf-human-variation&lt;/a&gt;/&lt;a href=&quot;/workflows/wf-somatic-variation/&quot;&gt;wf-somatic-variation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Fastq Human exome&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-amplicon/&quot;&gt;wf-amplicon&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Fastq RNA Control experiment&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-alignment/&quot;&gt;wf-alignment&lt;/a&gt;/&lt;a href=&quot;/workflows/wf-transcriptomes/&quot;&gt;wf-transcriptomes&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Fastq control experiment&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-alignment/&quot;&gt;wf-alignment&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Fastq human alignment GRCh38&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-alignment/&quot;&gt;wf-alignment&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Fastq custom alignment&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-alignment/&quot;&gt;wf-alignment&lt;/a&gt;/&lt;a href=&quot;/workflows/wf-amplicon/&quot;&gt;wf-amplicon&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Fastq WIMP (Human + Viral)&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-metagenomics/&quot;&gt;wf-metagenomics&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Fastq QC + ARTIC + NextClade&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-artic/&quot;&gt;wf-artic&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Fastq clone validation&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-clone-validation/&quot;&gt;wf-clone-validation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Fastq antimicrobial resistance&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/workflows/wf-bacterial-genomes/&quot;&gt;wf-bacterial-genomes&lt;/a&gt;/&lt;a href=&quot;/workflows/wf-metagenomics/&quot;&gt;wf-metagenomics&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;reporting-issues&quot;&gt;Reporting issues&lt;/h2&gt;&lt;p&gt;If you find anything you require is missing within the EPI2ME Desktop product, or need further guidance, please &lt;a href=&quot;/contactus&quot;&gt;let us know&lt;/a&gt;.
If you encounter an error whilst running a workflow, you can use the &lt;em&gt;Report issue&lt;/em&gt; option in the workflow overview page.
If you have any questions or other issues whilst transitioning to the new application, please reach out to us via &lt;a href=&quot;mailto:support@nanoporetech.com&quot;&gt;support@nanoporetech.com&lt;/a&gt; or on &lt;a href=&quot;https://github.com/epi2me-labs&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/aff3ca8e61cbf11493232f125a5b4a2c/59ccf/look-to-the-cloud.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/aff3ca8e61cbf11493232f125a5b4a2c/look-to-the-cloud.jpg</content:toenail></item><item><title><![CDATA[An experimental extremely high-accuracy, ultra-long sequencing kit]]></title><description><![CDATA[A peak of things to come: the Genome in a Bottle GM24385 sample sequenced on PromethION with a new chemisty from Oxford Nanopore Technologies.]]></description><link>https://labs.epi2me.io/gm24385_ncm23_preview</link><guid isPermaLink="false">https://labs.epi2me.io/gm24385_ncm23_preview</guid><pubDate>Wed, 06 Dec 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/487c7dbda29f56c61e25bac2a42e5e77/59ccf/houston.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;We are pleased to announce a new experimental dataset comprising extremely high accuracy,
ultra-long sequencing reads, shared during our Nanopore Community Meeting technical update. &lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The following cell line samples were obtained from the NIGMS Human Genetic Cell
Repository at the Coriell Institute for Medical Research: GM24385&lt;/p&gt;&lt;/blockquote&gt;&lt;h3 id=&quot;data-location&quot;&gt;Data location&lt;/h3&gt;&lt;p&gt;As with previous releases the new dataset is available for anonymous download from
an Amazon Web Services S3 bucket. The bucket is part of the &lt;a href=&quot;https://aws.amazon.com/opendata/&quot;&gt;Open Data on AWS&lt;/a&gt;
project enabling sharing and analysis of a wide range of data.&lt;/p&gt;&lt;p&gt;The data is located in the bucket at:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;s3://ont-open-data/gm24385_2023.12/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;See the &lt;a href=&quot;/tutorials/&quot;&gt;tutorials&lt;/a&gt; page for information on downloading the dataset.&lt;/p&gt;&lt;h3 id=&quot;sample-preparation-and-analysis&quot;&gt;Sample preparation and analysis&lt;/h3&gt;&lt;p&gt; Ultra-long libraries of native DNA from GM24385 (HG002) were prepared using a modified
Ultra-Long DNA Sequencing Kit V14 motor protein and experimental high-accuracy run conditions. Sequencing
was performed on a PromethION instrument to obtain 125 Gbp of sequencing data
passing quality filters (read Q-score &amp;gt; Q10, &lt;a href=&quot;/quality-scores/#guppy-read-q-scores-and-read-accuracies&quot;&gt;see here&lt;/a&gt;),
with a read length N50 of 91 kbp. This data was basecalled using a
bespoke dorado model to yield a median accuracy of Q26.4.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The per-base quality scores produced by the experimental basecaller model have not been calibrated and may
not reflect the empirical accuracy of the called bases.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt; Read alignment accuracy was measured against
the telomere-to-telomere consortium’s HG002 &lt;a href=&quot;https://github.com/marbl/HG002&quot;&gt;reference sequence&lt;/a&gt; using the
&lt;code&gt;bamstats&lt;/code&gt; tool from the &lt;a href=&quot;https://github.com/epi2me-labs/fastcat&quot;&gt;fastcat&lt;/a&gt; package. The histogram outputs
of &lt;code&gt;bamstats&lt;/code&gt; was used to produce the plots in Fig. 1.&lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;statsPlot&quot; plotCaption=&quot;Figure 1. Sequencing summary metrics for a new experimental Oxford Nanopore Technologies sequencing chemisty. Alignment accuracy was measure with the bamstats program for the fastcat suite.&quot;&gt;&lt;/div&gt;&lt;h3 id=&quot;genome-assembly&quot;&gt;Genome assembly&lt;/h3&gt;&lt;p&gt;With this dataset, parental sequencing reads, and the &lt;a href=&quot;https://github.com/chhylp123/hifiasm&quot;&gt;Hifiasm&lt;/a&gt; and
&lt;a href=&quot;https://github.com/at-cg/RAFT&quot;&gt;RAFT&lt;/a&gt; tools, a diploid human genome assembly was contructed.
The assembly included 19 telomere-to-telomere chromosomes. We hope
release of this dataset will spurn innovation from assembly algorithm developers
as well as serve as a resource for researchers studying the most complicated and
inaccessible reaches of the human genome.&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;Size / Gbp&lt;/th&gt;&lt;th&gt;Scaffold N50 / Mbp&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Maternal&lt;/td&gt;&lt;td&gt;3.03&lt;/td&gt;&lt;td&gt;135&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Paternal&lt;/td&gt;&lt;td&gt;2.94&lt;/td&gt;&lt;td&gt;133&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:553px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:106.3157894736842%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAYAAABG1c6oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEZklEQVR42o2Ui2+TZRTG+9eQoIIkwCYJ8RLFmJhIIJhAcEYFNCZElBAjiUZJwKgJMCEbls3RrZvAuhvr/ba2Y2Nru7Zru3Z0F9nG1nXMdaVbt5Zefr5fWyaYQDjJyfl6mjzv857zPK8sm82iMtmYXFpFiny+wIvEaiaLf3yW+bk5luLxjb4s+yjDWXkjnaFYsVEoFCiU6/NiLZPDH44RnYuSTCZJpVKk02lkjwTg+SYVDf2j9IcnmFp6SC6XLYFLjKUDCqXvJyMtbuabuMfM9AyRSIREIsH6+jqyxMME5xU3qLcPc9XiRmlzoQ1OMbOS3gB9VuTFwYV8/qnbyNz+ACpnGEVfiAabh+6BYc6YAhzscDMdX2MqsUo0scKKuOLjkeTyhWceJLM5XZy0T1Bt8dBsG6K938fPlgD7VW5OmcK8KtdTo7WhHn3AYir9FOfiKP43a9nw3Qh7VXd455qec+YAugEfP5n8HGgf4oQ+yKbL3SgMdk73jPFJsx6D1Uqd6y7hf5JkcvkN1huAUnNvax/brhrZ1eCg1TFITV+Ej7q8nDSF2Fyr5ZLazJtKB0db9Fy+coVd1R1UKOz82BtifDbKmtjuauYRsURSbLkMuONPMy9d0VHV3kdVl5s9Lb1ijk4qGsy8r7Syq9HG8ZtG5PX17G8y84N9ks1yI792GRkJ+gWRO1x3hx4D3mZ7nYlNlzTsFvUVUbfWaKmUG9hSq2ObyJ31Zg406rjYdIPdtWp2Nzp4Q2Hl9QYjv7drOKPS8lt/GJl0fc1YjLbR+3jnlwksJPDFlosZWFjGK+qwSL/oe+fjQqdJAuK3M7okelKNM76YELnMbHINmTTIiwNTnDL7S9oqvJj1nhVFwAuDM1S12hmZmiWRyRelIQFLC8sKzUmZK+svX67FXjnz5f+lZ0CWSq1R7Y5ySKlHaXVzTD9Cx6jwZyb7lFOkmnsO+6JNJYa+YIhLnnkOKg1cd3g51OHlC3WAw52D3AxNMxlPsSo8ulI+gKK3//P34xH5YnEU/nvInN5han0xDguGbb0eDnb5OGkMUVlnLm76K62fE5ev8Y1+iIXVdQGQf8IphQ1x2/5e4KeeILJepxu5b4EPmoxctPio6pYAw0J/ViqFBndcNfBtXQsVQqdH/jLg9gwxEV0gmc6WX6ISw/H5GFrvSEk2cs8cW4TmPu70cUTt40udn50CYJ9qkPeEwKu7hYsUPRxRarFaDCh1Jr7XuQiNRXDeX2QlW2D+wSKukVBpy/XeEuBngt1xvZ9tch3b6418rvGxr3WAPQojrylsHG3WYreZOKe2c8EawGLUcOCaju9MHmpMvfwi3lSZtP5P1R5e/sPAh21DwsMeAWYR3jZRKVhuFfbaKjfxlrKX09ctaMw23m12cOyWm6+7HFQ02nlbHH62zcLZ/rviyoKhJTKNOjAmcgK9qMaRSTSBcW55R8VbGaDTG6bbFcAg3sqeoRA3+zy0OIMobntp6Bmk0xWk+7YH59g9/gUusJhdioyBVgAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;ncm_assembly&quot; title=&quot;Assembly karyogram from high-accuracy ultralong nanopore sequencing data.&quot; src=&quot;/static/e4b16ed2a9558dfc256adcd6dbd99e9a/74cfa/hifi-ncm.png&quot; srcSet=&quot;/static/e4b16ed2a9558dfc256adcd6dbd99e9a/0e2fe/hifi-ncm.png 285w,/static/e4b16ed2a9558dfc256adcd6dbd99e9a/74cfa/hifi-ncm.png 553w&quot; sizes=&quot;(max-width: 553px) 100vw, 553px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Assembly karyogram from high-accuracy ultralong nanopore sequencing data.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;h3 id=&quot;further-information&quot;&gt;Further information&lt;/h3&gt;&lt;p&gt;For additional information regarding these data please contact &lt;a href=&quot;mailto:support@nanoporetech.com&quot;&gt;support@nanoporetech.com&lt;/a&gt;.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/487c7dbda29f56c61e25bac2a42e5e77/59ccf/houston.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/487c7dbda29f56c61e25bac2a42e5e77/houston.jpeg</content:toenail></item><item><title><![CDATA[How to build and use databases to run wf-metagenomics and wf-16s offline]]></title><description><![CDATA[A brief guide in building and using metagenomic databases]]></description><link>https://labs.epi2me.io/how-to-meta-offline</link><guid isPermaLink="false">https://labs.epi2me.io/how-to-meta-offline</guid><pubDate>Tue, 17 Oct 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/29210ec0e5360fd5b5dad8b62f87bac0/59ccf/metagenomics-offline.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Headed for the unknown like Amundsen in the Antarctic? Determined to unveil the vast microbial world hidden in such an isolated landscape? Looking forward to sequencing what mysterious life may dwell in these most extreme niches? You are packing winter gear into suitcases, getting ready for your biggest scientific adventure. But then the realisation hits you… there is no internet at the South Pole!&lt;/p&gt;&lt;p&gt;To decipher the microbial sequencing data generated from an environmental sample, the current approach, roughly speaking, is to compare each read with existing sequences hosted in a database. These reference sequences have been curated and assigned to specific organisms in the past. Naturally, the composition and size of this database can influence the results of the analysis. Therefore, choosing an appropriate database should be the first step to run the workflow offline. But be warned, these databases may require a large amount of disk space, sometimes in excess of 500GB! This is why they are usually stored on servers and access to them requires an internet connection.&lt;/p&gt;&lt;p&gt;Here we explain how to prepare these databases before you set off on an exciting expedition to somewhere the internet does not reach. Or perhaps just a compute cluster node without access to the internet. &lt;/p&gt;&lt;p&gt;But before we delve into database building, let’s recap some background. There are two main modes of running our &lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot;&gt;wf-metagenomics&lt;/a&gt; and &lt;a href=&quot;https://github.com/epi2me-labs/wf-16s&quot;&gt;wf-16s&lt;/a&gt; workflows: either with &lt;a href=&quot;https://github.com/DerrickWood/kraken2&quot;&gt;Kraken2&lt;/a&gt; or using &lt;a href=&quot;https://github.com/lh3/minimap2&quot;&gt;minimap2&lt;/a&gt;. In both cases you can use one of the default databases or your own custom database (online or offline; for more info see &lt;a href=&quot;https://labs.epi2me.io/metagenomic-databases/&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&lt;h2 id=&quot;the-structure-and-composition-of-the-databases&quot;&gt;The structure and composition of the databases&lt;/h2&gt;&lt;p&gt;In both pipelines, there are two main steps in terms of classifying a sequence taxonomically. In brief, reads are first mapped against a database to find the most similar sequence and then they ‘inherit’ the taxonomy of this matched reference.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:34.73684210526316%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Databases&quot; title=&quot;Figure 1 - Different types of databases that are needed to run the workflow.&quot; src=&quot;/static/3b86d43d1d03ca55455ad5ab4e0be948/b5cea/databases.png&quot; srcSet=&quot;/static/3b86d43d1d03ca55455ad5ab4e0be948/0e2fe/databases.png 285w,/static/3b86d43d1d03ca55455ad5ab4e0be948/432e7/databases.png 570w,/static/3b86d43d1d03ca55455ad5ab4e0be948/b5cea/databases.png 1140w,/static/3b86d43d1d03ca55455ad5ab4e0be948/09ede/databases.png 1710w,/static/3b86d43d1d03ca55455ad5ab4e0be948/d50e7/databases.png 2280w,/static/3b86d43d1d03ca55455ad5ab4e0be948/e1a32/databases.png 4400w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 1 - Different types of databases that are needed to run the workflow.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;Before explaining how to generate them, let’s have a quick look at the different types of databases the workflows need (Fig. 1).&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;The taxonomy database&lt;/em&gt;&lt;/strong&gt; contains the organism names and the phylogenetic hierarchy of each taxon. This means, for a given species, it contains the information of its “parent” ranks as well as their taxonomic identifiers (henceforth referred to as “TaxID”). The &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/books/NBK53758/&quot;&gt;TaxID&lt;/a&gt; is a unique identifier for each taxonomic entry. The &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/taxonomy&quot;&gt;NCBI Taxonomy database&lt;/a&gt; can be downloaded &lt;a href=&quot;https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/&quot;&gt;here&lt;/a&gt; and contains different files whose content is described in &lt;code&gt;taxdump_readme.txt&lt;/code&gt;. To briefly mention a couple of files within the database:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;the &lt;strong&gt;nodes.dmp&lt;/strong&gt; file represents taxonomy nodes. It includes the TaxID of each node, and also the TaxID of the parent nodes as well as their rank.&lt;/li&gt;&lt;li&gt;the &lt;strong&gt;names.dmp&lt;/strong&gt; file includes the scientific name of each TaxID.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The workflow accepts as inputs: a folder, a &lt;code&gt;.tar.gz&lt;/code&gt; file, or a &lt;code&gt;.zip&lt;/code&gt; file.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;If you wish to use a taxonomy database different from the one provided by the NCBI Taxonomy, you might want to use &lt;a href=&quot;https://bioinf.shenwei.me/taxonkit/usage/#create-taxdump&quot;&gt;taxonkit&lt;/a&gt; to create NCBI-style taxdump files for custom taxonomy.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;The minimap2 database&lt;/em&gt;&lt;/strong&gt; is a file particular to the &lt;code&gt;minimap2&lt;/code&gt; program (&lt;a href=&quot;https://academic.oup.com/bioinformatics/article/34/18/3094/4994778&quot;&gt;Li, 2018&lt;/a&gt;). You can read about &lt;code&gt;minimap2&lt;/code&gt; in one of our previous &lt;a href=&quot;https://labs.epi2me.io/how-to-align/&quot;&gt;posts&lt;/a&gt;; it is a tool to align reads against a reference. A reference file provided as a FASTA file must first be converted to a minimap index: an MMI file. To create an MMI from a FASTA file you can run the following command:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;minimap2 -x map-ont -d reference.mmi reference.fa
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To use this index within wf-metagenomics you also need a &lt;code&gt;ref2taxid&lt;/code&gt; file. This is a tab-delimited file containing the TaxID for each reference sequence in the reference file. We will discuss how to create this below.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;The Kraken2 database&lt;/em&gt;&lt;/strong&gt; is used with the Kraken2 (&lt;a href=&quot;https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0&quot;&gt;Wood &lt;em&gt;et al&lt;/em&gt;., 2019&lt;/a&gt;) program for assigning taxonomic labels. Kraken2 uses a &lt;a href=&quot;https://en.wikipedia.org/wiki/K-mer&quot;&gt;k-mer&lt;/a&gt;-based approach which drastically reduces memory usage as well time requirements of the taxonomic classification.&lt;/p&gt;&lt;p&gt;A Kraken 2 database is a directory containing at least 3 files:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;hash.k2d&lt;/code&gt;: Contains the minimizer to taxon mappings.&lt;/li&gt;&lt;li&gt;&lt;code&gt;opts.k2d&lt;/code&gt;: Contains information about the options used to build the database.&lt;/li&gt;&lt;li&gt;&lt;code&gt;taxo.k2d&lt;/code&gt;: Contains taxonomy information used to build the database (the taxonomy database).&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Apart from these files, you will also neeed an extra file to run &lt;a href=&quot;https://github.com/jenniferlu717/Bracken&quot;&gt;bracken&lt;/a&gt;. Bracken (&lt;a href=&quot;https://peerj.com/articles/cs-104/&quot;&gt;Lu &lt;em&gt;et al&lt;/em&gt;., 2017&lt;/a&gt;) is a complementary tool to Kraken2 that uses the classifications to estimate the abundances of each organism at a specific taxonomic rank.&lt;/p&gt;&lt;p&gt;The workflow accepts as inputs: a folder and a &lt;code&gt;.tar.gz&lt;/code&gt; file. &lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;An essential point when using any of the databases above is that we must have a coherent set of files. Importantly the taxonomy and database reference set must match in their labellings of sequences.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Now that we have covered the relevant background information, I will use the following lines to explain:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;How to store the default databases so that they can be used offline.&lt;/li&gt;&lt;li&gt;How to obtain pre-built Kraken2 databases.    &lt;/li&gt;&lt;li&gt;How to make your own database from scratch.    &lt;/li&gt;&lt;li&gt;How to build the SILVA database for Kraken2 and minimap2.&lt;/li&gt;&lt;/ol&gt;&lt;h2 id=&quot;storing-a-database-the-easy-way&quot;&gt;Storing a database the easy way&lt;/h2&gt;&lt;p&gt;Now, that we know which files are needed to run the workflow, let’s continue with how to prepare these files.&lt;/p&gt;&lt;p&gt;First, a solution without headache. If you wish to use one of the databases provided by wf-metagenomics (‘ncbi_16s_18s’, ‘ncbi_16s_18s_28s_ITS’, ‘SILVA_138_1’, ‘PlusPF-8’, ‘PlusPFP-8’), the easiest option is run the workflow while connected to the internet and store the databases to your device.&lt;/p&gt;&lt;p&gt;For saving the database to device, use the the &lt;code&gt;--database_set&lt;/code&gt; option (“Choose a database” under “Reference Options” in the EPI2ME Desktop application) to specify the database you wish to download and use. You also have to specify a location with &lt;code&gt;--store_dir&lt;/code&gt; (“Store directory name” in the application) where a copy of the database will be kept for later use.&lt;/p&gt;&lt;p&gt;If you have cloned the &lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot;&gt;GitHub repository&lt;/a&gt;, you can use the toy dataset in &lt;code&gt;test_data&lt;/code&gt; to quickly run the workflow and trigger the database download:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;nextflow run wf-metagenomics/main.nf --fastq wf-metagenomics/test_data/case01 --database_set ncbi_16s_18s --store_dir favourite_path_to_store_db
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Alternatively, in the EPI2ME application you can:    &lt;/p&gt;&lt;ol&gt;&lt;li&gt;Go to &lt;code&gt;Run this workflow &amp;gt; Setup and run &amp;gt; Reference Options&lt;/code&gt;.     &lt;/li&gt;&lt;li&gt;Choose your favourite database (Figure 2).&lt;br/&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:45.26315789473684%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABcElEQVR42nVRy07DMBDMP1JKuXDi0t8AlQMgUSrES0JC6qchgWganPgRP9Nh101LW4nDyM7O7nhnUgwm9ziezHB0cYfB5RSj62ec3r5mjG5ecDZ9w/hxjvHTHOcP71tuww9plueHVzOcEIqmtbAhoVxWKBclVCOB1WqLVdehSwkrBt13Oa575+BjgnMeIXgU5VLkDyFqtCTOvR03MzoS5AJ2ddY15pkJJNbQElobWNIptLFwMUC1JhMxxowQIgl2MKbNde8DUuoIifi0Pvs7gx/3IaCw1sOnCG1b1HWThx3ZYDFuElT7LimOSpCLhnpkrjF+BKPOpyF34U8wZEGlNBxtwgS/zpYsiStupu02dg/B9hMtsLeh6Td03mc7jGzDx5xx6lIe+g+RHswZOhJ09HdqpbAga4IsMcEIlOXnosLHVwmpDJRu9yB1X6OcHWXeWseCAcY7CClRURaiUWikRk1gC7If5M03Dx2C82Mx3vAXKcKh+gp1FS4AAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;databases_menu&quot; title=&quot;Figure 2 - How to choose between default databases in EPI2ME.&quot; src=&quot;/static/e4d5bea9231e9834a436773bb145396f/b5cea/choose_db.png&quot; srcSet=&quot;/static/e4d5bea9231e9834a436773bb145396f/0e2fe/choose_db.png 285w,/static/e4d5bea9231e9834a436773bb145396f/432e7/choose_db.png 570w,/static/e4d5bea9231e9834a436773bb145396f/b5cea/choose_db.png 1140w,/static/e4d5bea9231e9834a436773bb145396f/09ede/choose_db.png 1710w,/static/e4d5bea9231e9834a436773bb145396f/09262/choose_db.png 1896w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 2 - How to choose between default databases in EPI2ME.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/li&gt;&lt;li&gt;Select a place to store it. (Figure 3).&lt;br/&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1118px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:22.45614035087719%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAAl0lEQVR42m2P0Q6EIAwE/f8fPRWhKCgilL1S3wwPk21CM12m80r4zQbzIqwWmyXcuSDdWbnzg1Mypoz8lCFPqcI7T5csO9phycNsFkaE6+ZUvBg54DwoJphwqXwkDPGUt6zSqbcIIYJEuBoD6wjkd+xH0CR/IIuo9gZdOMDK0f5TbVhqRWUGt4YGaH4plaF7kiOY2zsz4w/mlTcwYX5QnQAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;databases_store&quot; title=&quot;Figure 3 - Set a path to store the databases in EPI2ME.&quot; src=&quot;/static/85940a503aefe18037eaabbf385f3534/0f246/stored_path.png&quot; srcSet=&quot;/static/85940a503aefe18037eaabbf385f3534/0e2fe/stored_path.png 285w,/static/85940a503aefe18037eaabbf385f3534/432e7/stored_path.png 570w,/static/85940a503aefe18037eaabbf385f3534/0f246/stored_path.png 1118w&quot; sizes=&quot;(max-width: 1118px) 100vw, 1118px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 3 - Set a path to store the databases in EPI2ME.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/li&gt;&lt;li&gt;Run the workflow with Kraken2 or minimap2. You could use the test data: go to the working directory (e.g.: C:\Users\xxxx\epi2melabs), you can look where it is in the &lt;code&gt;Settings&amp;gt;Configuration Options&amp;gt;Working Directory&lt;/code&gt;), and within it: workflows\epi2me-labs\wf-metagenomics\test_data\case01).     &lt;/li&gt;&lt;li&gt;You can now run the workflow offline by setting the &lt;code&gt;store_dir&lt;/code&gt; path to the folder selected above.    &lt;/li&gt;&lt;/ol&gt;&lt;h2 id=&quot;standard-kraken2-databases&quot;&gt;Standard Kraken2 databases&lt;/h2&gt;&lt;p&gt;Kraken2 supports a wide range of prebuilt databases that can be downloaded from &lt;a href=&quot;https://benlangmead.github.io/aws-indexes/k2&quot;&gt;here&lt;/a&gt;. The selection includes databases of varying sizes and including different organisms. You can inspect the content of the database using the command &lt;code&gt;kraken2-inspect&lt;/code&gt; (available with Kraken2). Larger databases contain more sequences and may provide more accurate results. On the other hand, though, they also require more computing resources (if you are worried about compute requirements, you can look into mitigating options like &lt;code&gt;--kraken2_memory_mapping&lt;/code&gt;).&lt;/p&gt;&lt;p&gt;To download one of the databases (e.g. PlusPF-8), run:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;wget https://genome-idx.s3.amazonaws.com/kraken/k2_pluspf_08gb_20231009.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These prebuilt databases also include the corresponding bracken files and are packaged as &lt;code&gt;.tar.gz&lt;/code&gt; archives. To uncompress them in Windows, you can right-click on the file and copy the path of the directory (&lt;code&gt;Properties&amp;gt;Location&lt;/code&gt;). Then, open Windows Powershell and run the following commands:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# Move to the folder where the file is:
cd path/to/directory_with_k2_index
# Create a directory to uncompress the file
mkdir k2_index
# Uncompress the file
tar -xzf k2_index.tar.gz -C .\k2_index\ 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now, you can run the workflow with the new database (and remember to also provide the taxonomy database):&lt;/p&gt;&lt;pre&gt;&lt;code&gt;nextflow run wf-metagenomics/main.nf --database k2_index --taxonomy path_to_taxonomy_database
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These prebuilt kraken2 databasee use the NCBI taxonomy described in &lt;a href=&quot;#the-structure-and-composition-of-the-databases&quot;&gt;section above&lt;/a&gt;. You can download it with the next command. For other databases, you will need to find out which taxonomy database they use (see below for SILVA database).&lt;/p&gt;&lt;pre&gt;&lt;code&gt;wget https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/new_taxdump.tar.gz
mkdir NCBI_taxdump
tar -xf new_taxdump.tar.gz -C NCBI_taxdump
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;wandering-off-the-beaten-track&quot;&gt;Wandering off the beaten track&lt;/h2&gt;&lt;p&gt;If you are feeling adventurous, you can also use your own database! You can build custom databases by following the instructions described &lt;a href=&quot;https://github.com/jenniferlu717/Bracken#step-0-build-a-kraken-1krakenuniqkraken2-database&quot;&gt;here&lt;/a&gt; for Kraken2 and &lt;a href=&quot;https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown#custom-databases&quot;&gt;here&lt;/a&gt; for the bracken file. To illustrate how to do it, we will walk through how the included &lt;code&gt;ncbi_16s_18s&lt;/code&gt; database resources were created by:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;downloading the taxonomy database (required in both pipelines),&lt;/li&gt;&lt;li&gt;creating a minimap2 index, and&lt;/li&gt;&lt;li&gt;creating a Kraken2 database.   &lt;/li&gt;&lt;/ol&gt;&lt;h3 id=&quot;the-ncbi-refseq-targeted-loci-project&quot;&gt;The NCBI RefSeq Targeted Loci Project&lt;/h3&gt;&lt;p&gt;These databases are specific for targeted loci, which act as specific molecular markers (16S rDNA, 18S rDNA (SSU), 28S rDNA (LSU) gene and internal transcribed spacer (ITS)) that are used for phylogenetic and barcoding analysis. They are described &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/refseq/targetedloci/&quot;&gt;here&lt;/a&gt;. You can also find the different projects for Bacteria, Archaea and Fungi at the &lt;a href=&quot;https://ftp.ncbi.nlm.nih.gov/refseq/TargetedLoci/&quot;&gt;NCBI RefSeq Targeted Loci FTP site&lt;/a&gt;.&lt;/p&gt;&lt;h4 id=&quot;the-taxonomy-database&quot;&gt;The taxonomy database&lt;/h4&gt;&lt;p&gt;In this case, we will use the NCBI taxonomy, so we need to download it from &lt;a href=&quot;https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/&quot;&gt;here&lt;/a&gt;, as it has been described in the Standard Kraken2 databases.&lt;/p&gt;&lt;h4 id=&quot;the-minimap2-reference-files&quot;&gt;The minimap2 reference files&lt;/h4&gt;&lt;p&gt;Here we show how to create a reference file (and its corresponding ref2taxid file) for the minimap2 pipeline. In this case, we build a reference that contains 16S rDNA and 18S rDNA sequences from the NCBI RefSeq Targeted Loci projects, but you can use your favourite sequences to create a FASTA/MMI file and its matching ref2taxid to use as inputs.&lt;/p&gt;&lt;p&gt;The process works as follows:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;## 1. Download the targeted loci projects and concatenate the files:
wget https://ftp.ncbi.nlm.nih.gov/refseq/TargetedLoci/Archaea/archaea.16SrRNA.fna.gz  # Archaea 16S
wget https://ftp.ncbi.nlm.nih.gov/refseq/TargetedLoci/Bacteria/bacteria.16SrRNA.fna.gz  # Bacteria 16S
wget https://ftp.ncbi.nlm.nih.gov/refseq/TargetedLoci/Fungi/fungi.18SrRNA.fna.gz  # Fungi 18S

cat *rRNA.fna.gz &amp;gt; ncbi16s18srRNA.bacteria_archaea_fungi.fna.gz 

## 2. Create an MMI index for the minimap2 pipeline (optional)
minimap2 -x map-ont -d ncbi16s18srRNA.bacteria_archaea_fungi.mmi ncbi16s18srRNA.bacteria_archaea_fungi.fna
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To create the ref2taxid file for NCBI sequences it can be downloaded again from &lt;a href=&quot;https://ftp.ncbi.nih.gov/pub/taxonomy/accession2taxid/&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;## 3. Download and prepare the ref2taxid TSV file
wget https://ftp.ncbi.nih.gov/pub/taxonomy/accession2taxid/nucl_gb.accession2taxid.gz

### As this file may contain unneeded entries, we extract only those of the sequences included in the FASTA file.
grep -oP &amp;#x27;^&amp;gt;[^\s]+&amp;#x27; ncbi16s18srRNA.bacteria_archaea_fungi.fna | sed &amp;#x27;s/^&amp;gt;//&amp;#x27; &amp;gt; accessions.txt

### filter the records using grep
zcat nucl_gb.accession2taxid.gz grep -w -f accessions.txt \
    &amp;gt; hit.acc2taxid.tsv

### pick the relevant columns (&amp;#x27;accession.version&amp;#x27; and &amp;#x27;taxid&amp;#x27;)
cut -f2,3 hit.acc2taxid.tsv &amp;gt; ref2taxid.targloci.tsv
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;the-kraken2-database-files&quot;&gt;The Kraken2 database files&lt;/h4&gt;&lt;p&gt;From these files we can build the corresponding Kraken2 database. When building a &lt;a href=&quot;https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown#custom-databases&quot;&gt;custom Kraken2 database&lt;/a&gt; you will obtain two main folders during the process: The &lt;strong&gt;library&lt;/strong&gt; and the &lt;strong&gt;taxonomy&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;The &lt;strong&gt;&lt;em&gt;library&lt;/em&gt;&lt;/strong&gt; contains the reference libraries. Some of them are available from the &lt;code&gt;kraken2-build&lt;/code&gt; command (see &lt;a href=&quot;https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown#custom-databases&quot;&gt;here&lt;/a&gt;). Alternatively, we can also use the files generated above (so that we’re using the same database as in the minimap2 workflow).&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# prepare header of the FASTA file to contain TaxIDs: kraken:taxid|XXX
cut -d$&amp;#x27;\t&amp;#x27; -f1,2 ref2taxid.targloci.tsv --output-delimiter=&amp;#x27;|kraken:taxid|&amp;#x27; &amp;gt; new_name.txt
cut -f1 ref2taxid.targloci.tsv | paste - new_name.txt &amp;gt; alias.txt
seqkit replace \
    -p &amp;#x27;^(\S+)(.+?)$&amp;#x27; \
    -r &amp;#x27;{kv}&amp;#x27; ncbi16s18srRNA.bacteria_archaea_fungi.fna \
    -k alias.txt ncbi16s18srRNA.bacteria_archaea_fungi.fna \
    &amp;gt; ncbi16s18srRNA.bacteria_archaea_fungi.rename.fna

# build the database using the option `add-to-library` to use your sequences
kraken2-build \
    --add-to-library ncbi16s18srRNA.bacteria_archaea_fungi.rename.fna \
    --db kraken2_database/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;strong&gt;&lt;em&gt;taxonomy&lt;/em&gt;&lt;/strong&gt; contains the taxonomy database. This can be the same folder that we have downloaded and uncompressed previously:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;mv NCBI_taxdump/ kraken2_database/taxonomy
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Alternatively, you can also download the NCBI taxonomy with the next &lt;a href=&quot;https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown#custom-databases&quot;&gt;command&lt;/a&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;kraken2-build --download-taxonomy --db kraken2_database
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With both directories, we are ready to &lt;strong&gt;build the Kraken2 database&lt;/strong&gt;: &lt;/p&gt;&lt;pre&gt;&lt;code&gt;# build the database
kraken2-build --build --db kraken2_database
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once the Kraken2 database has been built, you will need to create the &lt;strong&gt;bracken additional file&lt;/strong&gt; using the &lt;a href=&quot;https://github.com/jenniferlu717/Bracken#step-1-generate-the-bracken-database-file-databasexmerskmer_distrib&quot;&gt;following command&lt;/a&gt;: &lt;/p&gt;&lt;pre&gt;&lt;code&gt;bracken-build -d kraken2_database/ -l 1000
generate_kmer_distribution.py \
    -i kraken2_database/database1000mers.kraken \
    -o database1000mers.kmer_distrib
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;run-wf-metagenomics-or-wf-16s-using-these-databases&quot;&gt;Run wf-metagenomics or wf-16S using these databases:&lt;/h4&gt;&lt;p&gt;After all this we are finally ready to run wf-metagenomics with our custom database!&lt;/p&gt;&lt;p&gt;Using Kraken2:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;nextflow run ~/workflows/wf-metagenomics/main.nf \
    --fastq ~/workflows/wf-metagenomics/test_data/case01/ \
    --taxonomy NCBI_taxdump/ \
    --database kraken2_database \
    --bracken_dist kraken2_database/database1000mers.kmer_distrib
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Using minimap2:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;nextflow run ~/workflows/wf-metagenomics/main.nf \
    --fastq ~/workflows/wf-metagenomics/test_data/case01/
    --classifier minimap2 \
    --taxonomy NCBI_taxdump/ \
    --reference ncbi16s18srRNA.bacteria_archaea_fungi.fna \
    --ref2taxid ref2taxid.targloci.tsv 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;In EPI2ME&lt;/strong&gt; you can set the path to your databases as shown in Figure 4. The relevant options are in the &lt;code&gt;Reference Options&lt;/code&gt; section.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:29.122807017543863%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAA0UlEQVR42i2QC5KEIBBDvf85t9ZSR3GApvkI7EPWiq2GJEQWCXp/rXNSHq76tKox/azbuh9OAqtM5ydkvnjRXJ6Uy7Jf1++6b8dpbnsaYgR2M3bdP5e5ueGDpqDxBS8JMzL8iwsjkt3KG8akQs45xpRyhoF7aqUUjzn/lQ9m8cZ8nfcEI++9I4UBqhHJCEoDI7e86e8fttYwy3kZzHNxmjmF43PS+L6ttY7eXoebIJqwRwg6zD6EcV5e+NYYK1zr6EREdZQBkIQyBnqvteJvrf8BMJNbeISwZvEAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Databases_setpath&quot; title=&quot;Figure 4 - Set the path to your databases in EPI2ME.&quot; src=&quot;/static/800161af92534374e9f9dd583ef2d5e6/b5cea/custom_db.png&quot; srcSet=&quot;/static/800161af92534374e9f9dd583ef2d5e6/0e2fe/custom_db.png 285w,/static/800161af92534374e9f9dd583ef2d5e6/432e7/custom_db.png 570w,/static/800161af92534374e9f9dd583ef2d5e6/b5cea/custom_db.png 1140w,/static/800161af92534374e9f9dd583ef2d5e6/b2b2c/custom_db.png 1708w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 4 - Set the path to your databases in EPI2ME.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;To create the &lt;code&gt;ncbi_16s_18s_28s_ITS&lt;/code&gt; database we followed the same procedure, but also added the ITS project and 28S ribosomal RNA projects for Fungi from the Targeted Loci site.&lt;/em&gt;&lt;/p&gt;&lt;h2 id=&quot;the-silva-database&quot;&gt;The SILVA database&lt;/h2&gt;&lt;p&gt;The &lt;a href=&quot;https://www.arb-silva.de/&quot;&gt;SILVA database&lt;/a&gt; provides updated datasets of small (16S/18S, SSU) and large subunit (23S/28S, LSU) ribosomal RNA (rRNA) sequences for all three domains of life (Bacteria, Archaea and Eukarya). It does not provide species-level resolution, genus is the lowest taxonomic rank that can be used. You can use this database in wf-metagenomics by setting &lt;code&gt;--database_set&lt;/code&gt; to &lt;code&gt;SILVA_138_1&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;We have included SILVA as one of the default databases of the workflow and you can save it locally for running offline by following the instructions in the &lt;a href=&quot;#storing-a-database-the-easy-way&quot;&gt;Storing a database the easy way&lt;/a&gt; section above. If you want to build the database yourself there are two ways: either using the NCBI TaxIDs or using the IDs provided by the SILVA project itself (the files can be downloaded from &lt;a href=&quot;https://www.arb-silva.de/no_cache/download/archive/current/Exports/&quot;&gt;here&lt;/a&gt;). In the latter case you also need to build a custom taxonomy database using the SILVA TaxIDs.&lt;/p&gt;&lt;p&gt;When we built the SILVA database, we used the &lt;a href=&quot;https://github.com/DerrickWood/kraken2/wiki/Manual#16s-databases&quot;&gt;method recommended by Kraken2&lt;/a&gt; using SILVA TaxIDs. This process will also download the FASTA file that can be used as reference for the minimap2 subworkflow.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# Build SILVA Kraken2 database
kraken2-build --db SILVA_138_1 --special silva
# Create bracken files
bracken-build -d SILVA_138_1 -l 1000
generate_kmer_distribution.py \
    -i SILVA_138_1/database1000mers.kraken \
    -o database1000mers.kmer_distrib
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now you should be ready to prepare your databases to run the workflow offline!    &lt;/p&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;In this post we have surveyed a variety of methods for obtaining and storing metagenomic databases. Once obtained the databases can be used (and reused) in an offline setting. &lt;/p&gt;&lt;p&gt;The post is not intended as a comprehensive user guide to the various programs used. Please see their respective documentation and command-line instructions for additional information. We hope this information proves useful to the community.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;List of dependencies&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/lh3/minimap2&quot;&gt;minimap2&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/DerrickWood/kraken2&quot;&gt;kraken2&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/jenniferlu717/Bracken&quot;&gt;bracken&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/shenwei356/taxonkit&quot;&gt;taxonkit&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/shenwei356/seqkit&quot;&gt;seqkit&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These commands have been used in a linux environment except where otherwise indicated.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/29210ec0e5360fd5b5dad8b62f87bac0/59ccf/metagenomics-offline.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/29210ec0e5360fd5b5dad8b62f87bac0/metagenomics-offline.jpeg</content:toenail></item><item><title><![CDATA[EPI2ME 23.10-01 Release]]></title><description><![CDATA[Proxy support added to the EPI2ME Desktop Application.]]></description><link>https://labs.epi2me.io/epi2me-23.10-01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-23.10-01-release</guid><pubDate>Fri, 06 Oct 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/bedd0c862206f01cea08182f8d5470d8/59ccf/peacock.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Dear Nanopore Community,&lt;/p&gt;&lt;p&gt;We are delighted to release a collection of updates and improvements to our bioinformatics software collection.&lt;/p&gt;&lt;h3 id=&quot;platform&quot;&gt;Platform&lt;/h3&gt;&lt;p&gt;The EPI2ME Desktop Application [v5.1.3] provides a GUI that simplifies the usage of our bioinformatics workflows. This software update introduces a collection of usability improvements.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Addition of HTTPS proxy support - further proxy support is planned for the future.
Users may provide an HTTP or HTTPS proxy address to the application. An HTTP proxy should be used only when it supports redirecting HTTPS traffic via tunneling.&lt;/p&gt;&lt;p&gt;Please note: users on MacOS and Linux operating systems will need to separately configure Docker’s proxy settings (via Docker Desktop on MacOS and on the Command Line on Linux) (Docker daemon)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Multiple fixes to support Windows users with spaces or special characters in their usernames.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Improved management of the download and installation of EPI2ME WSL distribution during setup; interrupted installations are now handled.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Updates to the “Report Issue” functionality on Windows computers.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Bioinformatics analyses will now be updated as ‘Stopped’ if they become stuck in ‘Running’ or ‘Unknown’ states - this may occur following unexpected shutdowns.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;workflows&quot;&gt;Workflows&lt;/h3&gt;&lt;h4 id=&quot;wf-metagenomics---v260--wf-16s---v002&quot;&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot;&gt;wf-metagenomics&lt;/a&gt; - v2.6.0 / &lt;a href=&quot;https://github.com/epi2me-labs/wf-16s&quot;&gt;wf-16s&lt;/a&gt; - v0.0.2&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Fixed the plots perturbed by the presence of quote symbols in NCBI taxon names.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Added a new output; an &lt;code&gt;abundance_table_&amp;lt;rank&amp;gt;.tsv&lt;/code&gt; is now reported for the last analysed taxonomic rank.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;An optional &lt;code&gt;--minimap2_by_reference&lt;/code&gt; parameter has been added; this will output additional information on the sequencing depth and coverage for mapped reference sequences.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;wf-human-variation---v182&quot;&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation&quot;&gt;wf-human-variation&lt;/a&gt; - v1.8.2&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Performance improvements to the SNP subworkflow when using BED files to define genomic regions of interest.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Fixed an issue with the SV HTML report generation.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Improvements to the ClinVar-annotated VCF.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;wf-somatic-variation---v050&quot;&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation&quot;&gt;wf-somatic-variation&lt;/a&gt; - v0.5.0&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Updates to include the annotation of SVs.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Clair v0.1.5 is now included in the supplied containers.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Other nonspecific improvements.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;wf-transcriptomes---v041&quot;&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes&quot;&gt;wf-transcriptomes&lt;/a&gt; - v0.4.1&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Added support for LSK114 direct cDNA sequencing via a Pychopper update (see below).&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;wf-single-cell---v028&quot;&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell&quot;&gt;wf-single-cell&lt;/a&gt; - v0.2.8&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Increased memory allocation for adapter_scan  to fix out-of-memory issues.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;summarize_adapter_table&lt;/code&gt; updates to increase memory-efficiency.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Fixed bug that was causing single base read truncations.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;pychopper---v279&quot;&gt;&lt;a href=&quot;https://github.com/epi2me-labs/pychopper&quot;&gt;pychopper&lt;/a&gt; - v2.7.9&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Added support for data generated by the LSK114 direct cDNA sequencing kit.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We welcome all feedback and would appreciate any information on your experiences with the new proxy functionality that has been added to the Desktop Application.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/bedd0c862206f01cea08182f8d5470d8/59ccf/peacock.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/bedd0c862206f01cea08182f8d5470d8/peacock.jpeg</content:toenail></item><item><title><![CDATA[How to interpret exit codes]]></title><description><![CDATA[How to interpret exit codes to troubleshoot workflows.]]></description><link>https://labs.epi2me.io/how-to-exits</link><guid isPermaLink="false">https://labs.epi2me.io/how-to-exits</guid><pubDate>Fri, 06 Oct 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/6feb9bc4507dc8dee469fd343416dccc/59ccf/exit.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;You have almost certainly encountered a problem with some software in your day to day life at some point.
The error message may have given you a cryptic number, or code to describe what went wrong.
If documented and understood, such error codes could be used to diagnose and perhaps resolve a technical fault.
For example, when your browser shows you a message to explain that a web page has not been found, it is reacting to a &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404&quot;&gt;404 code&lt;/a&gt;; which is defined by a well-established set of agreed upon codes for web requests.&lt;/p&gt;&lt;p&gt;We try our best to make our workflows as robust as possible, but encountering software errors is an inevitability.
Your cluster could lose connectivity with your job scheduler, or you could run out of disk space, or the website we use to host our workflows is unreachable at just the wrong moment.
Some of the user reports we get are the result of problems that can be fixed by a user in-the-know without having to &lt;a href=&quot;/contactus&quot;&gt;contact us&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The aim of this blog post is to introduce you to error codes that may arise while running &lt;a href=&quot;https://labs.epi2me.io/wfindex/&quot;&gt;our workflows&lt;/a&gt; on the command line, or through our &lt;a href=&quot;/quickstart&quot;&gt;EPI2ME Desktop application&lt;/a&gt;, and what you need to do to resolve them.
Much of what is introduced here is equally applicable to other command line software too.&lt;/p&gt;&lt;h2 id=&quot;how-do-i-know-the-exit-code-of-a-workflow&quot;&gt;How do I know the exit code of a workflow?&lt;/h2&gt;&lt;p&gt;One of the first things that we will do when reading a user report for a problem with a workflow, is look for the exit status.
To the uninitiated, these simple numbers seem like an unhelpful piece of information, but once you have finished reading this blog post, you too will be able to cut through lines of red text and diagnose some problems immediately.
Bioinformatics is generally not well known for its user experience (and our team’s core goal is to change this), so you would be forgiven if you have not looked at an error produced by Nextflow too closely before — here is one now:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;% nextflow run main.nf
N E X T F L O W  ~  version 23.04.4
Launching `main.nf` [nauseous_ramanujan] DSL2 - revision: 734d48c47d
executor &amp;gt;  local (3)
executor &amp;gt;  local (3)
[b1/4f1c6b] process &amp;gt; countOwls (1) [100%] 1 of 1, failed: 1
ERROR ~ Error executing process &amp;gt; &amp;#x27;countOwls (2)&amp;#x27;

Caused by:
  Process `countOwls (2)` terminated with an error exit status (1)

Command executed:

  #!/usr/bin/env python3
  raise Exception(&amp;quot;Not enough owls.&amp;quot;)

Command exit status:
  1

Command output:
  (empty)

Command error:
  Traceback (most recent call last):
    File &amp;quot;.command.sh&amp;quot;, line 2, in &amp;lt;module&amp;gt;
      raise Exception(&amp;quot;Not enough owls.&amp;quot;)
  Exception: Not enough owls.

Work dir:
  /Users/Sam.Nicholls/scratch/nf-testing/exit_codes/work/79/f9172717a1de210d6e6e17a45a0481

Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`

 -- Check &amp;#x27;.nextflow.log&amp;#x27; file for details
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here’s what we know by reading each of the sections of this human readable output:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Caused by&lt;/strong&gt;: This section tells you the Nextflow process that encountered the problem, here the error was triggered by the “countOwls” process.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Command executed&lt;/strong&gt;: This section will print out the relevant command that triggered the problem. Here it was an inline Python script.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Command exit status&lt;/strong&gt;: The error code returned by the process, which was 1.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Command output&lt;/strong&gt;: Any lines of text written to the terminal by the process, in our case there weren’t any.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Command error&lt;/strong&gt;: Similarly, any lines of error text written to the terminal by the process will be printed here. For this example we can see a Python traceback.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Work dir&lt;/strong&gt;: Where to go and look on the file system to inspect any of the inputs and outputs for this process.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This is not much of a whodunnit, our Python script has raised an &lt;a href=&quot;https://en.wikipedia.org/wiki/Exception_handling&quot;&gt;Exception&lt;/a&gt;.
We’re going to have to get some more owls!
In the case of a real problem, the command execution, output and error sections may have a lot more lines of information which may or may not be related to the problem.
This is the very reason why the command exit status can be so useful.&lt;/p&gt;&lt;p&gt;If you’re using the EPI2ME Desktop application, this Nextflow output can be found on the “Logs” tab:&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:58.59649122807018%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAEDBf/EABYBAQEBAAAAAAAAAAAAAAAAAAABAv/aAAwDAQACEAMQAAAB6ukXMB//xAAaEAACAgMAAAAAAAAAAAAAAAAAEgECESEx/9oACAEBAAEFAkoJUSkGdtJ0/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAwEBAAAAAAAAAAAAAAAAAAIyASD/2gAIAQEABj8ClSVIzj//xAAbEAEBAAEFAAAAAAAAAAAAAAABAMEQETFBof/aAAgBAQABPyHccUKYrgeMsWgva//aAAwDAQACAAMAAAAQlO//xAAWEQADAAAAAAAAAAAAAAAAAAABEBH/2gAIAQMBAT8QgX//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAeEAEAAgEEAwAAAAAAAAAAAAABABEhMWFxwYGR0f/aAAgBAQABPxBbKEdnUpFu9fkatvxeoAAYWcXqIr1aYn//2Q==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Nextflow error inside EPI2ME&quot; title=&quot;Figure 1 - An exemplar Nextflow error message inside the EPI2ME desktop application.&quot; src=&quot;/static/74f73ee1cdbf347b16a0cc859ca4641f/0470c/epi2me_nextflow_error.jpg&quot; srcSet=&quot;/static/74f73ee1cdbf347b16a0cc859ca4641f/6c669/epi2me_nextflow_error.jpg 285w,/static/74f73ee1cdbf347b16a0cc859ca4641f/882a4/epi2me_nextflow_error.jpg 570w,/static/74f73ee1cdbf347b16a0cc859ca4641f/0470c/epi2me_nextflow_error.jpg 1140w,/static/74f73ee1cdbf347b16a0cc859ca4641f/7c2eb/epi2me_nextflow_error.jpg 1173w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 1 - An exemplar Nextflow error message inside the EPI2ME desktop application.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;h2 id=&quot;what-error-codes-might-i-see-while-running-a-workflow&quot;&gt;What error codes might I see while running a workflow?&lt;/h2&gt;&lt;p&gt;Now that you know how to read Nextflow errors and locate the command exit status, we can cover what these mysterious numbers mean.
The tables below enumerate common (and not so common) command exit status codes, why they might manifest and what you could try to do about it.&lt;/p&gt;&lt;h3 id=&quot;common-exit-codes&quot;&gt;Common exit codes&lt;/h3&gt;&lt;p&gt;Despite their ubiquity, exit codes mostly rely on historical conventions and &lt;a href=&quot;https://tldp.org/LDP/abs/html/exitcodes.html#FTN.AEN23647&quot;&gt;few are reserved for specific meaning&lt;/a&gt;.
The most common exit codes are:&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Exit status&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Potential causes&lt;/th&gt;&lt;th&gt;Possible next steps&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;OK (successful exit)&lt;/td&gt;&lt;td&gt;A successful process&lt;/td&gt;&lt;td&gt;Nextflow will error on a successful process if output files that should be present are not present. The process has failed to produce output that we always expect and our workflow has not accounted for a case where a file may optionally be generated. You should open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;Failure&lt;/td&gt;&lt;td&gt;General catch-all for an application error&lt;/td&gt;&lt;td&gt;Open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;Shell misuse&lt;/td&gt;&lt;td&gt;General catch-all for misuse of the shell commands&lt;/td&gt;&lt;td&gt;Open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;125&lt;/td&gt;&lt;td&gt;Docker command error&lt;/td&gt;&lt;td&gt;The command to run a Docker container did not execute successfully.&lt;/td&gt;&lt;td&gt;Check the Docker engine is running and that you have permissions to start containers. See our &lt;a href=&quot;https://labs.epi2me.io/installation/&quot;&gt;install guide&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;127&lt;/td&gt;&lt;td&gt;Command not found&lt;/td&gt;&lt;td&gt;The command line tool the workflow is trying to run cannot be found.&lt;/td&gt;&lt;td&gt;You may encounter this if you do not have Docker or Singularity installed, which is required to run the dependencies of our workflows. Check our &lt;a href=&quot;https://labs.epi2me.io/installation/&quot;&gt;install guide&lt;/a&gt; for how to install Docker. Otherwise, ensure that you have not provided a Nextflow configuration file that has changed the executor or container directives. You may have inadvertently caused the workflow to run outside of a container and the dependencies are not on your computer.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;255&lt;/td&gt;&lt;td&gt;Out of range&lt;/td&gt;&lt;td&gt;The exit status was beyond the allowed range of 0-255&lt;/td&gt;&lt;td&gt;Check the error message for actionable information about your compute environment and contact your system administrator. Otherwise, open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id=&quot;exit-codes-from-clusters-and-containers&quot;&gt;Exit codes from clusters and containers&lt;/h3&gt;&lt;p&gt;Exit codes beyond 128 have a special meaning and indicate that a process was sent a “signal” by the operating system.
Many common signals are &lt;a href=&quot;https://en.wikipedia.org/wiki/Signal_(IPC)#POSIX_signals&quot;&gt;associated with a signal number&lt;/a&gt; and the signal can be determined by subtracting 128 from the exit code.
These codes are often encountered when running workflows in compute clusters and containers and useful examples are included below.&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Exit status&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Potential causes&lt;/th&gt;&lt;th&gt;Possible next steps&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;128 + 2 = 130&lt;/td&gt;&lt;td&gt;SIGINT&lt;/td&gt;&lt;td&gt;Process was interrupted&lt;/td&gt;&lt;td&gt;The process was sent the interrupt signal, you may have used Ctrl+C to quit the Nextflow workflow and Nextflow forwarded the signal to your processes. Some clusters will use this signal to stop processes that have exceeded their time or memory limit. You should speak to your system administrator to diagnose the reason that the job was terminated.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;128 + 6 = 134&lt;/td&gt;&lt;td&gt;SIGABRT&lt;/td&gt;&lt;td&gt;Process aborted&lt;/td&gt;&lt;td&gt;The process has aborted as it cannot continue. Some clusters will use this signal to stop processes that have exceeded their time or memory limit. You should speak to your system administrator to diagnose the reason that the job was terminated. If you are not using a cluster, open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;128 + 7 = 135&lt;/td&gt;&lt;td&gt;SIGBUS&lt;/td&gt;&lt;td&gt;Invalid memory address&lt;/td&gt;&lt;td&gt;You should speak to your system administrator to check the installation of prerequisites like Docker. If you are not using a cluster, open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;128 + 9 = 137&lt;/td&gt;&lt;td&gt;SIGKILL&lt;/td&gt;&lt;td&gt;Process was killed&lt;/td&gt;&lt;td&gt;The process was forcibly killed. This may be your operating system invoking the out-of-memory (OOM) killer to stop a process from freezing your computer. Some clusters will send a kill signal to a job for exceeding its memory or time limit. You should speak to your system administrator to diagnose the reason that the job was terminated. See additional guidance below this table.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;128 + 10 = 138&lt;/td&gt;&lt;td&gt;SIGUSR1&lt;/td&gt;&lt;td&gt;Process received user signal (1)&lt;/td&gt;&lt;td&gt;Typically observed when a your cluster is signalling that the job should wrap up because it is about to exceed a time or memory limit. You should speak to your system administrator to help you adjust your job resource requests. See additional guidance below this table.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;128 + 11 = 139&lt;/td&gt;&lt;td&gt;SIGSEGV&lt;/td&gt;&lt;td&gt;Invalid memory access&lt;/td&gt;&lt;td&gt;The process tried to access some memory it should not have, open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;128 + 12 = 140&lt;/td&gt;&lt;td&gt;SIGUSR2&lt;/td&gt;&lt;td&gt;Process received user signal (2)&lt;/td&gt;&lt;td&gt;Typically observed when your cluster is signalling that the job should wrap up because it is about to exceed a time or memory limit. You should speak to your system administrator to help you adjust your job resource requests. See additional guidance below this table.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;128 + 15 = 143&lt;/td&gt;&lt;td&gt;SIGTERM&lt;/td&gt;&lt;td&gt;Process was terminated&lt;/td&gt;&lt;td&gt;Some clusters will use this signal to stop processes that have exceeded their time or memory limit. You should speak to your system administrator to diagnose the reason that the job was terminated.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Several of the above exit codes are encountered across different cluster schedulers when a job tries to use more memory than has been requested by the workflow configuration.
We try to define sensible limits for common use cases of each workflow but we cannot get this right for every possible use case.
In these cases, you will need to increase the memory limit applied to the process that encountered the problem.
You can do this by using Nextflow &lt;a href=&quot;https://www.nextflow.io/docs/latest/config.html#process-selectors&quot;&gt;process selectors&lt;/a&gt; to apply configuration to particular parts of our workflow.&lt;/p&gt;&lt;p&gt;For example, to increase the memory for a process called “countOwls”, create a file (for example, named &lt;code&gt;increase_memory.config&lt;/code&gt;) with the following configuration (replacing XX with a number higher than the memory limit defined in the workflow’s configuration; i.e. the &lt;code&gt;nextflow.config&lt;/code&gt; file):&lt;/p&gt;&lt;pre&gt;&lt;code&gt;process {
    withName:countOwls {
        memory = &amp;quot;XX GB&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When invoking Nextflow with &lt;code&gt;nextflow run&lt;/code&gt;, reference this extra configuration with &lt;code&gt;-c increase_memory.config&lt;/code&gt;.
If you are using the EPI2ME Desktop application, you can instead specify additional Nextflow config options such as the example provided here in the “Extra configuration” options section at the bottom of the launch window. The configuration you provide in that text box will be loaded into the workflow.&lt;/p&gt;&lt;h3 id=&quot;sysexit-codes&quot;&gt;Sysexit codes&lt;/h3&gt;&lt;p&gt;Introduced by the BSD operating system for its mail utility, the &lt;a href=&quot;https://man.freebsd.org/cgi/man.cgi?query=sysexits&amp;amp;apropos=0&amp;amp;sektion=0&amp;amp;manpath=FreeBSD+15.0-CURRENT&amp;amp;arch=default&amp;amp;format=html&quot;&gt;&lt;code&gt;sysexits&lt;/code&gt; library attempted to standardise exit codes&lt;/a&gt; to indicate common problems when processing input arguments and files from users.
Usage of sysexit codes does not appear to have caught on very widely but some well used programs such as the AWS CLI do use them.&lt;/p&gt;&lt;p&gt;We aim to use some of the sysexit codes in Python scripts that we include with our workflows.
Although we use Nextflow to catch these codes and generate clear error messages to users, they are presented here in case you see one!&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Exit status&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Potential causes&lt;/th&gt;&lt;th&gt;Possible next steps&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;64&lt;/td&gt;&lt;td&gt;Usage error&lt;/td&gt;&lt;td&gt;A command was used incorrectly&lt;/td&gt;&lt;td&gt;Check your workflow parameters to ensure they are set correctly. We try our best to make sure incorrect parameters are caught by the Nextflow workflow before the workflow is run. This may be a bug inside the workflow, open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;65&lt;/td&gt;&lt;td&gt;Data error&lt;/td&gt;&lt;td&gt;The input data was incorrect&lt;/td&gt;&lt;td&gt;Check your input files are in the correct format and not corrupted. If the error persists, open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;66&lt;/td&gt;&lt;td&gt;No input&lt;/td&gt;&lt;td&gt;The input data is missing, not readable or empty&lt;/td&gt;&lt;td&gt;Check your files exist, can be read by your user and are not empty.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;68&lt;/td&gt;&lt;td&gt;No host&lt;/td&gt;&lt;td&gt;The network host could not be found&lt;/td&gt;&lt;td&gt;If you are specifying a remote host (e.g. a kraken classification server), check the host is correct.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;70&lt;/td&gt;&lt;td&gt;Software error&lt;/td&gt;&lt;td&gt;An unspecific software error&lt;/td&gt;&lt;td&gt;You have likely encountered an error state we hoped would not be possible. Open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;73&lt;/td&gt;&lt;td&gt;Cannot create&lt;/td&gt;&lt;td&gt;An output file cannot be created&lt;/td&gt;&lt;td&gt;Check that you have permission to write to any defined output locations and that you have not run out of disk space.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;74&lt;/td&gt;&lt;td&gt;I/O error&lt;/td&gt;&lt;td&gt;An error occurred while reading or writing a file&lt;/td&gt;&lt;td&gt;This could be temporary, try the workflow again. Check you can access your inputs and defined output locations. If this persists, open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;75&lt;/td&gt;&lt;td&gt;Temporary failure&lt;/td&gt;&lt;td&gt;A task failed for a reason that is probably temporary&lt;/td&gt;&lt;td&gt;Wait a little while and retry the workflow. If this persists, open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;78&lt;/td&gt;&lt;td&gt;Config error&lt;/td&gt;&lt;td&gt;A configuration loaded is incorrect or missing&lt;/td&gt;&lt;td&gt;Check your workflow parameters to ensure they are set correctly. We try our best to make sure incorrect parameters are caught by the Nextflow workflow before the workflow is run. This may be a bug inside the workflow, open an issue with us.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Hopefully this post has enlightened you to recognise some common exit codes and help you determine problems that require a sanity check of your workflow parameters, a configuration change, or opening a bug report with us.
If you have a Github account, you can open an issue on the relevant &lt;a href=&quot;https://github.com/epi2me-labs&quot;&gt;epi2me-labs workflow repository&lt;/a&gt;.
Please use the “Bug report” issue type and provide answers to all the questions to help us help you!
If you’re using the EPI2ME Desktop application, you can follow the instructions on the “Report issue” option on the errored workflow’s “Overview” tab to generate a bug report and send it to us.&lt;/p&gt;&lt;p&gt;This post continues our weekly How To series, if you have any feedback on these posts or our workflows, please &lt;a href=&quot;https://labs.epi2me.io/contactus/&quot;&gt;get in touch&lt;/a&gt;!&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/6feb9bc4507dc8dee469fd343416dccc/59ccf/exit.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/6feb9bc4507dc8dee469fd343416dccc/exit.jpeg</content:toenail></item><item><title><![CDATA[How to align your data]]></title><description><![CDATA[How to align reads using Minimap2.]]></description><link>https://labs.epi2me.io/how-to-align</link><guid isPermaLink="false">https://labs.epi2me.io/how-to-align</guid><pubDate>Fri, 29 Sep 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/db5e58bd4267424b4b1dc66fb6d313dd/59ccf/alionment.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Read alignment is central to most bioinformatics analyses. In this short post we explain how to align reads against a reference and analyse the resulting files. &lt;/p&gt;&lt;p&gt;Read alignment isn’t usually an end point of analysis; its typically combined with subsequent steps to obtain useful information. Readers might be interested in some of our workflows that include read alignment as an integral step:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-alignment&quot;&gt;wf-alignment&lt;/a&gt; - for a simple alignment of your reads with one or more references.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-amplicon&quot;&gt;wf-amplicon&lt;/a&gt; - for alignment of amplicon sequence data with relevant references.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes&quot;&gt;wf-transcriptomes&lt;/a&gt; - for alignment and subsequent analysis of cDNA and direct RNA data.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All will output a sorted aligned BAM file as well as other useful statistics and an easy to interpret report. Our preferred tool for alignment is Minimap2.&lt;/p&gt;&lt;h2 id=&quot;minimap2&quot;&gt;Minimap2&lt;/h2&gt;&lt;p&gt;We advise using &lt;a href=&quot;https://github.com/lh3/minimap2&quot;&gt;Minimap2&lt;/a&gt; for alignment of long reads to a reference. There are various publications (eg. &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2021.07.09.451840v1.full&quot;&gt;LoTempio et al., 2021&lt;/a&gt;, &lt;a href=&quot;https://www.degruyter.com/document/doi/10.1515/cdbme-2021-2212&quot;&gt;Becht et al., 2021&lt;/a&gt;) that compare alignment tools using real and simulated reads for a variety of applications. Minimap2 is typically faster than other tools whilst maintaining general good accuracy across different types of Oxford Nanopore Technologies’ read data.&lt;/p&gt;&lt;p&gt;Minimap2 is a fast sequence mapping and alignment program. It works by first mapping reads to the approximate origin of the reference sequence, and then running a computationally intensive comparison between localised reads to create the final alignments. You will often see these two steps referred to together as alignment. If you are interested in further detail on how minimap2 works you might like to read &lt;a href=&quot;https://academic.oup.com/bioinformatics/article/34/18/3094/4994778&quot;&gt;the author’s paper&lt;/a&gt;.&lt;/p&gt;&lt;h2 id=&quot;using-minimap2-with-nanopore-data&quot;&gt;Using minimap2 with nanopore data&lt;/h2&gt;&lt;p&gt;Let’s break down the basic invocation of the tool for genomic reads:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;minimap2 -a -x map-ont ref.fa query.fq &amp;gt; alignment.sam
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;minimap2&lt;/strong&gt; - is the base command.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;-a&lt;/strong&gt; -  a flag to turn on SAM output, otherwise minimap2 outputs a PAF format by default.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;-x map-ont&lt;/strong&gt; - use the map-ont preset will set parameters with suitable defaults.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;ref.fa&lt;/strong&gt; - refers to the reference: the sequence or sequences against which your reads will be aligned. This can also be a precomputed &lt;code&gt;.mmi&lt;/code&gt; index file.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;query.fq&lt;/strong&gt; - is the &lt;a href=&quot;https://en.wikipedia.org/wiki/FASTQ_format&quot;&gt;FASTQ&lt;/a&gt; file containing your reads.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&amp;gt; alignment.sam&lt;/strong&gt; - the output will be directed to this &lt;a href=&quot;https://en.wikipedia.org/wiki/SAM_(file_format)&quot;&gt;SAM&lt;/a&gt; file.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;You should not adjust any parameters as they have been fine tuned with tested defaults using the &lt;code&gt;-x map-ont&lt;/code&gt; preset.&lt;/p&gt;&lt;p&gt;If working with cDNA use the &lt;code&gt;-x splice&lt;/code&gt; preset instead eg.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;minimap2 -a -x splice ref.fa query-cDNA.fq &amp;gt; alignment-cDNA.sam
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;how-to-post-process-the-alignment&quot;&gt;How to post process the alignment&lt;/h2&gt;&lt;p&gt;A few common steps might be required to get the alignments in a useful form.&lt;/p&gt;&lt;p&gt;The first of these is to convert the SAM file to a more space efficient BAM (&lt;strong&gt;-S&lt;/strong&gt; for SAM, &lt;strong&gt;-b&lt;/strong&gt; for BAM):&lt;/p&gt;&lt;pre&gt;&lt;code&gt;samtools view -S -b alignment.sam &amp;gt; alignment.bam
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can also sort the alignments by reference position to create a sorted BAM:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;samtools sort -o alignment.sorted.bam alignment.bam
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And we can index the sorted alignments file to allow quick retrieval of reads spanning a given reference position:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;samtools index alignment.sorted.bam
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It is possible to combine all of these steps in a single incantation using &lt;a href=&quot;https://en.wikipedia.org/wiki/Pipeline_(Unix)&quot;&gt;pipes&lt;/a&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;minimap2 -a -x map-ont ref.fa query.fq \
    samtools sort -O BAM --write-index -o alignment.sorted.bam -
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;how-to-analyse-alignments&quot;&gt;How to analyse alignments.&lt;/h2&gt;&lt;p&gt;Minimap2 can output three types of alignment for long reads, they are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Primary alignments&lt;/strong&gt; - the alignment with the highest alignment score. There should only ever be one primary alignment per read.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Secondary alignments&lt;/strong&gt; - a read may align ambiguously to multiple locations, any additional possible alignments will be secondary alignments.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Supplementary alignments&lt;/strong&gt; - the aligner may break a read into two or more pieces and aligns both, perhaps due to a structural variation. The best aligned segment of the chain will be the primary alignment while the others will be marked as supplementary. Supplementary alignments are also of key importance when analyses cDNA or direct RNA reads aligned to a genomic reference (or incomplete transcriptome).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Unmapped&lt;/strong&gt; - if the read cannot be aligned it is still included in the alignment file and marked as unmapped.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A good way to get a quick summary report of the types of alignment records in your aligned bam file is with &lt;code&gt;samtools flagstat&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;samtools flagstat sorted.alignment.bam

12000 + 0 in total (QC-passed reads + QC-failed reads)
5000 + 0 secondary
3000 + 0 supplementary
0 + 0 duplicates
10000 + 0 mapped (93.98% : N/A)
0 + 0 paired in sequencing
0 + 0 read1
0 + 0 read2
0 + 0 properly paired (N/A : N/A)
0 + 0 with itself and mate mapped
0 + 0 singletons (N/A : N/A)
0 + 0 with mate mapped to a different chr
0 + 0 with mate mapped to a different chr (mapQ&amp;gt;=5)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This shows that there are 10000 primary alignments indicated by mapped and 2000 unmapped (total - mapped = unmapped). Other categories may or may not be relevant depending on your use case --- many of those above relate specifically to paired-end short-read sequencing.&lt;/p&gt;&lt;p&gt;More detailed per-read information can be viewed using &lt;code&gt;samtools view&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;samtools view alignment.bam
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;a href=&quot;https://samtools.github.io/hts-specs/&quot;&gt;SAM/BAM specification&lt;/a&gt; provides a detailed explanation of the information available in the alignment file but some main points are the name of the read that was mapped, the position with respect to the reference, the sequence of the read and per base quality scores of the read. &lt;/p&gt;&lt;p&gt;One column which deserves additional comment (and people frequently ask us about), is the “mapping quality” (MAPQ) column. For minimap2 this is given as a score in the range 0-60: it is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Phred_quality_score&quot;&gt;phred-scaled&lt;/a&gt; probability of the alignment being incorrect. For example if the score is 60 it means we can consider the read uniquely mapped to a single region of the reference sequence. A lower values indicates that the read shows similarity to multiple regions of the reference sequence and cannot be uniquely assigned to a single region.&lt;/p&gt;&lt;p&gt;Samtools has many more tools to help filter, visualise and manipulate alignment files. Read more about samtools and the options available &lt;a href=&quot;http://www.htslib.org/doc/samtools.html&quot;&gt;here&lt;/a&gt;. We also have a more detailed tutorial of working with SAM/BAM files available &lt;a href=&quot;https://labs.epi2me.io/notebooks/Introduction_to_SAM_and_BAM_files.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Another great way to make sense of your alignment files is to visualise them in an alignment viewer such as &lt;a href=&quot;https://software.broadinstitute.org/software/igv/&quot;&gt;IGV&lt;/a&gt; or &lt;a href=&quot;https://jbrowse.org/jb2/&quot;&gt;JBrowse&lt;/a&gt;. The minimum you will require for this is the fasta file you originally used for the alignment and the sorted, and indexed bam files we produced earlier with samtools.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:59.298245614035096%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAACS0lEQVR42jVSiXKiQBDl/z9qdyvJiiAgSBBhhityxlsjGjX1tntwp+pV9/Q1r3taq+oau90Wm80GVVWhLEvs9jvcbnfc73eSN5zPPS6XC75OX+jPZ1yvV3x/fyv0/UWB/WfyaavVCvv9Huv1GmmaIkkSVZjt2+3wUF23hAZVWaNpanx+dsrPOU3bka0jfYW2baE9Hg8wuDozYp1f6/v+yYDZ9YrV8XhUPrbz/fxke7sNcezXmMXhcEBRFHAcB57nIQgCZFmmbMxYJhJCCIRhiCiKVCcM1qWUCv/tGrd7Op1Ue8vlUs2wpra6rkXbDGgYLbfb0L0ZZDugIdT0D9xuWZXQmP79fsNmvUWaZEglsYpzkhlEmCMROWRYQEaZQkp3Zq+QkE/FUlxMMUJC41/8+flRA9dfLEx0B+YTxsjBxLBhkjQNC4Y+GXTy2Zaj9PHLM2Zs08hcaDzkx/2hKDuWC8fwMbVnsA0P7tSDrc9g/g5gmx495mLy5sPSPYqhWNOH+TojOYPnejRnYsjzY5Z12WIydmCN3aEwBU0dD/ofB/orsftrK2Zs870AgR/CMqZw7XfF1JrY6mPU2vBpuw5TYuRMfEpY4MqrQdjtDmiqT/hOTOznEDH//hJ5UmLuCywCiTAQCGYxsrSgtdnQ2hwPyNMMo18jGG8jatdCkSYoaeg1BVVFhWxBazP1Id7nyCJJSCgnxYcgyISQ4WNJBfM8V+uymC9UUi4jFHmCTJCMBD4WMZKQEwU9EKOUAkUsFTLav4KKs0/OQ4TRHP8AOx1noAON8csAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;IGV alignment example&quot; title=&quot;Figure 1 - Example of visualising an alignment file in IGV.&quot; src=&quot;/static/abd3bdfd4d803d5b074b4781872b60c4/b5cea/igv_alignment.png&quot; srcSet=&quot;/static/abd3bdfd4d803d5b074b4781872b60c4/0e2fe/igv_alignment.png 285w,/static/abd3bdfd4d803d5b074b4781872b60c4/432e7/igv_alignment.png 570w,/static/abd3bdfd4d803d5b074b4781872b60c4/b5cea/igv_alignment.png 1140w,/static/abd3bdfd4d803d5b074b4781872b60c4/09ede/igv_alignment.png 1710w,/static/abd3bdfd4d803d5b074b4781872b60c4/ec09f/igv_alignment.png 1916w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 1 - Example of visualising an alignment file in IGV.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;Alignment is a crucial first step in many bioinformatic tasks. Here we have given a brief survey of the tools of the trade for creating and summarising alignments with nanopore data.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/db5e58bd4267424b4b1dc66fb6d313dd/59ccf/alionment.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/db5e58bd4267424b4b1dc66fb6d313dd/alionment.jpeg</content:toenail></item><item><title><![CDATA[Creating correlation plots from bedMethyl files]]></title><description><![CDATA[A brief guide to manipulating bedMethyl files from modkit and modbam2bed]]></description><link>https://labs.epi2me.io/how-to-mod</link><guid isPermaLink="false">https://labs.epi2me.io/how-to-mod</guid><pubDate>Fri, 22 Sep 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/5cf3badf731bcf2c2146b7da80fd22ea/59ccf/mod-base-duck.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;In this vignette we will show how we have in the past produced correlation plots showing concordance between modified base frequencies calculated from bisulfite sequencing and sequencing with Oxford Nanopore Technologies sequencing devices.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;This article is intended to give a leg up to bioinformaticians exploring modified base information stored in BAM files.
If you are more generally interested in preparing 5mC and 5hmC data for onward investigation you may like to consider our &lt;a href=&quot;/workflows/wf-human-variation/&quot;&gt;wf-human-variation&lt;/a&gt; workflow.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Without further ado, we start from BAM files output by either the dorado or guppy basecallers.
To create a bedMethyl file we have two options:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The deprecated program &lt;a href=&quot;https://github.com/epi2me-labs/modbam2bed&quot;&gt;modbam2bed&lt;/a&gt;&lt;/li&gt;&lt;li&gt;The next generation program &lt;a href=&quot;https://github.com/nanoporetech/modkit&quot;&gt;modkit&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We stress that modbam2bed is deprecated and modkit should be used in preference.
There are no known bugs in modbam2bed, it is deprecated ony because the more fulsome package modkit subsumes its functionality.
We present code for using the output of both only because users may have historic data from either.
This article is not intended as a full user guide to these programs, please see their respective documentation and command-line help instructions for additional information.&lt;/p&gt;&lt;p&gt;To create a bedMethyl file using modbam2bed we can run:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# an input BAM file and reference sequence
BAM=chr20.bam
REF=GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta

modbam2bed \
    --cpg --threads 24 \
    --region chr20:0-99999999 \
    --combine --extended \
    &amp;quot;${REF}&amp;quot; &amp;quot;${BAM} &amp;gt; modbam2bed.bed
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can run modkit to produce a file similar to the &lt;code&gt;modbam2bed.cpg.acc.bed&lt;/code&gt; file:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;modkit pileup \
    &amp;quot;${BAM}&amp;quot; modkit.bed \
    --region chr20 --threads 24 \
    --ref &amp;quot;${REF}&amp;quot; --preset traditional
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The results of the two programs are broadly similar, but not identical.&lt;/p&gt;&lt;p&gt;The output &lt;code&gt;modbam2bed.bed&lt;/code&gt; contains rows for each strand of DNA.
The program can produce results aggregated across neighbouring reference positions (corresponding to the two cytosine bases on opposite strands in a CpG motif).
We produce the stranded counts here for illustrative purposes.
There are other subtleties in how the two programs handle data and create their outputs, which are broadly of little consequence for our purposes here.
We encourage you to explore to options of modkit to produce the outputs required for your use case.&lt;/p&gt;&lt;p&gt;So to the aim of this how to article. Let’s load the data into Python and start plotting:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# All the packages we need for loading the data and plotting
import aplanat
from aplanat import spatial
from bokeh.models.widgets import Tabs, Panel
import numpy as np
import pandas as pd

# the input files
bisulfite_file = &amp;quot;CpG.gz.bismark.zero.cov.gz&amp;quot;
modbam2bed_file = &amp;quot;modbam2bed.bed&amp;quot;
modkit_file = &amp;quot;modkit.bed&amp;quot;

# load bisulfite data
#   we need to calculate the coverage here
bis = pd.read_csv(
    bisulfite_file,
    sep=&amp;quot;\t&amp;quot;, header=None, engine=&amp;quot;c&amp;quot;,
    dtype={
        &amp;#x27;chrom&amp;#x27;:str, &amp;#x27;start&amp;#x27;:int, &amp;#x27;end&amp;#x27;:int,
        &amp;#x27;freq&amp;#x27;:float, &amp;#x27;mod&amp;#x27;:int, &amp;#x27;canon&amp;#x27;:int},
    names=[&amp;#x27;chrom&amp;#x27;, &amp;#x27;start&amp;#x27;, &amp;#x27;end&amp;#x27;, &amp;#x27;freq&amp;#x27;, &amp;#x27;mod&amp;#x27;, &amp;#x27;canon&amp;#x27;]
)
bis[&amp;quot;coverage&amp;quot;] = bis[&amp;quot;mod&amp;quot;] + bis[&amp;quot;canon&amp;quot;]

# load modbam2bed data
#   the file contains counts from + and - strands separately, which
#   are combined below. See docs for further information.
modbam = pd.read_csv(
    modbam2bed_file,
    sep=&amp;quot;\t&amp;quot;, header=None, engine=&amp;quot;c&amp;quot;,
    usecols=[&amp;quot;chrom&amp;quot;, &amp;quot;start&amp;quot;, &amp;quot;coverage&amp;quot;, &amp;quot;freq&amp;quot;, &amp;quot;strand&amp;quot;, &amp;quot;mod&amp;quot;],
    dtype={
        &amp;quot;chrom&amp;quot;:str, &amp;quot;start&amp;quot;:int, &amp;quot;coverage&amp;quot;:int,
        &amp;quot;freq&amp;quot;:float, &amp;quot;strand&amp;quot;:str, &amp;quot;mod&amp;quot;:int},
    names=[
        &amp;quot;chrom&amp;quot;, &amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;, &amp;quot;name&amp;quot;, &amp;quot;score&amp;quot;, &amp;quot;strand&amp;quot;,
        &amp;quot;tstart&amp;quot;, &amp;quot;tend&amp;quot;, &amp;quot;color&amp;quot;,
        &amp;quot;coverage&amp;quot;, &amp;quot;freq&amp;quot;,
        &amp;quot;canon&amp;quot;, &amp;quot;mod&amp;quot;, &amp;quot;filt&amp;quot;, &amp;quot;nocall&amp;quot;, &amp;quot;altmod&amp;quot;])

# load modkit data
#    nothing special to do here. See docs for further information
modkit = pd.read_csv(
    modkit_file,
    sep=&amp;quot;\s+&amp;quot;, header=None, engine=&amp;quot;c&amp;quot;,
    usecols=[&amp;quot;chrom&amp;quot;, &amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;, &amp;quot;coverage&amp;quot;, &amp;quot;freq&amp;quot;],
    dtype={
        &amp;quot;chrom&amp;quot;:str, &amp;quot;start&amp;quot;:int, &amp;quot;end&amp;quot;:int,
        &amp;quot;coverage&amp;quot;:int, &amp;quot;freq&amp;quot;:float},
    names=[
        &amp;quot;chrom&amp;quot;, &amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;, &amp;quot;name&amp;quot;, &amp;quot;score&amp;quot;, &amp;quot;strand&amp;quot;,
        &amp;quot;tstart&amp;quot;, &amp;quot;tend&amp;quot;, &amp;quot;color&amp;quot;,
        &amp;quot;coverage&amp;quot;, &amp;quot;freq&amp;quot;, &amp;quot;mod&amp;quot;, &amp;quot;canon&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;del&amp;quot;, &amp;quot;fail&amp;quot;, &amp;quot;diff&amp;quot;, &amp;quot;nocall&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With data loaded we can plot heat maps to illustrate correlations between the datasets:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# a function that create the plot for two datasets
def make_heatmap(bis, nano, title):
    r_coeff = np.corrcoef(bis, nano)
    r_coeff = r_coeff[0,1]
    rmse = np.sqrt(np.mean((bis-nano)**2))

    # the curious bin numbers here attempt to mitigate aliasing artefacts around
    # rational modification fractions.
    p = spatial.heatmap2(
        bis, nano,
        tools = &amp;quot;pan,wheel_zoom,box_zoom,reset&amp;quot;,
        log=True, x_bins=53, y_bins=53, xlim=(0,100), ylim=(0,100), zlim=(100, 100000),
        title=&amp;quot;Methylation comparison. R={:.3f}, RMSE={:.3f}&amp;quot;.format(r_coeff, rmse))

    p.match_aspect = True
    p.aspect_ratio = 1.2
    p.xaxis.axis_label = &amp;#x27;Bisulphite Methylation Frequency&amp;#x27;
    p.yaxis.axis_label = f&amp;#x27;{title} Methylation Frequency&amp;#x27;
    p.toolbar.logo = None
    return p

# filter the bisulphite data to chromosome 20
select = {&amp;quot;chr20&amp;quot;}
bis_sel = bis.loc[(bis[&amp;quot;chrom&amp;quot;].isin(select))]
plots = dict()

for ver, df in zip((&amp;quot;modbam2bed&amp;quot;, &amp;quot;modkit&amp;quot;), (modbam, modkit)):
    # filter to chrom 20 and drop any rows with NaNs
    sel = df.loc[(df[&amp;quot;chrom&amp;quot;].isin(select))].dropna()
    if ver == &amp;quot;modbam2bed&amp;quot;:
        # combine strands
        sel.loc[sel.strand == &amp;quot;-&amp;quot;, &amp;#x27;start&amp;#x27;] -= 1
        sel = sel.groupby([&amp;#x27;chrom&amp;#x27;,&amp;#x27;start&amp;#x27;]).sum().reset_index()
        sel[&amp;#x27;freq&amp;#x27;] = 100 * sel[&amp;#x27;mod&amp;#x27;] / sel[&amp;#x27;coverage&amp;#x27;]
    # perform an inner join on genomic location
    combined = pd.merge(
        bis_sel, sel, on=[&amp;quot;chrom&amp;quot;, &amp;quot;start&amp;quot;], suffixes=[&amp;quot;.bis&amp;quot;, &amp;quot;.nano&amp;quot;])
    # filter data to where both bisulfite and nanopore have &amp;gt;20 reads
    plot_data = combined.loc[
        (combined[&amp;#x27;coverage.bis&amp;#x27;] &amp;gt; 20) &amp;amp;
        (combined[&amp;#x27;coverage.nano&amp;#x27;] &amp;gt; 20)]
    # make a plot
    plot = make_heatmap(plot_data[&amp;quot;freq.bis&amp;quot;], plot_data[&amp;quot;freq.nano&amp;quot;], ver)
    plots[ver] = Panel(child=plot, title=ver)

tabs = Tabs(tabs=list(plots.values()))
aplanat.show(tabs)    
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The output of the above will look something like the plots below. &lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;howToModPlot&quot; plotCaption=&quot;Figure 1. Correlation plots generated through the example code using both modbam2bed and modkit to extract modified base information from a BAM file output by either of the basecallers guppy or dorado.&quot;&gt;&lt;/div&gt;&lt;p&gt;This concludes our short tour of handling bedMethyl files from our deprecated and current tools for handling modified base information present in BAM files from Oxford Nanopore Technologies’ basecallers.
We hope this information provides useful to the community.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/5cf3badf731bcf2c2146b7da80fd22ea/59ccf/mod-base-duck.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/5cf3badf731bcf2c2146b7da80fd22ea/mod-base-duck.jpeg</content:toenail></item><item><title><![CDATA[Supporting Future Clinical Bioinformaticians]]></title><description><![CDATA[Oxford Nanopore supports the NHS STP program.]]></description><link>https://labs.epi2me.io/clinical-bioinformatics</link><guid isPermaLink="false">https://labs.epi2me.io/clinical-bioinformatics</guid><pubDate>Wed, 20 Sep 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/2b7396c37d2337d5bd1ea28495bf0a80/59ccf/bioinformatics.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;blockquote&gt;&lt;p&gt;Oxford Nanopore Technologies products are not intended for use for health assessment or to diagnose, treat, mitigate, cure or prevent any disease or condition.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Over the past couple of months the EPI2ME team have welcomed two trainee clinical bioinformaticians from the National Health Service in England (NHS) to spend some time working on small projects in the team.&lt;/p&gt;&lt;p&gt;Sophia Johnson and Benjamin Bunce are individuals on the NHS scientist training programme (STP), training to become clinical scientists specialising in bioinformatics. There are clinical scientists in numerous healthcare specialisms working behind the scenes in hospitals, and those on the Bioinformatics-Genomics pathway in the NHS are primarily responsible for the analysis of next-generation sequencing data. Based in regional laboratories, they work closely with scientists in specialist disease teams, to provide clinicians with diagnostic reports for their patients.&lt;/p&gt;&lt;p&gt;Matt Parker and I have previously worked in this role, providing safe and effective analysis of NGS data for the benefit of patient care.&lt;/p&gt;&lt;p&gt;We’ve loved having Sophia and Ben as guests in the team and we wish them every success in the remainder of their training. Sophia writes about her experience with us below.&lt;/p&gt;&lt;h2 id=&quot;sophia-johnson---repeat-expansion-interruptions&quot;&gt;Sophia Johnson - Repeat Expansion Interruptions&lt;/h2&gt;&lt;p&gt;Spending 6 weeks at ONT with the EPI2ME team for my elective has been a great opportunity to broaden my Bioinformatics skills before starting my third and final year of the NHS STP. &lt;/p&gt;&lt;h3 id=&quot;the-project&quot;&gt;The Project&lt;/h3&gt;&lt;p&gt;My main project has been to create a new plot for the &lt;code&gt;wf-human-variation&lt;/code&gt; STR report. Short Tandem Repeat (STR) expansions can cause diseases such as Huntington’s and Fragile X Syndrome. The plot I aimed to create needed to give a more complete picture of an individual’s STRs through displaying base content of each read and highlighting any interruptions.&lt;/p&gt;&lt;p&gt;&lt;code&gt;wf-human-variation&lt;/code&gt; utilises a fork of &lt;a href=&quot;https://github.com/philres/straglr&quot;&gt;Straglr&lt;/a&gt; to genotype STR expansions from long-read alignments. The Staglr tool outputs a VCF and a TSV file with detailed information per read. The BAM file and Straglr TSV from the workflow would provide all the information required for me to create the plot.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Weeks 1-2&lt;/em&gt;&lt;/p&gt;&lt;p&gt;I spent my first two weeks getting up to speed with the EPI2ME codebase. I also worked through tutorials around Nextflow and various plotting tools that could be of use such as &lt;a href=&quot;https://echarts.apache.org/en/index.html&quot;&gt;Apache ECharts&lt;/a&gt; and &lt;a href=&quot;https://bokeh.org/&quot;&gt;Bokeh&lt;/a&gt;. Personally, I find tutorials and Youtube videos are often good starting points before reading the tools documentation in more detail. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=wbtMbJTo1xo&amp;amp;list=PLPZ8WHdZGxmUVZRUfua8CsjuhjZ96t62R&amp;amp;pp=iAQB&quot;&gt;Youtube: Nextflow Training Workshop Playlist&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=UINdSjavjss&quot;&gt;Youtube: Demo of various Apache ECharts&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://sateeshperi.github.io/nextflow_varcal/nextflow/nextflow_intro&quot;&gt;Tutorial: Introduction to NextFlow&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://rtd-bioinformatics.readthedocs.io/en/latest/nextflow/basic_concepts.html&quot;&gt;Tutorial: NextFlow Basic Concepts&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;em&gt;Weeks 3-4&lt;/em&gt;&lt;/p&gt;&lt;p&gt;My next focus was the data pre-processing steps required for the plot. Here’s an overview of the main steps…&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Filtering the workflow BAM:&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;For speed purposes I first used samtools to subset the BAM to contain only STR regions&lt;/li&gt;&lt;li&gt;Next I filtered the BAM to contain only STR supporting reads from the Straglr TSV (utilising Pandas and pysam)&lt;/li&gt;&lt;/ul&gt;&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Extracting STR sequences from the reads:&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;First, relevant information was extracted from the Straglr TSV such as haplotype, repeat start position, and repeat size&lt;/li&gt;&lt;li&gt;Supporting read sequences were extracted from the BAM&lt;/li&gt;&lt;li&gt;STR sequences were then extracted from the read sequences&lt;/li&gt;&lt;li&gt;Next I created lists of indexes to identify repeat units and interruptions within the STR sequences (utilising Regex)&lt;/li&gt;&lt;li&gt;All the information was then added to a summary TSV that could be used as input for the plot&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;em&gt;Week 5&lt;/em&gt;&lt;/p&gt;&lt;p&gt;I spent week 5 working on the code for the plot itself. We decided to utilise Bokeh as it provided the best starting point for the plot. Progress pictures below…&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:55.78947368421052%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAACK0lEQVR42oWRW08aQRiG+d29qklNatNetIneNCZNa60nDisCCiyIWg9FFBsrp11g2QUq7IptWgSUffq5mvaOTvIm78z3zTPvzPhuRyOG/RsmjX6/j23bOI5Dr9eb2Otr1usUc6cTm1zXZTwe/9VEYKtjk8przKcvuHP57xi7D0Dd1nl3OI91XqF0kvdCOfULATo9omc1pgJZRo+HuxPTPjQZV1XeH88R3Q+Sy+ySz3zmeymLz7rsEsmVeLl+zHAwwB3dcDe8YTwQjQaMhwPPu56X2qAPd7dcX7fYqyrMZd+wsx/n/OCIjpaXN+w4xL4aTK0coG0qtJLrmFtBrHiI+maAsvIJK6lgJmQeXcWQmhkNou8pJLQ1Xm0/51nqKel0lPb5Mb5G+5LIica0/4haIkxLDVONrnmqy+ZqZFXAQWqxAPrGigdsCLCyq3BoxUiW1niRnmY2/ZpCSb2/si1A3QMaaoSmJKzF/NRFjbgiqfwe2EyIjz0cYkQl+V6IXDtO3kqyePKWmZ1pssV1+ZTuFZvyKTOBL9RViZ3a8DY3JWlT/H0iS3x3dwtLrm2pUt9S0A/CnF6qnJoJzpoq89lZjgsb+Ix2ByWn8WQhQyXipxpeouj/QDm4SDkkCizI2jIVectyQNZDH6kGl/imLrOtBciZMVIX4kurNIwMvp+/fqO1bY6KJk5Vw9FL2Frxn2TuPMrWCnQqBexKkY5RptEt03R0Gp0ypvgf1xZ/AOZ2DDu+dXCCAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;periodic table&quot; title=&quot;Interestingly, a Periodic Table plot (https://docs.bokeh.org/en/0.11.1/docs/gallery/periodic.html) in the Bokeh gallery served as a great starting point to create the STR content plot!&quot; src=&quot;/static/a256a1d8dcec6711665ffdf3f0edcf35/b5cea/periodic.png&quot; srcSet=&quot;/static/a256a1d8dcec6711665ffdf3f0edcf35/0e2fe/periodic.png 285w,/static/a256a1d8dcec6711665ffdf3f0edcf35/432e7/periodic.png 570w,/static/a256a1d8dcec6711665ffdf3f0edcf35/b5cea/periodic.png 1140w,/static/a256a1d8dcec6711665ffdf3f0edcf35/844cc/periodic.png 1306w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Interestingly, a Periodic Table plot (https://docs.bokeh.org/en/0.11.1/docs/gallery/periodic.html) in the Bokeh gallery served as a great starting point to create the STR content plot!&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:660px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:62.10526315789474%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAA7DAAAOwwHHb6hkAAABIUlEQVR42qWT3XKEIAyFff+H7MX2ou0qyK+KIKcJsq5ubbudOvNN4hgOxxAaoQxaZSGthw8R7kfSFodoseSIDCDnTKA8TWccxDBD+IDOEbfodu+P3yi+GQs1hiJup1hg0UYYD0mCkopOGSo15417qn/pPMZ5weNDDldBLizURb/RugmGXPk5bS05OPyLINdfJPc8IZHJOWXEtNx7uBeUTzq82oAYPLCE//8yb/qqRghjYAdPv55wEZ7c5uOhiHoQh8W7uHfPB6Oph6awrufx+erwSXh0uIdTXOi0OaYyk6djs3cnT/Jba9wci9hQKYJ8S7o6Bt8OdsmnLefaluI6NvebVARluXpEr/EhelylJlTJj6gK5fT9naLSGrqilIJzDp+ADJ90laYweQAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;version 1 plot&quot; title=&quot;Version 1 of the plot still required a lot of work but it was exciting to see the plot starting to come to life with interruptions showing in Dark Blue, Orange, and Green.&quot; src=&quot;/static/78cefd4bb934e02d9ddc426ec0f85a67/1f083/v1-repeat-plot.png&quot; srcSet=&quot;/static/78cefd4bb934e02d9ddc426ec0f85a67/0e2fe/v1-repeat-plot.png 285w,/static/78cefd4bb934e02d9ddc426ec0f85a67/432e7/v1-repeat-plot.png 570w,/static/78cefd4bb934e02d9ddc426ec0f85a67/1f083/v1-repeat-plot.png 660w&quot; sizes=&quot;(max-width: 660px) 100vw, 660px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Version 1 of the plot still required a lot of work but it was exciting to see the plot starting to come to life with interruptions showing in Dark Blue, Orange, and Green.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:878px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:53.68421052631579%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAACBklEQVR42m2TDW+aUBiF/f//YsmSbVnXrlu7dku2ZQ0KIkxtq6Io13tFUJFPi18Yz16pbVYr5OQGbu6Tc84Lha4r0LYnMAZjdEcCTSeEHc3hRAu48aN6/gQiCuCncyzWa2y3W+zuw2u5nKNQdU2obAhVhFCHPZzULPw0Hfw2PZT6AcmHxDzcMBdS38Flpwbmz3LAIfJhviCg00eZBwT0oTkdnFQ5ZILUnAjVYYi/A3ovJrTv4NrU8dmQ8aN7By9NXkHXWYZCzeVQeQyNDqp2Cx+0PhRyJijqOH7AjGLG8xSL5QrWdIRzQ8Fpq4g/rI315iByttkBGQEjcuFDd5p4pzEC+qiTwzs3RnOckGb52hil+EIOrzo6zpolDOPVK5cEtChyRJGCHPi2YuW97Z4V7j9L5lOog4hgVVy2KzhtSjC95AjQEfvIHhRh5B3uIms0pMp/0kSMIg3vu3mLr+0yLgh6YWgI0s0LaKGeR47pEEGGLbynDuUjwN2+PkhwZVYJpuYuz1oSipztu9w+AZ+GQgf2QOUoMMyBErPxraMRUCW4jk8U/blLYhZ0mzqzQurJgzy4xxulB6nn54OR2UuV2JT6TnDeKpM7hVTCx8YNfvXaSFfZo0M79MDpQ+XTBHY4Qp2+OebFEH6Cvhft1zjf5z6tNAg2DdCgqowxJwncOxb9WQGyLMM/XKoiWHdGomAAAAAASUVORK5CYII=&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;example 2 plot&quot; title=&quot;A later example of the plot in development. This one shows some interesting interruption patterns.&quot; src=&quot;/static/2036c15880a93b9c70c453964f872922/94829/repeat-plot-example-2.png&quot; srcSet=&quot;/static/2036c15880a93b9c70c453964f872922/0e2fe/repeat-plot-example-2.png 285w,/static/2036c15880a93b9c70c453964f872922/432e7/repeat-plot-example-2.png 570w,/static/2036c15880a93b9c70c453964f872922/94829/repeat-plot-example-2.png 878w&quot; sizes=&quot;(max-width: 878px) 100vw, 878px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;A later example of the plot in development. This one shows some interesting interruption patterns.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Week 6&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Finally, in week 6 I worked on integrating my code into EPI2ME’s &lt;code&gt;wf-human-variation&lt;/code&gt;. This involved converting my Python code into Nextflow and thinking about how best to display the plot to the users.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:71.9298245614035%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABYlAAAWJQFJUiTwAAABwklEQVR42qVSy07bQBT13yAh6AIQ/BCqihQeC4SgUDb9hi6QEO2OHYglT0GoQGzatGogIKA0zsOJE8exMvb4lcOd8ZAQlS5SLB1d33uuj8+9M9qb1HsMTM5j8O0Cht4tYvhfmFrC+OwqRqdXMDFHcfUTxj+uYyS1grGZD5IbSS1D8zmH3bDAKUZRiDAMEAQBxVAiCiNZe8o7ED2qL1K1drsNjXkcDnPhugTPA/d9eFyA0098CZd6RC3huEI356o/IFEtiGIwUSBBRojjGP/7SIfixbZtlI0K6pYlx321YOAHaDkOTIfBbvGE7FMoUpNpIhFAHOHBbKJQtTtN/QiGdHhdQUWUGwxGzVFNr3KYELrVQpFc9uuwZ4fPBUuNFkpKMI6TVTzx3aiAXrwomK8z2mGz75FfcJh83Wha+J6+wO+bPPxaEV5NBzP/IK+bMO4eqF6AXndRo5sg7qs8y2eG/hIs2Q4+72WwuZvB1lkG2+ci/sB6+habZ1l8Oc1h7eQeG1/vsf2tgJ1MEVsUj6+rHZc9I5uOi6OsjsPrCvazBg4uq4QKjnMGDq8qOLkqI50r4+iyhINfRewT9n7qMmd+KAUfAaGdGiPh4ftiAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;example 3 plot&quot; title=&quot;An example of the plot showing in the demo wf-human-variation report.&quot; src=&quot;/static/97359bee74b25d7e35a5cacd24070dd2/b5cea/repeat-plot-example-3.png&quot; srcSet=&quot;/static/97359bee74b25d7e35a5cacd24070dd2/0e2fe/repeat-plot-example-3.png 285w,/static/97359bee74b25d7e35a5cacd24070dd2/432e7/repeat-plot-example-3.png 570w,/static/97359bee74b25d7e35a5cacd24070dd2/b5cea/repeat-plot-example-3.png 1140w,/static/97359bee74b25d7e35a5cacd24070dd2/9cea8/repeat-plot-example-3.png 1278w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;An example of the plot showing in the demo wf-human-variation report.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;h3 id=&quot;other-highlights&quot;&gt;Other Highlights&lt;/h3&gt;&lt;p&gt;Outside of gaining new technical skills through the repeat expansion project it has also been really interesting to spend time with a team of Industry Bioinformaticians. Industry and Clinical Bioinformaticians have slightly different considerations in their day-to-day work, however, many of the technical and project management tools that we utilise overlap, and so there are always things we can learn from each other. As the workflows developed by the EPI2ME team are open-source and utilised by many people outside their department (not just internally as in the NHS) they have a strong focus on user feedback and usability testing. We also have different accreditation standards that we need to meet.&lt;/p&gt;&lt;p&gt;Another highlight of the elective was having the opportunity to visit the ONT labs during my first week. One of ONT’s Technical Product Managers showed me how to undertake library preparation and load a GridION Flow Cell. It was fascinating to see the real-time sequencing analysis via MinKNOW (Watching things like the channel states panel and the cumulative output). The next day I was able to access the sequencing data which I could then run through an EPI2ME workflow to see the whole end-to-end process.&lt;/p&gt;&lt;p&gt;Overall, this has been a really valuable experience and I want to thank the EPI2ME team for having me!&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1000px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:66.66666666666666%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAUCAwT/xAAWAQEBAQAAAAAAAAAAAAAAAAABAAL/2gAMAwEAAhADEAAAAdsC7SlGoX//xAAbEAADAAIDAAAAAAAAAAAAAAABAgMAIQQiQf/aAAgBAQABBQLS5SKUw8cqYr09L7//xAAYEQACAwAAAAAAAAAAAAAAAAABAhAREv/aAAgBAwEBPwFlyaj/xAAVEQEBAAAAAAAAAAAAAAAAAAAREP/aAAgBAgEBPwFZ/8QAGBAAAgMAAAAAAAAAAAAAAAAAABEBITH/2gAIAQEABj8CL2BDm2Iw/8QAGhABAQEBAQEBAAAAAAAAAAAAAREAITFRcf/aAAgBAQABPyGRrB5fmFCoc0or+GNqr3cBy8DdJDOb/9oADAMBAAIAAwAAABD87//EABcRAQEBAQAAAAAAAAAAAAAAAAERAFH/2gAIAQMBAT8QBQ2anN//xAAYEQACAwAAAAAAAAAAAAAAAAAAIQERUf/aAAgBAgEBPxBFyLT/xAAbEAEAAwEAAwAAAAAAAAAAAAABABEhMUFR8f/aAAgBAQABPxAomFiULxfqOC6rM+xqInGLBx7GiCZUJAjCrHT1n//Z&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;flowcell loading&quot; title=&quot;Loading a flowcell onto the Oxford Nanopore Technologies MinION.&quot; src=&quot;/static/ca3feb96a787345b02c4a87f4ba09788/a2510/flowcell.jpg&quot; srcSet=&quot;/static/ca3feb96a787345b02c4a87f4ba09788/6c669/flowcell.jpg 285w,/static/ca3feb96a787345b02c4a87f4ba09788/882a4/flowcell.jpg 570w,/static/ca3feb96a787345b02c4a87f4ba09788/a2510/flowcell.jpg 1000w&quot; sizes=&quot;(max-width: 1000px) 100vw, 1000px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Loading a flowcell onto the Oxford Nanopore Technologies MinION.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/2b7396c37d2337d5bd1ea28495bf0a80/59ccf/bioinformatics.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/2b7396c37d2337d5bd1ea28495bf0a80/bioinformatics.jpeg</content:toenail></item><item><title><![CDATA[Coding everything, everywhere, all at once]]></title><description><![CDATA[A post to describe the experience of the VIB-UAntwerp - Babraham Institute Hackathon.]]></description><link>https://labs.epi2me.io/vib-uantwerp-babraham-institute-hackathon</link><guid isPermaLink="false">https://labs.epi2me.io/vib-uantwerp-babraham-institute-hackathon</guid><pubDate>Fri, 15 Sep 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/73d135d44532df3e5ead7e4e8a1341c9/59ccf/komodo-dragon.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Finally another hackathon! A new challenge for the adventurous coder and great opportunity to boost one’s skills while meeting a bunch of friendly people who all share our passion for the life sciences and computational biology!&lt;/p&gt;&lt;p&gt;Oxford Nanopore Technologies proudly supports the &lt;a href=&quot;https://vmalysheva.github.io/VIB_BI_Hackathon/&quot;&gt;VIB-BI Joint Hackathon&lt;/a&gt;, organised by the &lt;a href=&quot;https://uantwerpen.vib.be/&quot;&gt;VIB-UAntwerp Center for Molecular Neurology&lt;/a&gt; (Antwerp, Belgium) and the &lt;a href=&quot;https://www.babraham.ac.uk/&quot;&gt;Babraham Institute&lt;/a&gt; in Cambridge, UK. As a joint event, the hackathon took part simultaneously in both locations (Sept. 11&lt;sup&gt;th&lt;/sup&gt;–13&lt;sup&gt;th&lt;/sup&gt;, 2023), gathering an international crowd of bioinformatics specialists and computer scientists from all over Belgium and the UK.&lt;/p&gt;&lt;p&gt;The hackathon allowed participants to bring their own projects so that other scientists could help add new features or optimise performance. It also represented an excellent opportunity to escape the daily routine and finally spend some quality coding time on developing that new tool one might have been thinking about (or simply to dabble with a fancy new programming language to see what all the buzz is about). Such a chance we could of course not let slip by and sent several members of our team to Cambridge as well as Antwerp.&lt;/p&gt;&lt;p&gt;The breadth and diversity of topics at the hackathon were truly inspiring. Let us briefly mention just a few examples of the projects undertaken: one implemented a Nextflow workflow to perform the merging of SVs within- and across-samples while another used current deep learning methods to trace influencer–influencee interactions across art history and a third attempted to overcome performance limitations in a popular package used for handling genomic ranges. Only the sky was the limit of the participants’ ambitions.&lt;/p&gt;&lt;p&gt;So what does a hackathon look like? You might have the stereotypical idea of a room of nerds, sitting in front of their computers and quietly typing away, and, well, sometimes it is like that, but there is also a great deal of laughter, chatting, friendly competition (in coding challenges and quizzes), sharing ideas, forging collaborations, and simply an amazing amount of enthusiasm for the common goal of advancing science and creating tools to probe difficult biological questions.&lt;/p&gt;&lt;p&gt;We also very much enjoyed that this was an &lt;em&gt;in person&lt;/em&gt; event. Aristotle is supposed to have said ‘Man is by nature a social animal’ and, when it comes to discussing novel ideas or arguing over the best approaches to tackle a problem, nothing beats sitting next to each other; either during the day or in the evenings with a drink in hand. Inspiration usually strikes in the most unexpected moments and having a pleasant dinner with collaborators is as delightful an opportunity to brainstorm and network as is sitting in the computer room programming!&lt;/p&gt;&lt;p&gt;This was a very pleasant experience all-round and we would like to use these lines to encourage everyone to, when next time you hear about such an event, leave any reservations aside and join the fun!&lt;/p&gt;&lt;p&gt;Andrea Talenti, Julian Libiseller-Egger, Natalia Garcia, Neil Horner, Sarah Griffiths&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/73d135d44532df3e5ead7e4e8a1341c9/59ccf/komodo-dragon.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/73d135d44532df3e5ead7e4e8a1341c9/komodo-dragon.jpeg</content:toenail></item><item><title><![CDATA[EPI2ME 23.09-01 Release]]></title><description><![CDATA[An large release including updates to the EPI2ME application and workflows.]]></description><link>https://labs.epi2me.io/epi2me-23.09-01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-23.09-01-release</guid><pubDate>Wed, 06 Sep 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/631b1f37c4725964b03f2350fc9dffe4/59ccf/release_whale.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Dear Nanopore Community,&lt;/p&gt;&lt;p&gt;The Customer Workflows team hope that our users in the northern hemisphere had a great summer break. We have been busy and in this post-holidays release we are delighted to introduce an updated version of our EPI2ME desktop application, a brand-new 16S/18S rRNA workflow, and a healthy collection of workflow updates.&lt;/p&gt;&lt;h3 id=&quot;platform&quot;&gt;Platform&lt;/h3&gt;&lt;p&gt;The EPI2ME desktop application has been updated to v5.1.2. This update includes a collection of updates and fixes - including improved WSL integration (for our Windows users). This update is available for Windows, macOS, and Linux - EPI2ME desktop can be downloaded from &lt;a href=&quot;https://labs.epi2me.io/downloads/&quot;&gt;https://labs.epi2me.io/downloads/&lt;/a&gt;.&lt;/p&gt;&lt;h3 id=&quot;workflows&quot;&gt;Workflows&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;New workflow&lt;/strong&gt;: &lt;a href=&quot;https://github.com/epi2me-labs/wf-16s&quot;&gt;wf-16s&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-16s/blob/master/CHANGELOG.md&quot;&gt;v0.0.1&lt;/a&gt;) simplifies your 16S/18S/ITS amplicon sequencing data analyses. The new workflow is based entirely on &lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot;&gt;wf-metagenomics&lt;/a&gt; and is configured with example data, rRNA databases and parameters to help you get started with your 16S rRNA sequencing adventures. Feedback, as always, is welcome. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation&quot;&gt;wf-clone-validation&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation/blob/master/CHANGELOG.md&quot;&gt;v0.5.1&lt;/a&gt;) now outputs a gbk file describing the feature annotations that may be used with downstream tools. A collection of minor fixes are also included.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-pore-c&quot;&gt;wf-pore-c&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-pore-c/blob/master/CHANGELOG.md&quot;&gt;v0.1.1&lt;/a&gt;) now has a &lt;code&gt;--hi_c&lt;/code&gt; parameter to create and outputs a &lt;code&gt;hi_c&lt;/code&gt; file that is compatible with downstream juicer contact map visualisation tools. Other fixes and changes are described in the linked changelog.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes/&quot;&gt;wf-transcriptomes&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes/blob/master/CHANGELOG.md&quot;&gt;v0.3.1&lt;/a&gt;) includes a bug fix to enable the workflow to accommodate input transcriptome files which contain the &lt;code&gt;|&lt;/code&gt; symbol in their headers.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation&quot;&gt;wf-somatic-variation&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation/blob/master/CHANGELOG.md&quot;&gt;v0.4.0&lt;/a&gt;) adds modified base aggregation with &lt;a href=&quot;https://github.com/nanoporetech/modkit/&quot;&gt;modkit&lt;/a&gt;, differentially modified loci and region detection with DSS, and functional annotation for SNV and small indels. These annotations are prepared using snpEff. The workflow now also supports non-human species.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation&quot;&gt;wf-human-variation&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation/blob/master/CHANGELOG.md&quot;&gt;v1.8.0&lt;/a&gt;) adds &lt;em&gt;LRP12&lt;/em&gt; (a gene harbouring an expansion in amyotrophic lateral sclerosis (ALS) - details &lt;a href=&quot;https://www.cell.com/ajhg/fulltext/S0002-9297(23)00198-2&quot;&gt;here&lt;/a&gt;) to the genotyped STR repeats, replaces the &lt;a href=&quot;https://github.com/epi2me-labs/modbam2bed&quot;&gt;modbam2bed&lt;/a&gt; with &lt;a href=&quot;https://github.com/nanoporetech/modkit/&quot;&gt;modkit&lt;/a&gt; and renames the &lt;code&gt;--methyl&lt;/code&gt;/&lt;code&gt;--phase_methyl&lt;/code&gt; options with &lt;code&gt;--mod&lt;/code&gt;/&lt;code&gt;--phase_mod&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling/&quot;&gt;wf-basecalling&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling/blob/master/CHANGELOG.md&quot;&gt;v1.1.0&lt;/a&gt;) adds dorado v0.3.4 and auto-conversion of &lt;code&gt;FAST5&lt;/code&gt; to &lt;code&gt;POD5&lt;/code&gt; when performing duplex calling.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/pychopper&quot;&gt;pychopper&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/pychopper/blob/master/CHANGELOG.md&quot;&gt;v2.7.7&lt;/a&gt;):&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Previous versions only filtered reads by length if an output path for filtered reads was specified with &lt;code&gt;-l&lt;/code&gt;. This release fixes this undocumented behaviour and will now always filter reads by the length set with &lt;code&gt;-z&lt;/code&gt; default 50.&lt;/li&gt;&lt;li&gt;Bug fixes for UMIs containing N in small reads,  and incorrect mean quality calculation resulting in too many reads passing quality filter. &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell&quot;&gt;wf-single-cell&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell/blob/master/CHANGELOG.md&quot;&gt;v0.2.7&lt;/a&gt;) now copies the mitochondrial expression file now to output folder. Polars thread usage is now set correctly.  The geneName attribute is now allowed in GTF annotation files.  &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot;&gt;wf-metagenomics&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics/blob/master/CHANGELOG.md&quot;&gt;v2.5.0&lt;/a&gt;): &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;An option to remove host reads has been included with the &lt;code&gt;--exclude_host&lt;/code&gt; parameter. The user can provide either a FASTA file or MMI index, the sequence reads that map to this reference will be excluded from downstream metagenomic classification. The &lt;code&gt;BAM&lt;/code&gt; file with the excluded reads is included as a result.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The option &lt;code&gt;--include_kraken2_assignments&lt;/code&gt; will produce a per sample TSV file that indicates how each input sequence was classified as well as the taxon that has been assigned to each read. This TSV file will only be output at the completion of the workflow and therefore not at all if using the real time option running indefinitely.  Available in the kraken2 pipeline.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The new option, &lt;code&gt;--kraken_confidence&lt;/code&gt;, specifies a threshold score in the kraken2 pipeline. See &lt;a href=&quot;https://github.com/DerrickWood/kraken2/wiki/Manual#confidence-scoring&quot;&gt;this  document&lt;/a&gt; for more information.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;For the report, we have included &lt;code&gt;--abundance_threshold&lt;/code&gt; (if it is a natural number it removes from the table those taxa with fewer counts; however to remove those taxa below a given percentage - use a value between 0-1) and &lt;code&gt;--n_taxa_barplot&lt;/code&gt; controls the number of taxa displayed in the barplot and groups the rest under the category ‘Other’.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All of our workflows are open source. Would you like to contribute a feature? Please let us know how you think our content could be made even better.&lt;/p&gt;&lt;p&gt;Thanks for listening!&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/631b1f37c4725964b03f2350fc9dffe4/59ccf/release_whale.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/631b1f37c4725964b03f2350fc9dffe4/release_whale.jpeg</content:toenail></item><item><title><![CDATA[EPI2ME 23.08-01 Release]]></title><description><![CDATA[An minor release including small UI fixes to the EPI2ME application and workflow updates.]]></description><link>https://labs.epi2me.io/epi2me-23.08-01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-23.08-01-release</guid><pubDate>Thu, 10 Aug 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/001ab376b5bfb5c384ad08a397d1d331/59ccf/release-croc.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Dear Nanopore Community,&lt;/p&gt;&lt;p&gt;We’re please to bring a small set of updates to our Nextflow workflows. This release also lays the foundations for future user interface updates in the area of workflow setup.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;wf-basecalling &lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling/releases/tag/v1.0.0&quot;&gt;v1.0.0&lt;/a&gt; includes and updated version of dorado and automates the efficient generation of duplex reads.&lt;/li&gt;&lt;li&gt;wf-human-variation &lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation/releases/tag/v1.7.0&quot;&gt;v1.7.0&lt;/a&gt; contains a veritable smörgåsbord of updates covering improvements to automated annotation, downsampling and joint phasing.&lt;/li&gt;&lt;li&gt;wf-clone-validation &lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation/releases/tag/v0.5.0&quot;&gt;v0.5.0&lt;/a&gt; contains an &lt;a href=&quot;https://labs.epi2me.io/workflows/wf-metagenomics-report.html&quot;&gt;updated report&lt;/a&gt; and &lt;a href=&quot;/inkling&quot;&gt;dedicated support&lt;/a&gt; for Apple silicon Mac computers (and ARM devices in general).
&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The EPI2ME software release is available from the &lt;a href=&quot;/downloads&quot;&gt;downloads&lt;/a&gt; page. Current users should have been notified of the update within the application.&lt;/p&gt;&lt;p&gt;As a reminder we are currently soliciting early access users for the new cloud analysis functionality in the EPI2ME desktop application that will launch in 2024. You can register your interest &lt;a href=&quot;https://register.nanoporetech.com/epi2me-cloud&quot; title=&quot;https://register.nanoporetech.com/epi2me-cloud&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;We look forward to any feedback and welcome recommendations for workflows and EPI2ME features that could be included in future releases.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/001ab376b5bfb5c384ad08a397d1d331/59ccf/release-croc.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/001ab376b5bfb5c384ad08a397d1d331/release-croc.jpeg</content:toenail></item><item><title><![CDATA[The road not taken]]></title><description><![CDATA[A ramble through the undergrowth on a road to bioinformatics on ARM64 devices.]]></description><link>https://labs.epi2me.io/inkling</link><guid isPermaLink="false">https://labs.epi2me.io/inkling</guid><pubDate>Wed, 09 Aug 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/78d100efddafbf9215dc107b302fbe98/59ccf/inkling-river.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Three years ago I foresaw a cloud arriving on the bioinformatics horizon. Apple were about to
launch their new line of hardware
that would include their own system-on-a-chip, &lt;a href=&quot;https://en.wikipedia.org/wiki/Apple_M1&quot;&gt;Apple M1&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The release of M1 powered computers would be the third time that Apple changed the CPU architecture used in their products. For the
previous 14 years Apple had used processors from Intel in its laptop and desktop hardware. These were
the same Intel processors ubiquitously used across the rest of the computer industry from home computers
to high-performance computer clusters.&lt;/p&gt;&lt;h3 id=&quot;two-roads-diverged&quot;&gt;Two roads diverged&lt;/h3&gt;&lt;p&gt;For the living memory of many (but not all) people working in the bioinformatics space,
distributing software that could run on both Apple and non-Apple computers had been made less
complicated by Apple’s choice to use Intel processors. There were certainly still wrinkles when
trying to compile code to run on on macOS compared to Linux operating systems (let’s not get
into the Windows thing here), but things were certainly easier than when Apple used
&lt;a href=&quot;https://en.wikipedia.org/wiki/Mac_transition_to_Intel_processors&quot;&gt;PowerPC&lt;/a&gt; processors.
With the introduction of these new powerful and efficient M1 processors from Apple there
would be the task of compiling code to run on them.&lt;/p&gt;&lt;p&gt;Before the fanboys get flustered, let’s talk about &lt;a href=&quot;https://en.wikipedia.org/wiki/Rosetta_(software)&quot;&gt;Rosetta&lt;/a&gt;.
Apple &lt;a href=&quot;https://web.archive.org/web/20101116094453/http://www.apple.com/asia/rosetta/&quot;&gt;once&lt;/a&gt; described Rosetta as
“The most amazing software you’ll never see.” This is a bold claim, with some justification. Without
getting too technical, Rosetta allows current Apple hardware to run computer code created for
Intel processors to run on M1 (and M2) processors. It implements something called “dynamic binary
translation”: processor instructions are translated from the language of Intel processors (x86-64) to
that of M1 processors (AArch64, also known as ARM64) whilst a program is running. Rosetta allows code created for earlier
Intel-based Apple computers to run on current ARM-based systems.&lt;/p&gt;&lt;p&gt;The binary translation afforded by Rosetta is not however perfect. Certain extensions to the x86-64
language used by Intel processors are &lt;a href=&quot;https://developer.apple.com/documentation/apple-silicon/about-the-rosetta-translation-environment#What-Cant-Be-Translated&quot;&gt;not supported&lt;/a&gt;
by Rosetta. This means various programs simply will not run without being recompiled to run natively
on ARM processors. The situation is worse than this in fact because of the dynamic nature of
Rosetta: programs will run happily until the point at which an instruction that Rosetta cannot
handle is encountered. When this happens the program will abruptly exit, often without any indication
as to the cause.&lt;/p&gt;&lt;p&gt;“This is all great Chris, but aren’t you being a bit melodramatic? We’re all still running things
just fine on our MacBooks.” Yes, yes you are. Rosetta truly is a marvel, accept when it isn’t. Its
a useful technology whilst developers play catchup and release their software to run natively
on ARM processors. The use of Rosetta is not free. Aside from the issue of not supporting all x86-64
instructions such that not all programs will run, there is a performance cost to use of Rosetta.
In our experiments below, recompiling a set of commonly used tools to run natively without Rosetta leads
to at worst a 2x performance improvement.&lt;/p&gt;&lt;h3 id=&quot;then-took-the-other-as-just-as-fair&quot;&gt;Then took the other, as just as fair&lt;/h3&gt;&lt;p&gt;On the surface recompiling bioinformatics tools to run natively on ARM would seem fairly simple. Just
grab your favourite software, run the compilation commands, and voila some nice new ARM binary files
ready to run. As it happens growing ARMs is not so easy in practice. We quickly find that many
software projects have been written in ways that mean they are intimately coupled to running on x86-64
processors. Add in the fact that many projects depend on other projects and you quickly find yourself
bent in the undergrowth.&lt;/p&gt;&lt;p&gt;To make the job somewhat easier we can make use of available public repositories of pre-compiled
software libaries. Unfortunately the go-to repository of bioinformatics software,
&lt;a href=&quot;https://bioconda.github.io/index.html&quot;&gt;bioconda&lt;/a&gt;, does not build any software for ARM64! This is the
cloud I saw on the horizon back in November 2020: without Rosetta duck-taping the ship together,
bioinformatics on Mac would be a no go for many. Theres little impetus to provide native packages,
with maintainers hiding behind the excuse, “Rosetta will take care of it.”&lt;/p&gt;&lt;p&gt;For software that doesn’t run with a helping hand from Rosetta the only option for end users
is to compile software themselves. This is beyond the skill of many. Getting a bioinformatics pipeline
running with multiple pieces of software all compiled to native ARM64 code is not trivial. After one
developer has gone through the pain of creating ARM software packages however, the results
can be shared with everyone. The premise of package libraries like bioconda is exactly this:
a community of package maintainers build executable code from source code for everyone’s use.&lt;/p&gt;&lt;p&gt;With this in mind we started project &lt;a href=&quot;https://octonauts.fandom.com/wiki/Professor_Inkling&quot;&gt;inkling&lt;/a&gt;.
The idea is to steadily and progressively create ARM conda packages for the software used
within our &lt;a href=&quot;/wfindex&quot;&gt;Nextflow workflows&lt;/a&gt;. To achieve this we make use of our existing
conda packaging continuous integration pipelines that we have used for packaging our own software
projects such as &lt;a href=&quot;www.github.com/epi2me-labs/fastcat&quot;&gt;fastcat&lt;/a&gt; and
&lt;a href=&quot;www.github.com/epi2me-labs/modbam2bed&quot;&gt;modbam2bed&lt;/a&gt;. These pipelines are backed by a set of
Linux virtual machines and Apple devices running on both x86-64 and ARM64 hardware. These
machines provide us with the ability to create a total of four different software builds which
we push to our &lt;a href=&quot;https://anaconda.org/nanoporetech/repo&quot;&gt;anaconda repository&lt;/a&gt;.&lt;/p&gt;&lt;h3 id=&quot;yet-knowing-how-way-leads-on-to-way&quot;&gt;Yet knowing how way leads on to way&lt;/h3&gt;&lt;p&gt;One of our most popular workflows is
&lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation&quot;&gt;wf-clone-validation&lt;/a&gt;, which can be used
for the de novo assembly and annotation of plasmid sequences. So let’s try to compile all the
software it requires for ARM. At its core the workflow uses
&lt;a href=&quot;https://github.com/fenderglass/Flye&quot;&gt;flye&lt;/a&gt; and &lt;a href=&quot;https://github.com/rrwick/Trycycler&quot;&gt;trycyler&lt;/a&gt;
to first create a high quality assembly before inspection and annotation with
&lt;a href=&quot;https://github.com/mmcguffi/pLannotate&quot;&gt;pLannotate&lt;/a&gt;. So three packages to build for ARM on
macOS and Linux. Not quite. Each of these, notably trycycler and pLannotate depend on multiple
other software libraries and packages. Not all of these are required for our use case in
wf-clone-validation but without deconstructing the original software components it is necessary
to also compile the dependencies for ARM. To do otherwise would lead to ARM packages without
the full functionality of the exisiting x86-64 packages. All-in-all we ended up needing
to create more than 20 new ARM packages! (Various non-specific libraries are available for
ARM already through &lt;a href=&quot;https://conda-forge.org/&quot;&gt;conda-forge&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;Building all these packages was not a terribly exciting affair, certainly not a &lt;a href=&quot;https://twitter.com/chrisnrg/status/1687178403567349761?s=20&quot;&gt;spectator
sport&lt;/a&gt;. But neither was it
objectively difficult given patience and perseverance. Starting from one of the direct
dependencies of wf-clone-validation (say flye) we can copy the
&lt;a href=&quot;https://github.com/bioconda/bioconda-recipes/tree/master/recipes/flye&quot;&gt;recipe&lt;/a&gt;
(package build instructions) from bioconda and run the build process on our ARM
continuous integration machines to give us shiny new ARM conda packages. However for many
packages this process doesn’t work out of the box. As noted above in order to build
a package successfully, its dependencies must be available. A usual build failure is
to find prerequisite packages need to be built first: so to build one package
we must first build others, potentially in a recursive fashion. In creating the
package hierarchy for wf-clone-validation we reached four layers deep of dependencies
that needed to be built.&lt;/p&gt;&lt;p&gt;Perhaps more interesting an issue is when the source code of a piece of software
is written in a manner which does not permit compilation on ARM. This is where we have
to get our hands dirty. A most common issue is when code has been used to explicitly
use particular extensions to the x86 instruction set. Often developers will use
&lt;a href=&quot;https://en.wikipedia.org/wiki/Intrinsic_function&quot;&gt;intrinsic functions&lt;/a&gt; in their code
to aid runtime performance. However such functions are generally not portable across
different types of processor such as x86 to ARM: code must be rewritten
if it is to work on multiple processor architectures. Fortunately there are special
software libraries that can help with this. For example &lt;a href=&quot;https://github.com/simd-everywhere/simde&quot;&gt;SIMD Everywhere&lt;/a&gt;
and &lt;a href=&quot;https://github.com/DLTcollab/sse2neon&quot;&gt;sse2neon&lt;/a&gt; are two tools for allowing the
use of so called &lt;a href=&quot;https://en.wikipedia.org/wiki/Single_instruction,_multiple_data&quot;&gt;SIMD&lt;/a&gt;
function intrinsics in a portable manner.&lt;/p&gt;&lt;p&gt;Tool developers who are on-the-ball will already be using these tools, so creating
and conda package from the source code is typically simply a matter of enabling
options during the build process. This was the case for tools such as
&lt;a href=&quot;https://github.com/lh3/minimap2&quot;&gt;minimap2&lt;/a&gt; (sse2neon) and
&lt;a href=&quot;https://github.com/rvaser/spoa&quot;&gt;spoa&lt;/a&gt;/racon (SIMDe).
Other times we found that we had to perform the work ourselves in order to create
a working software build. It always pays to ask the software developers before
embarking on anywork however: I spent a morning doing this for one tool, only to
find the developer had &lt;a href=&quot;https://github.com/EddyRivasLab/infernal/issues/30#issuecomment-1662351606&quot;&gt;already done&lt;/a&gt;
the work on a branch of their code.&lt;/p&gt;&lt;p&gt;A note of caution to anyone embarking on a similar ramble. An issue you will almost
certainly encounter at some stage is openssl versions being worn really about the
same. Various conda packages will require different, incompatible versions of
openssl with the result that some packages cannot be simultaneously used. In our
case we found that the project &lt;a href=&quot;https://github.com/capnproto/capnproto&quot;&gt;capnproto&lt;/a&gt;
only recently &lt;a href=&quot;https://github.com/capnproto/capnproto/pull/1596&quot;&gt;enabled&lt;/a&gt;
support for openssl version 3.0.8 in addition to version 1.1.1. Unfortunately
there remain some &lt;a href=&quot;https://github.com/conda-forge/capnproto-feedstock/pull/32&quot;&gt;issues&lt;/a&gt;
with compiling the 3.0.8 supporting capnproto with the conda toolchain. To resolve this,
and because we do not need openssl support we broke our own rule from above and
created a boutique
&lt;a href=&quot;https://anaconda.org/nanoporetech/capnproto-nossl&quot;&gt;capnproto-nossl&lt;/a&gt; package.&lt;/p&gt;&lt;p&gt;All-in-all in order to create the necessary packages for wf-clone-validation
we had to make patches to the source code of around a third of the projects. In
addition we had to make alterations to around two-thirds of the build recipes.&lt;/p&gt;&lt;h3 id=&quot;i-shall-be-telling-this-with-a-sigh&quot;&gt;I shall be telling this with a sigh&lt;/h3&gt;&lt;p&gt;So was all the effort worth it? We can judge this by whether the effort makes a difference
to end users. To do this we can run the wf-clone-validation on Apple hardware using
both the standard x86-64 compiled software and our new ARM64 code. The benchmarks
were run on a MacBook with an M1 Max processor (Model &lt;a href=&quot;https://support.apple.com/kb/SP854?locale=en_US&quot;&gt;A2442&lt;/a&gt;). For comparison we show also results using a Intel i7-11800H based device running Microsoft Windows. This processor is of a similar vintage to the M1 device and can be found in high-end laptops of late 2021. &lt;/p&gt;&lt;p&gt;The input data used to obtain the timings was
the demonstration data included with the workflow: a dataset comprising three samples, two of which are intended to not pass QC steps of the workflow.
Figure 1. shows the execution time of the core steps of the workflow. These are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Plasmid assembly using the flye assembler and Trycyler (which using a variety of tools to clean up reconcile assemblies).&lt;/li&gt;&lt;li&gt;Consensus calculation using &lt;a href=&quot;https://github.com/nanoporetech/medaka&quot;&gt;medaka&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Plasmid annotation using pLannotate, which itself uses a variety of tools including blastn, diamond, and cmscan from &lt;a href=&quot;http://eddylab.org/infernal/&quot;&gt;infernal&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Primer search using seqkit.&lt;/li&gt;&lt;li&gt;construction of an insert MSA across multiple samples using SPOA.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For each of these steps we see between a two and five-fold improvement in speed. The largest increase observed is for the numerically intensive medaka step. Note however that even the seemingly trivial step of the primer search using seqkit is sped up by almost 4 times. The total execution time for the workflow was reduced from 9 minutes and 2 seconds to just 3 minutes and 18 seconds. &lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;wfclonetimingPlot&quot; plotCaption=&quot;Figure 1. Workflow execution timings for critical steps of wf-clone-validation. We observed between a two and five-fold improvement in execution time when using ARM code compared to x86 code on a macOS device. For comparison we show also timings on an Intel based i7-11800H device running Microsoft Windows.&quot;&gt;&lt;/div&gt;&lt;p&gt;The wf-clone validation workflow is not a particularly strenuous workflow for the processing of a single sample. Ten minutes of bioinformatics analysis time is not really something necessarily deserving of optimisation. The results do however scale nicely with the number of samples processed.&lt;/p&gt;&lt;h3 id=&quot;and-that-has-made-all-the-difference&quot;&gt;And that has made all the difference&lt;/h3&gt;&lt;p&gt;In the above we’ve journeyed through the process of converting our popular wf-clone-validation Nextflow workflow to use ARM64 code - the language of Apple’s M1 and M2
families of processors. This was motivated by a desire to better support
users of Apple Hardware, which account for a good proportion of users of the EPI2ME Desktop
application. Apple’s Rosetta technology cannot be relied upon to allow the running of
all possible software. It does a good job in providing a compatibility layer, but ultimately
it is preferable to run natively compiled code.&lt;/p&gt;&lt;p&gt;For our wf-clone-validation workflow the performance improvement is not particularly relevant; the workflow runs quickly enough as it stands. The performance improvement of the individual tools is however more intriguing. For example the computational work that medaka performs scales with the length of the genome processed. We can
conceive that the processing of larger genomes, which could take on the order of hours to process previous, will now take only minutes. If several samples are to be processed the gains soon add up. Many of the tools used within wf-clone-validation are also used within
several other of our workflows.&lt;/p&gt;&lt;p&gt;We shall certainly be continuing project inkling until and hope to bring native ARM64 code
to all of our Nextflow workflows.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/78d100efddafbf9215dc107b302fbe98/59ccf/inkling-river.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/78d100efddafbf9215dc107b302fbe98/inkling-river.jpeg</content:toenail></item><item><title><![CDATA[EPI2ME 23.07-01 Release]]></title><description><![CDATA[A post London Calling 2023 release including the refreshed EPI2ME desktop bioinformatics application, updated workflows and newly released datasets.]]></description><link>https://labs.epi2me.io/epi2me-23.07-01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-23.07-01-release</guid><pubDate>Wed, 26 Jul 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/8160f37cabf4095a3e87c9acdb705af9/59ccf/singapore.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Dear Nanopore Community,&lt;/p&gt;&lt;p&gt;We’re pleased to bring user experience updates to our EPI2ME desktop application (v5.1.0). For our Microsoft Windows users we have overhauled and simplified the installation process and we are now providing an EPI2ME WSL distribution. To improve feedback while your analysis is running, the application now reports progress bars for our workflows (Figure 1).&lt;/p&gt;&lt;p&gt;Other improvements include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Enhanced handling of demonstration data when workflows are updated&lt;/li&gt;&lt;li&gt;Exporting of log information to assist issue reporting&lt;/li&gt;&lt;li&gt;Automatic checking for available updates to the application&lt;/li&gt;&lt;li&gt;Enabling more powerful workflow schemas&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The EPI2ME software release is available from the &lt;a href=&quot;/downloads&quot;&gt;downloads&lt;/a&gt; page&lt;/p&gt;&lt;p&gt;As a reminder we are currently soliciting early access users for the new cloud analysis functionality in the EPI2ME desktop application that will launch in 2024. You can register your interest &lt;a href=&quot;https://register.nanoporetech.com/epi2me-cloud&quot; title=&quot;https://register.nanoporetech.com/epi2me-cloud&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:65.26315789473685%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Workflow Progress&quot; title=&quot;Figure 1. EPI2ME Workflow Progress dialog&quot; src=&quot;/static/abfdaa6e534085c6cf28a2a76d202ebb/b5cea/progress.png&quot; srcSet=&quot;/static/abfdaa6e534085c6cf28a2a76d202ebb/0e2fe/progress.png 285w,/static/abfdaa6e534085c6cf28a2a76d202ebb/432e7/progress.png 570w,/static/abfdaa6e534085c6cf28a2a76d202ebb/b5cea/progress.png 1140w,/static/abfdaa6e534085c6cf28a2a76d202ebb/d5b59/progress.png 1370w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 1. EPI2ME Workflow Progress dialog&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;We also have several workflow updates to which we would like to draw your attention. Highlights include wf-metagenomics (v2.4.1) which now provides antimicrobial gene identification, the option to use the SILVA.138 database to classify reads, memory management to solve issues with the PlusPF8 database, and finally, striking sunburst plots to display your data (Figure 2). wf-somatic-variation (v.0.3.0) now includes modified-base basecalling with information on differentially modified loci.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:42.45614035087719%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Sunburst Plot&quot; title=&quot;Figure 2 - wf-metagenomics sunburst plot for interactively exploring taxonomic composition of a sample&quot; src=&quot;/static/3629d6b274197cb6289b3d7f88ac6306/b5cea/sunburst.png&quot; srcSet=&quot;/static/3629d6b274197cb6289b3d7f88ac6306/0e2fe/sunburst.png 285w,/static/3629d6b274197cb6289b3d7f88ac6306/432e7/sunburst.png 570w,/static/3629d6b274197cb6289b3d7f88ac6306/b5cea/sunburst.png 1140w,/static/3629d6b274197cb6289b3d7f88ac6306/04784/sunburst.png 1174w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 2 - wf-metagenomics sunburst plot for interactively exploring taxonomic composition of a sample&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;Other workflow updates include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation&quot; title=&quot;https://github.com/epi2me-labs/wf-clone-validation&quot;&gt;wf-clone-validation&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation/releases/tag/v0.4.0&quot; title=&quot;https://github.com/epi2me-labs/wf-clone-validation/releases/tag/v0.4.0&quot;&gt;v0.4.0&lt;/a&gt;) - Now outputs a FASTQ and average quality scores of final assembly and reports any variants between an insert reference and assembly consensus. Also some parameter changes and documentation improvements.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes&quot; title=&quot;https://github.com/epi2me-labs/wf-transcriptomes&quot;&gt;wf-transcriptomes&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes/releases/tag/v0.2.1&quot; title=&quot;https://github.com/epi2me-labs/wf-transcriptomes/releases/tag/v0.2.1&quot;&gt;v0.2.1&lt;/a&gt;) - Workflow has been updated to support only NCBI, Ensembl and Encode genome annotation files for differential gene expression. Also no longer requires a condition sheet.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-cas9&quot; title=&quot;https://github.com/epi2me-labs/wf-cas9&quot;&gt;wf-cas9&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-cas9/releases/tag/v0.1.12&quot; title=&quot;https://github.com/epi2me-labs/wf-cas9/releases/tag/v0.1.12&quot;&gt;v0.1.12&lt;/a&gt;) - Workflow has been updated to accept input files and directories that include spaces. &lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation/&quot; title=&quot;https://github.com/epi2me-labs/wf-somatic-variation/&quot;&gt;wf-somatic-variation&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation/releases/tag/v0.3.0&quot; title=&quot;https://github.com/epi2me-labs/wf-somatic-variation/releases/tag/v0.3.0&quot;&gt;v0.3.0&lt;/a&gt;) - Workflow has been updated to use nanomonsv (v.0.6.0) and now implements the call of modified bases and differentially modified loci and regions with (&lt;code&gt;--mod&lt;/code&gt;.)&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics/tree/master&quot; title=&quot;https://github.com/epi2me-labs/wf-metagenomics/tree/master&quot;&gt;wf-metagenomics&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics/releases/tag/v2.4.1&quot; title=&quot;https://github.com/epi2me-labs/wf-metagenomics/releases/tag/v2.4.1&quot;&gt;v2.4.1&lt;/a&gt;) - Workflow has been updated to identify antimicrobial resistance genes using Abricate (through the flag &lt;code&gt;--amr&lt;/code&gt; and &lt;code&gt;--amr_db&lt;/code&gt; to choose among different databases). The workflow now supports memory-mapping large database files so these can be used on devices where the database size exceeds the available system memory. The well-known SILVA database is also available as an alternative database in both pipelines. Further, sunburst plots to explore taxonomic assignments have also been included in the report for the first time.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell/tree/master&quot; title=&quot;https://github.com/epi2me-labs/wf-single-cell/tree/master&quot;&gt;wf-single-cell&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell/releases/tag/v0.2.6&quot; title=&quot;https://github.com/epi2me-labs/wf-single-cell/releases/tag/v0.2.6&quot;&gt;v0.2.6&lt;/a&gt;) - Workflow has been updated to fix the incorrect orientations of alignments when processing data from 10x Genomics 5-prime preparation methods.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We look forward to any feedback and welcome recommendations for workflows and EPI2ME features that could be included in future releases. See you in Singapore!&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/8160f37cabf4095a3e87c9acdb705af9/59ccf/singapore.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/8160f37cabf4095a3e87c9acdb705af9/singapore.jpeg</content:toenail></item><item><title><![CDATA[Targeted BRCA Gene Analysis with Oxford Nanopore]]></title><description><![CDATA[An example of using wf-human-variation for targeted ONT sequencing analysis.]]></description><link>https://labs.epi2me.io/human-targeted-analysis</link><guid isPermaLink="false">https://labs.epi2me.io/human-targeted-analysis</guid><pubDate>Mon, 19 Jun 2023 12:30:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/630c4bb28232e892dbb044ab35f3dcd4/59ccf/AdobeStock_210009171.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;blockquote&gt;&lt;p&gt;Oxford Nanopore Technologies products are not intended for use for health assessment or to diagnose, treat, mitigate, cure or prevent any disease or condition.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;code&gt;wf-human-variation&lt;/code&gt; is our “do it all” Nextflow workflow for the identification of human DNA variation from Oxford Nanopore Technologies’ long read sequencing data. With its latest update, &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/clinvar/&quot;&gt;ClinVar&lt;/a&gt; variant annotation is now also included.&lt;/p&gt;&lt;p&gt;This post will show that our human variation workflow can also process targeted sequence data. In the following text we will analyse targeted BRCA1 and BRCA2 sequence data that was generated in a research setting using Oxford Nanopore Technologies’ adaptive sampling methodology. The analysis will be performed with our EPI2ME software running on a laptop computer. While the BRCA gene enrichment here was prepared using adaptive sampling, this “how to” would also apply to capture and/or PCR based methods of enrichment. The analysis described here could similarly be applied to large panels of genes of interest in research environments.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Data was generated using DNA from the cell line &lt;a href=&quot;https://www.coriell.org/0/sections/Search/Sample_Detail.aspx?Ref=NA14636&amp;amp;product=DNA&quot;&gt;NA14636&lt;/a&gt; provided by the NIGMS Human Genetic Cell Repository at the &lt;a href=&quot;https://www.coriell.org/&quot;&gt;Coriell Institute for Medical Research&lt;/a&gt;. &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;NA14636 sample is from a 56 year-old female with family history of breast cancer. This individual was diagnosed with the disease at 50. We know that certain mutations in &lt;em&gt;BRCA1&lt;/em&gt; and &lt;em&gt;BRCA2&lt;/em&gt; can result in increased risk of hereditary breast and ovarian cancer, and so we sequenced the sample to see if we could detect any key &lt;em&gt;BRCA1&lt;/em&gt; and &lt;em&gt;BRCA2&lt;/em&gt; mutations.&lt;/p&gt;&lt;p&gt;The sample was prepared and sequenced using our ligation sequencing kit and the newer Ligation Sequencing Kit V14 on a GridION for 72hrs, with adaptive sampling of &lt;em&gt;BRCA1&lt;/em&gt; and &lt;em&gt;BRCA2&lt;/em&gt;.&lt;/p&gt;&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;&lt;p&gt;Let’s dive in and and see how we can get from our long Oxford Nanopore sequencing reads to a list of interesting variants.&lt;/p&gt;&lt;p&gt;If you haven’t already downloaded EPI2ME it’s freely available for all major operating systems here: &lt;a href=&quot;https://labs.epi2me.io/downloads/&quot;&gt;https://labs.epi2me.io/downloads/&lt;/a&gt;, there are some pre-requisites but the application will take you through these. &lt;/p&gt;&lt;h3 id=&quot;inputs&quot;&gt;Inputs&lt;/h3&gt;&lt;p&gt;Our 1st task is to make sure we have the required input files for the workflow. The required inputs to &lt;code&gt;wf-human-variation&lt;/code&gt; are the following:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;a BED file of regions we are interested in analysing (target regions)&lt;/li&gt;&lt;li&gt;a BAM file of mapped or unmapped sequencing reads (MinKNOW, dorado generated (or other))&lt;/li&gt;&lt;li&gt;a FASTA reference genome&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;bam-files&quot;&gt;BAM file(s)&lt;/h4&gt;&lt;p&gt;If you don’t have a mapped/unmapped BAM file input please see the appendix for instructions.&lt;/p&gt;&lt;h4 id=&quot;bed-file&quot;&gt;BED file&lt;/h4&gt;&lt;p&gt;If you have performed adaptive sampling you can simply use the &lt;code&gt;BED&lt;/code&gt; file you used as input into MinKNOW when setting up your sequencing run, if not then you can easily make a &lt;code&gt;BED&lt;/code&gt; file, it is a tab separated file with 3 mandatory columns of the chromosome, start and end position of your region. You can add additional columns for the name and size of the region for instance. More details on the &lt;code&gt;BED&lt;/code&gt; format &lt;a href=&quot;https://genome.ucsc.edu/FAQ/FAQformat.html#format1&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The BED file for the analysis described here looks like this:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;chr13   32305479    32409671    BRCA2       104192
chr17   43034294    43135363    BRCA1       101069
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;fasta-reference-genome&quot;&gt;FASTA reference genome&lt;/h4&gt;&lt;p&gt;Download the human reference genome - we’ll use hg38 from UCSC in this example:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/analysisSet/hg38.analysisSet.fa.gz&quot;&gt;https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/analysisSet/hg38.analysisSet.fa.gz&lt;/a&gt;&lt;/p&gt;&lt;h3 id=&quot;running-wf-human-variation&quot;&gt;Running wf-human-variation&lt;/h3&gt;&lt;p&gt;We can easily analyse data from small panels locally on a reasonably powered laptop, or on a GridION or PromethION device using the EPI2ME interface. Our workflow for the analysis of human variation, &lt;code&gt;wf-human-variation&lt;/code&gt;, is available in the application and is easy to install and run (Figure 1).&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:73.68421052631578%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;ClinVar Annotation&quot; title=&quot;Figure 1 - Bioinformatics workflows of all flavours are available in EPI2ME.)&quot; src=&quot;/static/86b343482b1c14b4b82ba87888376a62/b5cea/labs_humvar.png&quot; srcSet=&quot;/static/86b343482b1c14b4b82ba87888376a62/0e2fe/labs_humvar.png 285w,/static/86b343482b1c14b4b82ba87888376a62/432e7/labs_humvar.png 570w,/static/86b343482b1c14b4b82ba87888376a62/b5cea/labs_humvar.png 1140w,/static/86b343482b1c14b4b82ba87888376a62/09ede/labs_humvar.png 1710w,/static/86b343482b1c14b4b82ba87888376a62/d50e7/labs_humvar.png 2280w,/static/86b343482b1c14b4b82ba87888376a62/2abf7/labs_humvar.png 3068w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 1 - Bioinformatics workflows of all flavours are available in EPI2ME.)&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;In “Workflow Options” choose “Snp” - annotation of SNPs is now carried out automatically (&lt;code&gt;&amp;gt;= v1.6.0&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;In “Input Options” choose:&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;the &lt;code&gt;BAM&lt;/code&gt; file you created with &lt;code&gt;wf-alignment&lt;/code&gt; or any pre-generated BAM file you wish to analyse. &lt;/li&gt;&lt;li&gt;the &lt;code&gt;FASTA&lt;/code&gt; reference genome&lt;/li&gt;&lt;li&gt;the &lt;code&gt;BED&lt;/code&gt; file defining the regions you wish to analyse&lt;/li&gt;&lt;/ul&gt;&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;&lt;p&gt;Under “Small variant calling options” choose “Phase VCF” - ONT long reads mean we can determine the haplotype of the variants called&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Again you can alter the resources given to the workflow under “Multiprocessing Options” and “Extra Configuration”&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click “Launch workflow”&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;&lt;p&gt;Using ClinVar to annotate the variants found by &lt;code&gt;wf-human-variation&lt;/code&gt; (Figure 2) highlights one pathogenic variant in &lt;em&gt;BRCA1&lt;/em&gt; which is a 1bp insertion at nucleotide 5677 in exon 24 (5677insA). This results in a frameshift and truncation at codon 1853 (Y1853X). This matches the information provided by Coriell for this sample.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:47.01754385964912%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABEklEQVR42p2R0XKDIBBF/f9/bJ4KRgWxoKAI3uySkNg8dNoyc4S9syzr3WbxHlelobWGGhRCWLHvO7ZI7KmcIxOZWOKi0bnGdT+OA42ngqLr0RJXKqj1iHEcoUeDwXzBGFM0M02kmwJrJYd1Q3rJ0UgpoXFhg7YOK73iqSN+pYLT+bc0ez6wpYycmYSf1rfHHjHfO2sNf2bnoKhl52YsywK24b6Hp0es1Ut1WWux+AWZfjVR3qvgPEMrRV5MEL2CHDRk26IjXxXpQkh8XC74lAJCyuId3+n6HsMwoCccxc+CPgRY6pJhL2Od4mNf17XklM6JddtO04/YOE7pVdDyYOZQpvT+W39dTaYCKd95N/4/3ADaer9PwBFYwgAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;ClinVar Annotation&quot; title=&quot;Figure 2 - ClinVar annotated variants (benign &amp;amp; likely benign excluded.)&quot; src=&quot;/static/d4d79c5cadc4d61575cabadbf72091ef/b5cea/brca1_clinvar.png&quot; srcSet=&quot;/static/d4d79c5cadc4d61575cabadbf72091ef/0e2fe/brca1_clinvar.png 285w,/static/d4d79c5cadc4d61575cabadbf72091ef/432e7/brca1_clinvar.png 570w,/static/d4d79c5cadc4d61575cabadbf72091ef/b5cea/brca1_clinvar.png 1140w,/static/d4d79c5cadc4d61575cabadbf72091ef/09ede/brca1_clinvar.png 1710w,/static/d4d79c5cadc4d61575cabadbf72091ef/d50e7/brca1_clinvar.png 2280w,/static/d4d79c5cadc4d61575cabadbf72091ef/4f2ef/brca1_clinvar.png 2714w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 2 - ClinVar annotated variants (benign &amp;amp; likely benign excluded.)&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;Other important data is output by the workflow (Figure 3), including a summaries for the variants, and the alignment in &lt;code&gt;HTML&lt;/code&gt; format.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:67.71929824561404%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;EPI2ME Small Variant Report&quot; title=&quot;Figure 3 - EPI2ME small variant report.&quot; src=&quot;/static/1e5fe2f29fc8f218e2c7ce88870374a2/b5cea/labs_report.png&quot; srcSet=&quot;/static/1e5fe2f29fc8f218e2c7ce88870374a2/0e2fe/labs_report.png 285w,/static/1e5fe2f29fc8f218e2c7ce88870374a2/432e7/labs_report.png 570w,/static/1e5fe2f29fc8f218e2c7ce88870374a2/b5cea/labs_report.png 1140w,/static/1e5fe2f29fc8f218e2c7ce88870374a2/09ede/labs_report.png 1710w,/static/1e5fe2f29fc8f218e2c7ce88870374a2/d50e7/labs_report.png 2280w,/static/1e5fe2f29fc8f218e2c7ce88870374a2/5fe07/labs_report.png 3422w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 3 - EPI2ME small variant report.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;You can also take a look at the raw data such as the &lt;code&gt;VCF&lt;/code&gt; file for the variants (Figure 4). Clicking “Open folder” at the bottom of the page will open a file explorer to see the raw data.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:73.68421052631578%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Output Files&quot; title=&quot;Figure 4 - Useful output files are created by the workflow.&quot; src=&quot;/static/3791b4afa87d6abe44a5f32876aac04a/b5cea/output_files.png&quot; srcSet=&quot;/static/3791b4afa87d6abe44a5f32876aac04a/0e2fe/output_files.png 285w,/static/3791b4afa87d6abe44a5f32876aac04a/432e7/output_files.png 570w,/static/3791b4afa87d6abe44a5f32876aac04a/b5cea/output_files.png 1140w,/static/3791b4afa87d6abe44a5f32876aac04a/09ede/output_files.png 1710w,/static/3791b4afa87d6abe44a5f32876aac04a/d50e7/output_files.png 2280w,/static/3791b4afa87d6abe44a5f32876aac04a/2abf7/output_files.png 3068w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 4 - Useful output files are created by the workflow.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;&lt;p&gt;We hope that you found this quick tutorial on how to analyse targeted human Oxford Nanopore sequencing research data with &lt;code&gt;wf-human-variation&lt;/code&gt; and EPI2ME. Any comments, questions or suggestions don’t hesitate to let us know using the usual channels.&lt;/p&gt;&lt;h2 id=&quot;appendix-1---bam-file-generation&quot;&gt;Appendix 1 - BAM file generation&lt;/h2&gt;&lt;p&gt;You have two options if you have &lt;code&gt;FASTQ&lt;/code&gt; data:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Align your reads&lt;/li&gt;&lt;li&gt;Make an unmapped bam&lt;/li&gt;&lt;/ol&gt;&lt;h4 id=&quot;1-alignment-with-wf-alignment&quot;&gt;1. Alignment with wf-alignment&lt;/h4&gt;&lt;p&gt;You’ll need the reference genome &lt;code&gt;FASTA&lt;/code&gt; from above; we can align our sequencing reads from our &lt;code&gt;FASTQ&lt;/code&gt; files to this reference with &lt;code&gt;wf-alignment&lt;/code&gt; to generate some statistics on our sequencing data and also create our &lt;code&gt;BAM&lt;/code&gt; file for &lt;code&gt;wf-human-variation&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In EPI2ME Labs install the &lt;code&gt;wf-alignment&lt;/code&gt; workflow if you haven’t already done so. Click “Run this workflow”&lt;/p&gt;&lt;p&gt;Choose the path to your &lt;code&gt;FASTQ&lt;/code&gt; files and the path to your reference genome; &lt;code&gt;hg38.analysisSet.fa.gz&lt;/code&gt; under “Input Options”.&lt;/p&gt;&lt;p&gt;You can also increase the resources given to the workflow under “Misc Options” and “Extra Configuration”.&lt;/p&gt;&lt;p&gt;Click “Launch workflow”&lt;/p&gt;&lt;h4 id=&quot;2-make-an-unmapped-bam&quot;&gt;2. Make an unmapped BAM&lt;/h4&gt;&lt;p&gt;If you are reasonably familiar with the command line you can make what’s called an “unmapped” &lt;code&gt;BAM&lt;/code&gt; file using &lt;code&gt;samtools&lt;/code&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;samtools import -o umapped_reads.bam -O BAM &amp;lt;FASTQ&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Where &lt;code&gt;&amp;lt;FASTQ&amp;gt;&lt;/code&gt; is the path to your FASTQ file(s). This command will produce an umapped &lt;code&gt;BAM&lt;/code&gt; file called &lt;code&gt;unmapped_reads.bam&lt;/code&gt;.&lt;/p&gt;&lt;h2 id=&quot;appendix-2---running-on-the-command-line&quot;&gt;Appendix 2 - Running on the command line&lt;/h2&gt;&lt;p&gt;Just like with our desktop application, running &lt;code&gt;wf-human-variation&lt;/code&gt; on the command line is easy.&lt;/p&gt;&lt;p&gt;A good place to start is to list the parameters that can be used to run the workflow:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;nextflow run epi2me-labs/wf-human-variation --help
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To recreate the analysis above, these are the options to use:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;nextflow run epi2me-labs/wf-human-variation --bam &amp;lt;PATH_TO_READS&amp;gt; --ref &amp;lt;PATH_TO_FASTA&amp;gt;  --bed &amp;lt;PATH_TO_BED&amp;gt; --phase_vcf --snp
&lt;/code&gt;&lt;/pre&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/630c4bb28232e892dbb044ab35f3dcd4/59ccf/AdobeStock_210009171.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/630c4bb28232e892dbb044ab35f3dcd4/AdobeStock_210009171.jpeg</content:toenail></item><item><title><![CDATA[EPI2ME 23.06-01 Release]]></title><description><![CDATA[A post London Calling 2023 release including the refreshed EPI2ME desktop bioinformatics application, updated workflows and newly released datasets.]]></description><link>https://labs.epi2me.io/epi2me-23.06-01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-23.06-01-release</guid><pubDate>Wed, 14 Jun 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/6e59abd3871952e7ee61b5d7161440f0/59ccf/tulips-sky.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Dear Nanopore Community,&lt;/p&gt;&lt;p&gt;With this first June release, we are delighted to update our EPI2ME workflows with some exciting new functionality and a number of other improvements.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-cas9&quot;&gt;wf-cas9&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-cas9/blob/master/CHANGELOG.md&quot;&gt;v0.1.11&lt;/a&gt;) has been updated as a minor release that updates the default resource parameters.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-bacterial-genomes/&quot;&gt;wf-bacterial-genomes&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-bacterial-genomes/blob/master/CHANGELOG.md&quot;&gt;v0.3.0&lt;/a&gt;) has been updated to include an &lt;code&gt;--isolates&lt;/code&gt; mode. This introduces some exciting new functionality:&lt;ul&gt;&lt;li&gt;MLST characterisation has been added to the workflow to identify the species and sequence type from the genome assembly alone.&lt;/li&gt;&lt;li&gt; The &lt;code&gt;pointfinder&lt;/code&gt; method from &lt;a href=&quot;https://github.com/cadms/resfinder/tree/master&quot;&gt;Resfinder&lt;/a&gt; is run automatically using species information determined by the MLST analysis step described above. This update accommodates isolate samples of unknown species, or runs where you have multiple barcoded isolates, and will provide a full AMR analysis.&lt;/li&gt;&lt;li&gt;The HTML format report produced after an analysis has been revised with some dramatic new figures. Single sample reports are now also produced alongside the normal overall run report.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation&quot;&gt;wf-human-variation&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation/blob/master/CHANGELOG.md&quot;&gt;v1.6.0&lt;/a&gt;) is a minor release and solves an issue when calling CNV from CRAM format starting files.&lt;ul&gt;&lt;li&gt;The included &lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling/&quot;&gt;wf-basecalling&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling/blob/master/CHANGELOG.md&quot;&gt;v0.7.1&lt;/a&gt;) component has also been updated and includes other minor fixes. &lt;/li&gt;&lt;li&gt;the &lt;code&gt;--snp&lt;/code&gt; analysis will now produce VCF format output files that now also contain functional annotations from &lt;a href=&quot;https://pcingola.github.io/SnpEff/&quot;&gt;SnpEff&lt;/a&gt;. The HTML report now includes a table of variants annotated using the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/clinvar/&quot;&gt;ClinVar&lt;/a&gt; resource.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes/&quot;&gt;wf-transcriptomes&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes/blob/master/CHANGELOG.md&quot;&gt;v0.1.13&lt;/a&gt;) has been updated to provide a bug fix when the &lt;em&gt;de novo&lt;/em&gt; transcriptome workflow path is run.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-pore-c&quot;&gt;wf-pore-c&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-pore-c/blob/master/CHANGELOG.md&quot;&gt;v0.0.8&lt;/a&gt;) has been updated to improve the workflow documentation and to provide a more comprehensive description of the output files and to update the help texts made available through the &lt;a href=&quot;https://labs.epi2me.io/&quot;&gt;EPI2ME Labs&lt;/a&gt; desktop software. The existing &lt;code&gt;chunk_size&lt;/code&gt; parameter has been updated to a default value of &lt;code&gt;25000&lt;/code&gt; - this will address some memory usage issues.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation&quot;&gt;wf-somatic-variation&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation/blob/master/CHANGELOG.md&quot;&gt;v0.2.0&lt;/a&gt;) has been updated to include new functionality for the calling of somatic SV using &lt;a href=&quot;https://github.com/friend1ws/nanomonsv&quot;&gt;nanomonsv&lt;/a&gt;. Several minor bugs have also been fixed and the documentation has been expanded.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-amplicon&quot;&gt;wf-amplicon&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-amplicon/blob/master/CHANGELOG.md&quot;&gt;v0.2.0&lt;/a&gt;) has been updated to also report observed low coverage variants in an additional table.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell&quot;&gt;wf-single-cell&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell/blob/master/CHANGELOG.md&quot;&gt;v0.2.5&lt;/a&gt;) has been updated to improve the handling of system resources - this resolves out of memory issues when processing larger datasets.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We would welcome recommendations for new workflows that would benefit your research and would love to hear about new functionality to our existing workflows.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/6e59abd3871952e7ee61b5d7161440f0/59ccf/tulips-sky.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/6e59abd3871952e7ee61b5d7161440f0/tulips-sky.jpeg</content:toenail></item><item><title><![CDATA[EPI2ME 23.05-02 Release]]></title><description><![CDATA[A post London Calling 2023 release including the refreshed EPI2ME desktop bioinformatics application, updated workflows and newly released datasets.]]></description><link>https://labs.epi2me.io/epi2me-labs-23.05-02-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-23.05-02-release</guid><pubDate>Wed, 31 May 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/517e6a060e038e92101396dbb9799454/59ccf/lemur-single.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Dear Nanopore Community,&lt;/p&gt;&lt;p&gt;With this post London Calling ‘23 bioinformatics product release, we are delighted to introduce the updated EPI2ME desktop application that was described by &lt;a href=&quot;/author/chris-wright&quot;&gt;Chris Wright&lt;/a&gt; during his &lt;em&gt;Data For Breakfast&lt;/em&gt; presentation - this is &lt;a href=&quot;https://nanoporetech.com/resource-centre/london-calling-2023-leveraging-nextflow-analysis-anything-anywhere-anyone&quot;&gt;available online&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;This update provides a simplified user interface to aid the entry of paths and parameters required by workflows (example shown in figure 1). Updates in the future will include new functionality to run analyses in the cloud. Please &lt;a href=&quot;https://register.nanoporetech.com/epi2me-cloud&quot;&gt;register your interest&lt;/a&gt; for this cloud functionality - we’ll soon be looking for test projects.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:66.31578947368422%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;New look EPI2ME 5 Application&quot; title=&quot;Figure 1.  EPI2ME v5.0.2 has an updated user interface. In the screenshot a user is prompted to provide the required information to run the metagenomics workflow, wf-metagenomics. The structuring of workflow options is simplified.&quot; src=&quot;/static/ee7430c92efa392750b8af127877f2cc/b5cea/epi2me-5-screenshot.png&quot; srcSet=&quot;/static/ee7430c92efa392750b8af127877f2cc/0e2fe/epi2me-5-screenshot.png 285w,/static/ee7430c92efa392750b8af127877f2cc/432e7/epi2me-5-screenshot.png 570w,/static/ee7430c92efa392750b8af127877f2cc/b5cea/epi2me-5-screenshot.png 1140w,/static/ee7430c92efa392750b8af127877f2cc/09ede/epi2me-5-screenshot.png 1710w,/static/ee7430c92efa392750b8af127877f2cc/d50e7/epi2me-5-screenshot.png 2280w,/static/ee7430c92efa392750b8af127877f2cc/89587/epi2me-5-screenshot.png 2624w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 1.  EPI2ME v5.0.2 has an updated user interface. In the screenshot a user is prompted to provide the required information to run the metagenomics workflow, wf-metagenomics. The structuring of workflow options is simplified.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;We are also pleased to introduce two new datasets that are now available through the s3://ont-open-data resource.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The Ashkenazi trio and HG001 (CEPH/Utah) have been sequenced from libraries prepared using ligation kit 14 and sequenced using the latest 5khz sampling rate - please see the &lt;a href=&quot;/giab-2023.05/&quot;&gt;Sequencing Genome in a Bottle samples&lt;/a&gt; blog post.    &lt;/li&gt;&lt;li&gt;The COLO829 tumor/normal pair has been prepared using Ligation Sequencing Kit V14; a description of the data release can be found in the &lt;a href=&quot;/colo-2023.05/&quot;&gt;Sequencing A Tumor Normal Pair with Ligation Sequencing Kit V14&lt;/a&gt; post - this dataset is a useful reference dataset to explore our recently introduced &lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation&quot;&gt;wf-somatic-variation&lt;/a&gt; workflow.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A couple of workflows have also been updated&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;wf-basecalling&lt;/strong&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling&quot;&gt;v0.6.0&lt;/a&gt;) now works in real time - the workflow watches the input path and process raw data files as they become available. &lt;/li&gt;&lt;li&gt;&lt;strong&gt;wf-transcriptomes&lt;/strong&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-transcriptomes&quot;&gt;v0.1.12&lt;/a&gt;) has been updated to accommodate different reference annotation file types - annotations in both GFF3 and GTF format (optionally .gz compressed) are now accepted. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We welcome all feedback and recommendations for new workflows and functionality. Please register your interest and we’ll tell you more about the &lt;a href=&quot;https://register.nanoporetech.com/epi2me-cloud&quot;&gt;upcoming cloud functionality&lt;/a&gt;.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/517e6a060e038e92101396dbb9799454/59ccf/lemur-single.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/517e6a060e038e92101396dbb9799454/lemur-single.jpeg</content:toenail></item><item><title><![CDATA[Sequencing Genome in a Bottle samples]]></title><description><![CDATA[Several Genome in a Bottle samples including the Askenazi Trio samples sequenced on PromethION with Oxford Nanopore's latest Ligation Sequencing Kit]]></description><link>https://labs.epi2me.io/giab-2023.05</link><guid isPermaLink="false">https://labs.epi2me.io/giab-2023.05</guid><pubDate>Fri, 26 May 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/915ab28c03e84ea762edc89be8ff2d45/59ccf/giab_2023_05.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;We are pleased to announce the release of a new addition to the Oxford Nanopore
Open Data project: sequencing of several Genome in a Bottle samples (including the Ashkenazi Trio).
Sequencing was performed with the 5 kHz upgrade to the
&lt;a href=&quot;https://store.nanoporetech.com/uk/ligation-sequencing-kit-v14.html&quot;&gt;Ligation Sequencing Kit V14&lt;/a&gt;
released in MinKNOW 23.04.05. As such the quality of data presented here should be representative
of routine sequencing that can be performed by any lab using this latest release.&lt;/p&gt;&lt;p&gt;These reference samples were sequenced with two PromethION flow cells each to yield around
more than 200 Gbases sequencing data per sample.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The following cell line samples were obtained from the NIGMS Human Genetic Cell
Repository at the Coriell Institute for Medical Research: GM12878, GM24143, GM24149, GM24385&lt;/p&gt;&lt;/blockquote&gt;&lt;h3 id=&quot;data-location&quot;&gt;Data location&lt;/h3&gt;&lt;p&gt;As with previous releases the new dataset is available for anonymous download from
an Amazon Web Services S3 bucket. The bucket is part of the &lt;a href=&quot;https://aws.amazon.com/opendata/&quot;&gt;Open Data on AWS&lt;/a&gt;
project enabling sharing and analysis of a wide range of data.&lt;/p&gt;&lt;p&gt;The data is located in the bucket at:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;s3://ont-open-data/giab_2023.05/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;See the &lt;a href=&quot;/tutorials/&quot;&gt;tutorials&lt;/a&gt; page for information on downloading the dataset.&lt;/p&gt;&lt;h3 id=&quot;sequencing-outputs&quot;&gt;Sequencing Outputs&lt;/h3&gt;&lt;p&gt;Two flowcells were used to sequence each of the samples to high depth:&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Genome&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;th&gt;Cell line&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;HG001&lt;/td&gt;&lt;td&gt;CEPH/UTAH&lt;/td&gt;&lt;td&gt;GM12878&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;HG002&lt;/td&gt;&lt;td&gt;PGP Ashkenazi Son&lt;/td&gt;&lt;td&gt;GM24385&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;HG003&lt;/td&gt;&lt;td&gt;PGP Ashkenazi Father&lt;/td&gt;&lt;td&gt;GM24149&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;HG004&lt;/td&gt;&lt;td&gt;PGP Ashkenazi Mother&lt;/td&gt;&lt;td&gt;GM24143&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;For each flowcell used in the sequencing the PromethION device outputs are available. All
data is present as &lt;code&gt;.pod&lt;/code&gt; files, along with associated summary files in a structured fashion.
For example results from one of the flowcells used to sequence the GM24385 (HG002) sample are found as:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;$ aws s3 ls s3://ont-open-data/giab_2023.05/flowcells/hg002/20230424_1302_3H_PAO89685_2264ba8c/
                           PRE other_reports/
                           PRE pod5_fail/
                           PRE pod5_pass/
2023-05-12 12:10:53        248 barcode_alignment_PAO89685_2264ba8c_afee3a87.tsv
2023-05-12 12:39:33        656 final_summary_PAO89685_2264ba8c_afee3a87.txt
2023-05-12 12:39:33  224724629 full_ss_every_17.txt
2023-05-12 18:21:16    2269523 pore_activity_PAO89685_2264ba8c_afee3a87.csv
2023-05-12 18:21:16   22500344 read_list.txt
2023-05-12 18:21:18    1496823 report_PAO89685_20230424_1308_2264ba8c.html
2023-05-12 18:21:18     946254 report_PAO89685_20230424_1308_2264ba8c.json
2023-05-12 18:21:19    2817707 report_PAO89685_20230424_1308_2264ba8c.md
2023-05-12 18:21:19        180 sample_sheet_PAO89685_20230424_1308_2264ba8c.csv
2023-05-12 18:21:20 3602275623 sequencing_summary_PAO89685_2264ba8c_afee3a87.txt
2023-05-12 18:21:38     546602 throughput_PAO89685_2264ba8c_afee3a87.csv
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that for some flowcell there are multiple logical sequencing runs due to
the sequencing devices being restarted partway through the intended run times.&lt;/p&gt;&lt;h3 id=&quot;basecalling-and-analysis&quot;&gt;Basecalling and analysis&lt;/h3&gt;&lt;p&gt;The data analyses presented here were performed using our workflows:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling&quot;&gt;wf-basecalling&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-human-variation&quot;&gt;wf-human-variation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;implemented in &lt;a href=&quot;https://www.nextflow.io/&quot;&gt;Nexflow&lt;/a&gt;. Note that although
wf-human-variation incorporates the functionality of wf-basecalling, in this
instance the standalone basecalling workflow was used for logistical data
processing reasons. The wf-human-variation workflow is fully integrated using
containerised software to provide scalable analysis. As a brief overview the
workflow is capable of performing:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;GPU optimised basecalling with &lt;a href=&quot;https://github.com/nanoporetech/dorado&quot;&gt;dorado&lt;/a&gt;, our latest state-of-the-art basecaller&lt;/li&gt;&lt;li&gt;read alignment with &lt;a href=&quot;https://github.com/lh3/minimap2&quot;&gt;minimap2&lt;/a&gt;&lt;/li&gt;&lt;li&gt;small variant calling with a horizontally-scaled implementation of &lt;a href=&quot;https://github.com/HKU-BAL/clair3&quot;&gt;clair3&lt;/a&gt;, and inference models provided by Oxford Nanopore&lt;/li&gt;&lt;li&gt;structural variant calling with &lt;a href=&quot;https://github.com/fritzsedlazeck/Sniffles&quot;&gt;sniffles2&lt;/a&gt;&lt;/li&gt;&lt;li&gt;aggregation of 5mC and 5hmC modified base data with our own &lt;a href=&quot;https://github.com/epi2me-labs/modbam2bed&quot;&gt;modbam2bed&lt;/a&gt;&lt;/li&gt;&lt;li&gt;copy number variant (CNV) calling through &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/QDNAseq.html&quot;&gt;QDNAseq&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Short tandem repeat (STR) calling genotyped using a fork of &lt;a href=&quot;https://github.com/philres/straglr&quot;&gt;Straglr&lt;/a&gt; &lt;/li&gt;&lt;li&gt;creation of a consolidated summary reports&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The workflow was run on the combined sets of data from each pair of flowcells for
each sample. For each sample we have provided results for two flavours of the
basecalling algorithm: 1) &lt;em&gt;hac&lt;/em&gt; - high accuracy and 2) &lt;em&gt;sup&lt;/em&gt; - super accuracy. The
choice is reflected in the path names in the S3 bucket.&lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;statsPlot&quot; plotCaption=&quot;Figure 1. Exemplar sequencing summary metrics, in this case for the HG002 (GM24385) dataset sequenced with Oxford Nanopore Technologies&amp;#x27; PromethION instrument.&quot;&gt;&lt;/div&gt;&lt;p&gt;The results of the &lt;code&gt;wf-human-variation&lt;/code&gt; workflow can be found at:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;s3://ont-open-data/giab_2023.05/analysis/variant_calling
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;a-note-on-read-depth&quot;&gt;A note on read depth&lt;/h4&gt;&lt;p&gt;We find that Clair3 is sensitive to high read depth: variant calling
performance can suffer when read depth is excessive. Therefore we have performed
variant calling using the full datasets for each of the genomes HG001-004 and for a
downsampled random selection of reads leading to 60-fold coverage of each genome HG001-003.
Downsampling was not performed for HG004 as the total depth was approximately 60X in any case.
This data downsampling is not currently implemented in wf-human-variation.&lt;/p&gt;&lt;h3 id=&quot;variant-calling-summary-benchmarks&quot;&gt;Variant calling summary benchmarks&lt;/h3&gt;&lt;p&gt;In addition to running variant calling have provided also results of benchmarking analysis
using &lt;a href=&quot;https://github.com/Illumina/hap.py&quot;&gt;hap.py&lt;/a&gt; for small variants for all the genomes.
A summary of the benchmarking is shown in Figure 2, full results and output from hap.py can be found at:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;s3://ont-open-data/giab_2023.05/analysis/small_variants_happy/
&lt;/code&gt;&lt;/pre&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;variantStatsPlot&quot; plotCaption=&quot;Figure 2. Variant calling accuracy statistics for four Genome in a Bottle samples at 60-fold coverage, sequenced with Oxford Nanopore Technologies&amp;#x27; PromethION instrument.&quot;&gt;&lt;/div&gt;&lt;h3 id=&quot;further-information&quot;&gt;Further information&lt;/h3&gt;&lt;p&gt;For additional information regarding these data please contact &lt;a href=&quot;mailto:support@nanoporetech.com&quot;&gt;support@nanoporetech.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;We hope that these data and analyses provide a useful resource to the community.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/915ab28c03e84ea762edc89be8ff2d45/59ccf/giab_2023_05.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/915ab28c03e84ea762edc89be8ff2d45/giab_2023_05.jpeg</content:toenail></item><item><title><![CDATA[Sequencing A Tumor Normal Pair with Ligation Sequencing Kit V14]]></title><description><![CDATA[Sequencing the COLO829/BL tumour/normal pair with Oxford Nanopore's latest Ligation Sequencing Kit]]></description><link>https://labs.epi2me.io/colo-2023.05</link><guid isPermaLink="false">https://labs.epi2me.io/colo-2023.05</guid><pubDate>Fri, 12 May 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/720d4b72d869062f82238114a567b6b6/59ccf/colo_2023_05.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;We are pleased to announce the release of a new addition to the Oxford Nanopore
Open Data project: sequencing of the &lt;a href=&quot;https://www.atcc.org/products/crl-1974&quot;&gt;COLO829&lt;/a&gt; /
&lt;a href=&quot;https://www.atcc.org/products/crl-1980&quot;&gt;COLO829BL&lt;/a&gt; tumour/normal pair.&lt;/p&gt;&lt;p&gt;These reference samples were sequenced with three PromethION flow cells; two flowcells for
the cancer sample, one for the normal. They should provide a valuable resource for cancer
researchers.&lt;/p&gt;&lt;h3 id=&quot;data-location&quot;&gt;Data location&lt;/h3&gt;&lt;p&gt;As with previous releases the new dataset is available for anonymous download from
an Amazon Web Services S3 bucket. The bucket is part of the &lt;a href=&quot;https://aws.amazon.com/opendata/&quot;&gt;Open Data on AWS&lt;/a&gt;
project enabling sharing and analysis of a wide range of data.&lt;/p&gt;&lt;p&gt;The data is located in the bucket at:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;s3://ont-open-data/colo829_2023.04/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;See the &lt;a href=&quot;/tutorials/&quot;&gt;tutorials&lt;/a&gt; page for information on downloading the dataset.&lt;/p&gt;&lt;h3 id=&quot;sample-preparation&quot;&gt;Sample preparation&lt;/h3&gt;&lt;p&gt;COLO829 melanoma fibroblasts (ATCC CRL-1974) and COLO829BL normal B lymphoblasts
(ATCC CRL-1980) were cultured for three days in RPMI-1640 medium with 10% fetal
bovine serum and 1% antibiotic-antimycotic incubated at 37°C with 5% CO2. Five
million cells were extracted with the DNeasy Blood and Tissue Kit (Qiagen, Cat.
No. 69504) following the manufacturer’s instructions. Sequencing libraries were prepared
following Oxford Nanopore Ligation Sequencing Kit instructions, and
20 fmols were loaded onto R10.4.1 PromethION flow cells. Sequencing was
performed on a PromethION 24 instrument with the 22.12.5 MinKNOW software.&lt;/p&gt;&lt;p&gt;As a special bonus we have also included data from a second sample preparation
aimed at extracting ultra-high molecular weight DNA. Six million cells were
extracted with the Monarch HMW DNA Extraction Kit for Tissue (New England
Biolabs, T3060) following the modified instructions outlined in the Oxford
Nanopore Ultra-long DNA Sequencing Kit (SQK-ULK114). Sequencing libraries were
prepared and 1/3 of the final libraries were loaded onto R10.4.1 PromethION flow
cells. The flow cells were flushed and reloaded twice as described in the
section, “Reloading ultra-long DNA library on a PromethION flow cell.” Samples
were sequenced on a PromethION 24 instrument with the 22.12.5 MinKNOW software. &lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The ultra-high molecular weight data were &lt;strong&gt;not&lt;/strong&gt; used in the main analysis.&lt;/p&gt;&lt;/blockquote&gt;&lt;h3 id=&quot;sequencing-outputs&quot;&gt;Sequencing Outputs&lt;/h3&gt;&lt;p&gt;Three flowcells were used to sequence the samples to high depth:&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Genome&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;th&gt;Preparation&lt;/th&gt;&lt;th&gt;Flowcell&lt;/th&gt;&lt;th&gt;Yield / Gbase&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;COLO829&lt;/td&gt;&lt;td&gt;Tumour&lt;/td&gt;&lt;td&gt;DNeasy&lt;/td&gt;&lt;td&gt;PAO29420&lt;/td&gt;&lt;td&gt;102&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;DNeasy&lt;/td&gt;&lt;td&gt;PAO32033&lt;/td&gt;&lt;td&gt;118&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;UHMW&lt;/td&gt;&lt;td&gt;PAK76302&lt;/td&gt;&lt;td&gt;64&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;COLO829BL&lt;/td&gt;&lt;td&gt;Normal&lt;/td&gt;&lt;td&gt;DNeasy&lt;/td&gt;&lt;td&gt;PAO33946&lt;/td&gt;&lt;td&gt;123&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;UHMW&lt;/td&gt;&lt;td&gt;PAK76487&lt;/td&gt;&lt;td&gt;50&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;For each flowcell used in the sequencing the primary sequencer outputs are available
as &lt;code&gt;.pod&lt;/code&gt; files. We provide also sequencing reads in CRAM format produced by our
&lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling&quot;&gt;wf-basecalling&lt;/a&gt; workflow. Reads are
aligned to the GRCh38 human reference.&lt;/p&gt;&lt;div plotJson=&quot;[object Object]&quot; plotName=&quot;statsPlot&quot; plotCaption=&quot;Sequencing summary metrics for the COLO829/BL cancer/normal pair sequenced with Oxford Nanopore Technologies&amp;#x27; PromethION instrument.&quot;&gt;&lt;/div&gt;&lt;h3 id=&quot;analysis&quot;&gt;Analysis&lt;/h3&gt;&lt;p&gt;The data analyses presented here were performed using our workflows:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-basecalling&quot;&gt;wf-basecalling&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation&quot;&gt;wf-somatic-variation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The somatic variant calling workflow uses &lt;a href=&quot;https://github.com/HKU-BAL/ClairS&quot;&gt;ClairS&lt;/a&gt;
to create calls for the tumour sample by eliminating variants found
also in the paired-normal sample.&lt;/p&gt;&lt;p&gt;The variant calling workflow was run using data from both the COLO829 (tumour) flowcells
and the single COLO829BL (normal). The results of the workflow are present at:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;s3://ont-open-data/colo829_2023.04/analysis/sup_wf_som_var/
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;further-information&quot;&gt;Further information&lt;/h3&gt;&lt;p&gt;For additional information regarding these data please contact &lt;a href=&quot;mailto:support@nanoporetech.com&quot;&gt;support@nanoporetech.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;We hope that these data and analyses provide a useful resource to the community.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/720d4b72d869062f82238114a567b6b6/59ccf/colo_2023_05.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/720d4b72d869062f82238114a567b6b6/colo_2023_05.jpeg</content:toenail></item><item><title><![CDATA[London Calling 2023]]></title><description><![CDATA[How to get your EPI2ME Labs questions answered at London Calling 2023.]]></description><link>https://labs.epi2me.io/london-calling-2023</link><guid isPermaLink="false">https://labs.epi2me.io/london-calling-2023</guid><pubDate>Thu, 11 May 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/5f68cd3c4d1ca6ae797acbecbd669ebc/32411/lc23.png" length="0" type="image/png"/><content:encoded>&lt;p&gt;As is tradition we wanted to let you know how you can connect with the EPI2ME Labs team both online and in person at what is looking like a typically cloudy &lt;a href=&quot;https://londoncallingconf.co.uk/lc23&quot;&gt;London Calling 2023&lt;/a&gt;. There are some of the same old faces and some brand new ones to help you with your data analysis needs.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:29.122807017543863%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAABaElEQVR42kWQ627TQBCF/f7PgZAQUmndpIa4RlCaIoRUQS8pBKWJndiJc6ljx8Zer9cfE/9hpdWM5sw5M2es8XiMHywIw4iXJMG0La38OI4JgrnEDc/PU6qqojEGrbX0hkRRxMyfC2ePMcIB9mmGNZlMmPu/SNM9hzynrmuUUp2AH/hs4hmr1ZJahI71I55L33HgNp5SFIUsAGVZimCK5Xkf+ey+QmUPHJ/rup3Aer3GcRy+fnoNbUCaFXjepQxOZfOAszOb++9vhLERoZzh8JpYOJZz9Y3+h3P2fq8THI0eRGzFn98jLr7c4g7e0iTDDnsS7GW35e7HLc7NHVeeDCt+ohvhPd4LL8aqDegyBJN3pCQtKP4qsWFQgpnqP5YeSiqluzsruZupZlJtOsvZoRAXByyzfYfZ9Wm2Ns36hDaRuLOpFjZmaaOX79FRD704pY371GEPNT2nFUfaH6Al11Obdt4nmwz4B5WuuxxbLj9gAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;London Calling Weather Forecast&quot; title=&quot;We&amp;#x27;re expecting a cloudy London Calling 2023.&quot; src=&quot;/static/9b36253efcf4bb3d4d6e209d033ad4f8/b5cea/clouds.png&quot; srcSet=&quot;/static/9b36253efcf4bb3d4d6e209d033ad4f8/0e2fe/clouds.png 285w,/static/9b36253efcf4bb3d4d6e209d033ad4f8/432e7/clouds.png 570w,/static/9b36253efcf4bb3d4d6e209d033ad4f8/b5cea/clouds.png 1140w,/static/9b36253efcf4bb3d4d6e209d033ad4f8/0d292/clouds.png 1620w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;We&amp;#x27;re expecting a cloudy London Calling 2023.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;Highlights from the team include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Chris Wright&lt;/em&gt; giving you all a big fat helping of data for breakfast on Friday morning.&lt;/li&gt;&lt;li&gt;&lt;em&gt;Natalia Garcia&lt;/em&gt;, &lt;em&gt;Stephen Rudd&lt;/em&gt; and I demoing metagenomics and bacterial genomes in the live lounge on Thursday afternoon.&lt;/li&gt;&lt;li&gt;&lt;em&gt;Stephen Rudd&lt;/em&gt;’s bioinformatics workshop on Wednesday (now sold out).&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If you’re not attending in person then some of the team will be available online for the duration of the conference and can be reached through the LC2023 platform.&lt;/p&gt;&lt;p&gt;More information about London Calling 2023, how to register for online attendance (in person is SOLD OUT), and the jam packed agenda can be found on the conference website: &lt;a href=&quot;https://londoncallingconf.co.uk/lc23&quot;&gt;https://londoncallingconf.co.uk/lc23&lt;/a&gt;. Please take a look!&lt;/p&gt;&lt;h2 id=&quot;the-in-person-team&quot;&gt;The in-person team&lt;/h2&gt;&lt;p&gt;The following you can find in person on the days indicated:&lt;/p&gt;&lt;h3 id=&quot;chris-wright&quot;&gt;Chris Wright&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;In Person&lt;/em&gt;: Thursday, Friday&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Not only serving up some data for breakfast Chris will be hanging around the EPI2ME demos to guide you on our best practice bioinfomatics workflows.&lt;/p&gt;&lt;h3 id=&quot;stephen-rudd&quot;&gt;Stephen Rudd&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;In Person&lt;/em&gt;: Thursday, Friday&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Our bioinformatics product manager knows everything there is to know about what we offer for the analysis of ONT data, he’ll be there to answer any of your questions.&lt;/p&gt;&lt;h3 id=&quot;tom-rich&quot;&gt;Tom Rich&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;In Person&lt;/em&gt;: Thursday, Friday&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Tom will be on hand to show off all new features of EPI2ME Labs in person so if you have any feedback, bug reports, feature requests then be sure to track him down!&lt;/p&gt;&lt;h3 id=&quot;sirisha-hesketh&quot;&gt;Sirisha Hesketh&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;In Person&lt;/em&gt;: Friday&lt;/li&gt;&lt;li&gt;&lt;em&gt;Online&lt;/em&gt;: Wednesday, Thursday&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Clinical bioinformatician responsible for short tandem repeat calling and copy number calling in wf-human-variation.&lt;/p&gt;&lt;h3 id=&quot;matt-parker&quot;&gt;Matt Parker&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;In Person&lt;/em&gt;: Thursday, Friday&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I’ll be there to talk about your viral sequencing needs, SARS-CoV-2, Influenza, Monkeypox and clinical application of our technology.&lt;/p&gt;&lt;h3 id=&quot;sam-nicholls&quot;&gt;Sam Nicholls&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;In Person&lt;/em&gt;: Thursday, Friday&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Lead developer of our do it all workflow for the detection of human variation Sam can answer all you questions on how to get the best out of wf-human-variation as well as other infrastructure related STUFF. &lt;/p&gt;&lt;h3 id=&quot;natalia-garcia&quot;&gt;Natalia Garcia&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;In Person&lt;/em&gt;: Thursday&lt;/li&gt;&lt;li&gt;&lt;em&gt;Online&lt;/em&gt;: Wednesday,Friday&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;New to the team this year Natalia brings with her loads of metagenomics experience and has been polishing our metagenomics workflow.&lt;/p&gt;&lt;h2 id=&quot;the-online-team&quot;&gt;The online team&lt;/h2&gt;&lt;h3 id=&quot;sarah-griffiths&quot;&gt;Sarah Griffiths&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Online&lt;/em&gt;: Wednesday, Thursday, Friday&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Sarah still looks after our most popular workflow - plasmid sequencing analysis, wf-clone-validation. Fine her online if you have any questions about plasmid sequencing on ONT.&lt;/p&gt;&lt;h3 id=&quot;andrea-talenti&quot;&gt;Andrea Talenti&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Online&lt;/em&gt;: Wednesday, Thursday, Friday&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Andrea is developing our workflow for the detection of somatic variation which currently detects SNPs and Indels but more is coming soon!&lt;/p&gt;&lt;h3 id=&quot;julian-libiseller-egger&quot;&gt;Julian Libiseller-Egger&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Online&lt;/em&gt;: Wednesday, Thursday, Friday&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Another new team member Julian has just released our amplicon sequencing workflow.&lt;/p&gt;&lt;h3 id=&quot;lucia-estelles&quot;&gt;Lucia Estelles&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Online&lt;/em&gt;: Wednesday, Thursday, Friday&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Clinical bioinformatician working on our regulatory compliance, if you have questions on how we are developing our workflows to meet ISO/IEC standards then Lucia will be happy to help online.&lt;/p&gt;&lt;h3 id=&quot;neil-horner&quot;&gt;Neil Horner&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Online&lt;/em&gt;: Wednesday, Thursday, Friday&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Neil has been busy adding our single cell workflow to our suite of RNA sequencing analysis tools. He’ll be available online to answer any of your questions on this or our other RNA workflows.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/5f68cd3c4d1ca6ae797acbecbd669ebc/32411/lc23.png</content:thumbnail><content:toenail>https://labs.epi2me.io/static/5f68cd3c4d1ca6ae797acbecbd669ebc/lc23.png</content:toenail></item><item><title><![CDATA[Selecting the correct databases in the wf-metagenomics]]></title><description><![CDATA[A post to describe how to use different database depending on the the pipeline and data.]]></description><link>https://labs.epi2me.io/metagenomic-databases</link><guid isPermaLink="false">https://labs.epi2me.io/metagenomic-databases</guid><pubDate>Thu, 11 May 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/472c6e28e68958dcb102213ce163fb66/59ccf/water-test.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Our metagenomics workflow, &lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot;&gt;wf-metagenomics&lt;/a&gt;, gives users the ability to unveil the taxonomic composition of their Oxford Nanopore Technologies sequencing data. The workflow offers two different sub-workflows: kraken2 and minimap2. In this blog post we describe how to choose one of the default databases or how to use your custom database.&lt;/p&gt;&lt;p&gt;The wf-metagenomics workflow offers four databases which cover the more general cases in both pipelines. However, we are aware that you could have more specific questions that can be solved using your own custom databases, so we explain here how to run the workflow with them.&lt;/p&gt;&lt;h2 id=&quot;the-kraken2-sub-workflow&quot;&gt;The kraken2 sub-workflow&lt;/h2&gt;&lt;p&gt;Kraken2 is a taxonomic sequence classifier that relies on a &lt;em&gt;k&lt;/em&gt;-mers&lt;sup id=&quot;fnref-1&quot;&gt;&lt;a href=&quot;#fn-1&quot; class=&quot;footnote-ref&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; approach to assign taxonomic labels to DNA sequences. It examines the &lt;em&gt;k&lt;/em&gt;-mers within a query sequence (the read) and uses this information to query a database. This means that the database must be built in advance to extract the &lt;em&gt;k&lt;/em&gt;-mer information of each reference sequence and store it in an efficient format for later query. To Find out more about the files that comprise a Kraken2 database in the Kraken2 &lt;a href=&quot;https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown#kraken-2-databases&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;We offer a set of different databases that can be useful in many situations so that you do not need to be worried about it.&lt;/p&gt;&lt;h3 id=&quot;analyzing-16s18s-rrna-genes-and-its-regions-or-metataxonomics&quot;&gt;Analyzing 16S/18S rRNA genes and ITS regions (or Metataxonomics)&lt;/h3&gt;&lt;p&gt;To analyze archaeal, bacterial and fungal 16S/18S ribosomal RNA genes and ITS, there are two databases available built using data from &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/refseq/targetedloci/&quot;&gt;NCBI&lt;/a&gt;.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;ncbi_16s_18s&lt;/strong&gt;: contains 16S ribosomal RNA sequences that correspond to bacteria and archaea type materials and 18S ribosomal RNA Nucleotide sequence records from fungi. This is the default option.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;ncbi_16s_18s_28s_ITS&lt;/strong&gt;: contains 16S ribosomal RNA sequences that correspond to bacteria and archaea type materials, 18S ribosomal RNA Nucleotide sequence records and sequences from the ITS region from fungi.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;To change the database you should add the specific option to your command, or selecting it in the EPI2ME Labs menu (Reference Options &amp;gt; Database set).&lt;/p&gt;&lt;pre&gt;&lt;code&gt;nextflow run epi2me-labs/wf-metagenomics \
    --fastq &amp;lt;PATH_TO_FASTQ&amp;gt; --database_set ncbi_16s_18s_28s_ITS
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;shotgun-metagenomics&quot;&gt;Shotgun Metagenomics&lt;/h3&gt;&lt;p&gt;In this case, we need a database that contains whole genome information. The Kraken2 authors curate a set of &lt;a href=&quot;https://benlangmead.github.io/aws-indexes/k2&quot;&gt;pre-built databases&lt;/a&gt;. We have selected two of them based on their reasonable size and coverage of a wide diversity of organisms that can be found in the environment:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;PlusPF-8&lt;/strong&gt;: contains references for archaea, bacteria, viral, plasmid, human, UniVec_Core, protozoa and fungi. To use this database the memory available to the workflow must be slightly higher than size of the database index (8GB).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;PlusPFP-8&lt;/strong&gt;: It contains references as &lt;strong&gt;PlusPF-8&lt;/strong&gt; and additionally plants. To use this database the memory available to the workflow must be slightly higher than size of the database index (8GB).&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;custom-database&quot;&gt;Custom database&lt;/h3&gt;&lt;p&gt;If you need to use a different database, you should follow the kraken2 &lt;a href=&quot;https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown#custom-databases&quot;&gt;instructions&lt;/a&gt;. Then, you would need to create the bracken additional files as it is explained &lt;a href=&quot;https://github.com/jenniferlu717/Bracken#running-bracken-easy-version&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;With these steps performed you can provide the database directory to wf-metagenomics using the &lt;code&gt;--database&lt;/code&gt; option, which can be either a &amp;lt;.tar.gz&amp;gt; format file or a directory. Note that the memory available to the workflow must be slightly higher than size of the database index.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;nextflow run epi2me-labs/wf-metagenomics \
    --fastq &amp;lt;PATH_TO_FASTQ&amp;gt;  --database &amp;lt;DATABASE&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This can also be performed in EPI2ME Labs from the Reference Options &amp;gt; Database option, by pointing at the folder.&lt;/p&gt;&lt;h2 id=&quot;the-minimap2-sub-workflow&quot;&gt;The minimap2 sub-workflow&lt;/h2&gt;&lt;p&gt;This workflow relies on mapping the reads against a database based on their identity. To analyze archaeal, bacterial and fungal 16S/18S ribosomal DNA and ITS data, you can use the two databases available also for the kraken2 pipeline (see above for more information). In addition, you can use your custom database according to what you expect (or not) to find in the samples.&lt;/p&gt;&lt;h3 id=&quot;custom-database-1&quot;&gt;Custom database&lt;/h3&gt;&lt;p&gt;The reference file can be either a fasta format file or a minimap2 index file (&amp;lt;.mmi&amp;gt;). The mmi file can be created running (see their &lt;a href=&quot;https://github.com/lh3/minimap2&quot;&gt;Github&lt;/a&gt; for more information):&lt;/p&gt;&lt;pre&gt;&lt;code&gt;minimap2 -d &amp;lt;reference.mmi&amp;gt; &amp;lt;reference.fasta&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And then use it in the workflow by using the &lt;code&gt;--reference&lt;/code&gt; option:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;nextflow run epi2me-labs/wf-metagenomics \
    --fastq &amp;lt;PATH_TO_FASTQ&amp;gt; --classifier minimap2 --reference &amp;lt;reference.mmi&amp;gt; OR &amp;lt;reference.fasta&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;or in the app (Reference Options &amp;gt; Reference) pointing the file.&lt;/p&gt;&lt;p&gt;In this case, you may want to provide a file with the taxonomy of each of your reference sequences. For this purpose you can use the &lt;code&gt;--ref2taxid&lt;/code&gt; option which expects a tsv file without headers and with the taxid of each reference (from within EPI2ME Labs this is the parameter: Minimap2 Options &amp;gt; Ref2taxid).&lt;/p&gt;&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Minimap2: Li H. Minimap2: pairwise alignment for nucleotide sequences. &lt;em&gt;Bioinformatics&lt;/em&gt; 34(18):3094-3100.(2018). &lt;a href=&quot;https://doi.org/10.1093%2Fbioinformatics%2Fbty191&quot;&gt;10.1093/bioinformatics/bty191&lt;/a&gt;. &lt;a href=&quot;https://github.com/lh3/minimap2&quot;&gt;Github&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Kraken2: Wood, D.E., Lu, J. &amp;amp; Langmead, B. Improved metagenomic analysis with Kraken 2. &lt;em&gt;Genome Biology&lt;/em&gt; 20: 257. (2019). &lt;a href=&quot;https://doi.org/10.1186/s13059-019-1891-0&quot;&gt;10.1186/s13059-019-1891-0&lt;/a&gt;. &lt;a href=&quot;https://github.com/DerrickWood/kraken2/wiki&quot;&gt;Github&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Bracken: Lu J, Breitwieser FP, Thielen P, Salzberg SL. Bracken: estimating species abundance in metagenomics data. &lt;em&gt;PeerJ Computer Science&lt;/em&gt; 3:e104. (2017). &lt;a href=&quot;https://doi.org/10.7717/peerj-cs.104&quot;&gt;10.7717/peerj-cs.104&lt;/a&gt;. &lt;a href=&quot;https://github.com/jenniferlu717/Bracken&quot;&gt;Github&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;sup id=&quot;fnref-1&quot;&gt;&lt;a href=&quot;#fn-1&quot; class=&quot;footnote-ref&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;em&gt;k&lt;/em&gt;-mers: a sequence of k characters in a string (or nucleotides in a DNA sequence).&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/472c6e28e68958dcb102213ce163fb66/59ccf/water-test.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/472c6e28e68958dcb102213ce163fb66/water-test.jpeg</content:toenail></item><item><title><![CDATA[EPI2ME Labs 23.05-01 Release]]></title><description><![CDATA[Highlights of this release include updating the wf-metagenomics report to our latest fresh styling, and further speed improvements to wf-pore-c.]]></description><link>https://labs.epi2me.io/epi2me-labs-23.05-01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-23.05-01-release</guid><pubDate>Wed, 03 May 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/4822ea511f150945b1f748bf9ef96954/59ccf/release-the-floodgates.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;Dear Nanopore Community,&lt;/p&gt;&lt;p&gt;With our first release of May, we are delighted to introduce two new workflows. We are also updating a number of other workflows to fix issues and enable better functionality on computers with ARM processors.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation&quot;&gt;wf-somatic-variation&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-somatic-variation/blob/prerelease/CHANGELOG.md&quot;&gt;v0.1.0&lt;/a&gt;) is our new workflow for the analysis of somatic variation from paired tumour/normal sequence data. This workflow processes a pair of tumour/normal BAM files, prepares quality statistics and identifies candidate SNVs and short Indels using the &lt;a href=&quot;https://github.com/HKU-BAL/ClairS&quot;&gt;ClairS&lt;/a&gt; software. The workflow generates two reports showing:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The alignment QC for the paired BAM files, facilitating their comparison.&lt;/li&gt;&lt;li&gt;A summary and descriptions of the variants identified.&lt;/li&gt;&lt;li&gt;Variant allele frequency plot, mutation type counts.
Future updates to the workflow will include other ClairS functionality such as site genotyping/hybrid calling. Additional subworkflows for the detection of other variant types will also be added.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-amplicon&quot;&gt;wf-amplicon&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-amplicon/blob/master/CHANGELOG.md&quot;&gt;v0.1.0&lt;/a&gt;). This workflow facilitates the analysis of reads generated from one or multiple amplicons. After some pre-processing (filtering, trimming, downsampling), reads are aligned to a reference (containing the expected sequence for each amplicon). Variant calling is also performed using &lt;a href=&quot;https://github.com/nanoporetech/medaka&quot;&gt;Medaka&lt;/a&gt;. The workflow analyses demultiplexed data and has been designed to work with our Native Barcoding kits. The generated report presents for each sample:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A section illustrating the number and quality of the raw and pre-processed sequence reads.&lt;/li&gt;&lt;li&gt;Tables summarising the alignment and variant calling results.&lt;/li&gt;&lt;li&gt;Plots showing the coverage along the individual amplicons.
We plan to expand the functionality of wf-amplicon to also cover tiled amplicons and use cases where no reference is available in future releases.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell&quot;&gt;wf-single-cell&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell/blob/master/CHANGELOG.md&quot;&gt;v0.2.3&lt;/a&gt;) is our workflow for the analysis of single cell transcriptomics data. This contains two changes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Fixes a bug which causes incorrect barcode assignment and addresses an out of memory error in cases where a chromosome has no mapped sequence reads.&lt;/li&gt;&lt;li&gt;Sequence reads that map to intron-only regions are now assigned to genes and are included in the gene expression analysis. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics/&quot;&gt;wf-metagenomics&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics/blob/master/CHANGELOG.md&quot;&gt;v2.2.0&lt;/a&gt;) is a workflow for the analysis of metagenomic sequence data. The workflow has been updated to run natively on ARM devices, notably recent Apple M1 and M2 processors. The workflow should now be considerably faster when running on such devices. Other updates include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Updates to the existing databases and add a new kraken2 database, &lt;code&gt;PFP8&lt;/code&gt;.&lt;/li&gt;&lt;li&gt;Fixes file names when exporting tables.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-pore-c&quot;&gt;wf-pore-c&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-pore-c/blob/master/CHANGELOG.md&quot;&gt;v0.0.4&lt;/a&gt;) receives a small update to fix the handling of modified base information during the digest step and a fix to the Chromunity output handling.&lt;/p&gt;&lt;p&gt;Finally &lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation&quot;&gt;wf-clone-validation&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-clone-validation/blob/master/CHANGELOG.md&quot;&gt;v0.3.0&lt;/a&gt;), our plasmid assembly and annotation workflow has been amended to use Flye instead of Canu for the initial assembly step.&lt;/p&gt;&lt;p&gt;We would welcome any feedback - especially for our new amplicon and somatic variation workflows.&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/4822ea511f150945b1f748bf9ef96954/59ccf/release-the-floodgates.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/4822ea511f150945b1f748bf9ef96954/release-the-floodgates.jpeg</content:toenail></item><item><title><![CDATA[SARS-CoV-2 Midnight Chemistry V14 Update]]></title><description><![CDATA[Comparison of Kit 10 to Kit V14 data generated by the ONT Midnight SARS-CoV-2 sequencing assay.]]></description><link>https://labs.epi2me.io/sarscov2-midnight-kit14</link><guid isPermaLink="false">https://labs.epi2me.io/sarscov2-midnight-kit14</guid><pubDate>Wed, 19 Apr 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/0c994c86a92eeef32eb6a2614dff1fe6/59ccf/AdobeStock_447465507.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt;
the &lt;code&gt;wf-artic&lt;/code&gt; workflow has now been updated to select the Medaka variant model based on the provided
basecaller configuration (using the parameter &lt;code&gt;--basecaller_cfg&lt;/code&gt;). Alternatively the Medaka model can be provided
directly via the &lt;code&gt;--medaka_variant_model&lt;/code&gt; parameter.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;We are pleased to offer a new version of &lt;code&gt;wf-artic&lt;/code&gt; which supports models for optimal variant calling by medaka when using RBK114.96 and R10.4.1 flow cells for data generation.&lt;/p&gt;&lt;p&gt;This update coincides with the release of an early access protocol for Kit V14/R10.4.1 midnight on the Oxford Nanopore community which can be found &lt;a href=&quot;https://community.nanoporetech.com/docs/prepare/library_prep_protocols/pcr-tiling-of-sars-cov-2-virus-rbk114-and-midnight-rt&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Versions of &lt;code&gt;wf-artic&lt;/code&gt; &lt;code&gt;&amp;gt;= v0.3.23&lt;/code&gt; include this support for kit14/R10.4.1 medaka models. In this blog post we describe the improvements in Kit V14/R10.4.1 and how to run updated &lt;code&gt;wf-artic&lt;/code&gt; with this data on the command line and EPI2ME Labs.&lt;/p&gt;&lt;h2 id=&quot;why-use-kit-v14r1041-for-sars-cov-2-sequencing&quot;&gt;Why use Kit V14/R10.4.1 for SARS-CoV-2 sequencing?&lt;/h2&gt;&lt;p&gt;Using the latest sequencing kits and flow cells from Oxford Nanopore Technologies ensures that you are getting the best possible data for your application. We recently released Kit V14 chemistry along with R10.4.1 flow cells which together deliver our most accurate sequencing data to date. This increase in accuracy is now available for our SARS-CoV-2 sequencing workflow, Midnight, using &lt;code&gt;EXP-MRT001&lt;/code&gt; and &lt;code&gt;SQK-RBK114.96&lt;/code&gt;. We recommend users upgrade to get the highest accuracy Oxford Nanopore SARS-CoV-2 genomes. &lt;/p&gt;&lt;p&gt;For more information on the upgrade in performance see &lt;a href=&quot;https://community.nanoporetech.com/posts/q20-open-early-access-now&quot;&gt;this&lt;/a&gt; post in the Oxford Nanopore community.&lt;/p&gt;&lt;p&gt;Alongside general accuracy improvements there are particularly impressive improvements in homopolymer regions when moving to Kit V14 and R10.4.1 flow cells. Figure 1 shows the proportion of non-deletion calls in an adenosine homopolymer in the spike gene of SARS-CoV-2 (taller blue bars mean better performance). &lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:67.01754385964912%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAACj0lEQVR42lVTS08TURhl3+DGBFqqf8HEnTz8I5i4wMQQMHFBfaxFxQKiCxdqIrBT6YNWntMhMZFWsFpdYAXToIAY09Lpg5YZmLkzx+/eW3SY5PTc+33fnN6e09uEE4/jAqf/+8p+FaZ5dHLOacD1NDmNYn5fh5LNQfn+Awtfc4hlt6Bs7EDN/UR8fQvbe3vI7Oxiai2HMGGOZhNUV9a3sZT7jc1CWQpatk2CDKHsNpqHQmgdjaLt4bSAj3B2LIZTVB9dWceFFwmcDkZwZox6o9PwU4/PeO5P4VL0vRQ0GSOyEfq2g+ZgGF4u9igm4KMX/MReEvAOh9FCX+Z39x4T09oz5BLUSiUwvQ5l8w/OP19E+3gCnZMquiaW0Dmh4iJx+7iKDqrzE3ZNqAKiN7mEDuJzzxYwkPgiBUuahnqtJmw+YraASTaYx2uCxWE7kplkOeeIvtnoS8GSBl3XZUS2RR9MeCqDdIQdjmPjYL8CyzpsJMwkhP/2ibSbSuUyLOMA6d0ieiNv0T+zjL7YO/TNrOK28gnXZlPoiadwfX4VvW+SuBJPoie2jMB8CoNqBlenk7gcTeLJStaVMg9lbQvNd1+j9UEYbcMRtFFAPkrUPxyFl2pe2rcM8R4FQ3VfkJhC4jOeOy/RHUq6BR35t7lHgsGG4AgNE/tHIkLMS+sW3huRteMeZ8/gK3SHU1KQcUHLRPpXAf2zHxCgnxlQMoJvqJ9xk8B5YPEjAomM2N8Se+rT3AC3ZS6Np+kNKchPx+006nXKxPp3hYxaFezQIDcY2NEhDioaZWUSLJjkuU4hHT+MrqRp6K6rx10k5mCcaa8bBvKFPMrlCvaKRRQ1Dg08xHyhgGq1Kg7C3+G22Y2U/wKIgFoRKRyZhQAAAABJRU5ErkJggg==&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;kit14 homopolymer performance&quot; title=&quot;Figure 1 - Homopolymer Performance&quot; src=&quot;/static/a6092bb15325397eed28ff76c94cdae2/b5cea/homopolymer_performance.png&quot; srcSet=&quot;/static/a6092bb15325397eed28ff76c94cdae2/0e2fe/homopolymer_performance.png 285w,/static/a6092bb15325397eed28ff76c94cdae2/432e7/homopolymer_performance.png 570w,/static/a6092bb15325397eed28ff76c94cdae2/b5cea/homopolymer_performance.png 1140w,/static/a6092bb15325397eed28ff76c94cdae2/47218/homopolymer_performance.png 1344w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 1 - Homopolymer Performance&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;h2 id=&quot;analysis-of-midnight-kit-v14r1041-data&quot;&gt;Analysis of Midnight Kit V14/R10.4.1 data&lt;/h2&gt;&lt;p&gt;To take advantage of these improvements to our chemistry, &lt;code&gt;medaka&lt;/code&gt;, which is used by the ARTIC Network workflow to call the variants in the SARS-CoV-2 genomic data, needed to be updated. This has now been completed and versions of &lt;code&gt;wf-artic&lt;/code&gt; &lt;code&gt;&amp;gt;= v0.3.23&lt;/code&gt; include updated &lt;code&gt;medaka&lt;/code&gt; which is able to use variant calling models best suited to Kit V14/R10.4.1 chemistry.&lt;/p&gt;&lt;p&gt;The new models included are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;r1041_e82_400bps_hac_variant_g615&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;r1041_e82_400bps_sup_variant_g615&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;To use these models on the command line or in EPI2ME Labs see below.&lt;/p&gt;&lt;h3 id=&quot;running-on-command-line&quot;&gt;Running on command line&lt;/h3&gt;&lt;p&gt;To use updated medaka models on the command line simply include the flag &lt;code&gt;--medaka_model&lt;/code&gt; with the name of the model.&lt;/p&gt;&lt;p&gt;For high accuracy basecalls:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;nextflow run epi2me-labs wf-artic -r v0.3.25 --fastq &amp;lt;PATH_TO_DEMUX_FASTQ_FILES&amp;gt; --medaka_model r1041_e82_400bps_hac_variant_g615
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For super high accuracy basecalls:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;nextflow run epi2me-labs wf-artic -r v0.3.25 --fastq &amp;lt;PATH_TO_DEMUX_FASTQ_FILES&amp;gt; --medaka_model r1041_e82_400bps_sup_variant_g615
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;running-in-epi2me-labs&quot;&gt;Running in EPI2ME Labs&lt;/h3&gt;&lt;h4 id=&quot;update-wf-artic&quot;&gt;Update wf-artic&lt;/h4&gt;&lt;p&gt;Ensure you have updated &lt;code&gt;wf-artic&lt;/code&gt; in EPI2ME Labs and have version &lt;code&gt;&amp;gt;= v0.3.23&lt;/code&gt;. Click the workflow and scroll down to find some tools for workflow management.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:71.9298245614035%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;wf-artic updates&quot; title=&quot;Figure 2 - Update workflow button&quot; src=&quot;/static/483d7d02263f3ba21b2b5370fd89da23/b5cea/update_workflow_button.png&quot; srcSet=&quot;/static/483d7d02263f3ba21b2b5370fd89da23/0e2fe/update_workflow_button.png 285w,/static/483d7d02263f3ba21b2b5370fd89da23/432e7/update_workflow_button.png 570w,/static/483d7d02263f3ba21b2b5370fd89da23/b5cea/update_workflow_button.png 1140w,/static/483d7d02263f3ba21b2b5370fd89da23/09ede/update_workflow_button.png 1710w,/static/483d7d02263f3ba21b2b5370fd89da23/d50e7/update_workflow_button.png 2280w,/static/483d7d02263f3ba21b2b5370fd89da23/0bd07/update_workflow_button.png 2898w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 2 - Update workflow button&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;Click “Update workflow”&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:71.9298245614035%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;wf-artic update workflow&quot; title=&quot;Figure 3 - Click to update workflow&quot; src=&quot;/static/132dcecb0bf4999d396b1220790dfc92/b5cea/update_workflow_modal.png&quot; srcSet=&quot;/static/132dcecb0bf4999d396b1220790dfc92/0e2fe/update_workflow_modal.png 285w,/static/132dcecb0bf4999d396b1220790dfc92/432e7/update_workflow_modal.png 570w,/static/132dcecb0bf4999d396b1220790dfc92/b5cea/update_workflow_modal.png 1140w,/static/132dcecb0bf4999d396b1220790dfc92/09ede/update_workflow_modal.png 1710w,/static/132dcecb0bf4999d396b1220790dfc92/d50e7/update_workflow_modal.png 2280w,/static/132dcecb0bf4999d396b1220790dfc92/0bd07/update_workflow_modal.png 2898w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 3 - Click to update workflow&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;If required you can switch the version of the workflow you are using by clicking “Switch revision”&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:71.9298245614035%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;wf-artic switch revision&quot; title=&quot;Figure 4 - Switch workflow revision&quot; src=&quot;/static/d589990782fa144267af7153ba3f7b51/b5cea/switch_revision.png&quot; srcSet=&quot;/static/d589990782fa144267af7153ba3f7b51/0e2fe/switch_revision.png 285w,/static/d589990782fa144267af7153ba3f7b51/432e7/switch_revision.png 570w,/static/d589990782fa144267af7153ba3f7b51/b5cea/switch_revision.png 1140w,/static/d589990782fa144267af7153ba3f7b51/09ede/switch_revision.png 1710w,/static/d589990782fa144267af7153ba3f7b51/d50e7/switch_revision.png 2280w,/static/d589990782fa144267af7153ba3f7b51/0bd07/switch_revision.png 2898w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 4 - Switch workflow revision&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;After update the workflow should display the latest version&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:250px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:137.60000000000002%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAcCAYAAABh2p9gAAAACXBIWXMAABYlAAAWJQFJUiTwAAAChUlEQVR42s1WyWoUURTtj1DwG7J15VcofoBoEBe60FUWfoDgylVAcCHiVhBFcILoIrpxkWALVrqTdIbudM1dc3VVD8d7Xg90aSXpGFEfXN6rV++ed4dzb1UlCCKE4Ug8P1DiB+F0bx4JQjkfxUqvkmU5OIY4/UiSFJW021UPaTfD8vM3WHr4FJ+qmtobDA+/Zjh+l6YpdN1Q6yhOUIkFlePx6w84d/k6Fq7cxoVbd+GI+bOKhwG6rotGozECjGYAr91fxvkbS7j35BnOXFrEF21zZOVgcKyrE3AFmKQjl9fqDSxcvYOzFxdx88Gj8cGTxVa53JXYTcZ2S8dabRu+ZOwod+cGPO0oADquh1bbxIFuwe34aBs2DMuZK4algKSNIrS42xcQzgzySdxWgBMeVqvf8Pbde2jaBlZWPuLFy1dYXf2MLMt+D7Be3xQ+7WB/v4n19a/YqNXwXdPQ6/UKlaQyP/wXSbGcjmoMHY8NIkTbtGGrvRC66ahEMcau56v4lsW2AOiLIjNNQMf11RzFqXSSSK0JSLCOz04Ul2a/AEglUyyhpaSMLmLa7tRiWtk8MGDZHUWnfr9/NCAtMERJF1cJYliu4mVbeEkrLQFnCAhoiye8lO952RQw+gmQCsx6lueKl+yV3Ww08znPe2qmzmS/N2Np0WXG0PFKXfk/avmPA5IGqfRGxiaX6mCFcM05l5hOKmb+5iBg9a0Gdnab2GrswRRqcK0bFnb3Woqjx/XIAiAPlslgMJy74xQA4yRBLBv8JJBPtJjfG77n55HCZyrFUkHHEpv1SUIHUlYkK9ckMMuMF3DPG5cnOUtO/p0ss8OwtHz+gpRJEKjG8et+WBD+jtCrH1zEd/C89cIUAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;wf-artic updated&quot; title=&quot;Figure 5 - Updated workflow&quot; src=&quot;/static/334f8bd572cd6e043255db9d3bed4c55/63868/workflow_card.png&quot; srcSet=&quot;/static/334f8bd572cd6e043255db9d3bed4c55/63868/workflow_card.png 250w&quot; sizes=&quot;(max-width: 250px) 100vw, 250px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 5 - Updated workflow&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;h4 id=&quot;run-wf-artic-with-new-models&quot;&gt;Run wf-artic with new models&lt;/h4&gt;&lt;p&gt;To specify a medaka variant calling model with superior performance on Kit V14/R10.4.1:&lt;/p&gt;&lt;p&gt;Select wf-artic from the workflows menu of EPI2ME Labs and click “Run this workflow”. You will be presented with options for workflow execution. Scroll down to see the “Advanced options”.&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:71.9298245614035%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;wf-artic all options&quot; title=&quot;Figure 6 - Workflow options&quot; src=&quot;/static/914c9db9941fac633adbe74b79f91c88/b5cea/all_options.png&quot; srcSet=&quot;/static/914c9db9941fac633adbe74b79f91c88/0e2fe/all_options.png 285w,/static/914c9db9941fac633adbe74b79f91c88/432e7/all_options.png 570w,/static/914c9db9941fac633adbe74b79f91c88/b5cea/all_options.png 1140w,/static/914c9db9941fac633adbe74b79f91c88/09ede/all_options.png 1710w,/static/914c9db9941fac633adbe74b79f91c88/d50e7/all_options.png 2280w,/static/914c9db9941fac633adbe74b79f91c88/0bd07/all_options.png 2898w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 6 - Workflow options&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:71.9298245614035%;position:relative;bottom:0;left:0;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;wf-artic advanced options&quot; title=&quot;Figure 7 - Advanced workflow options&quot; src=&quot;/static/758312d75fe96ab9f32acea0af4f6f9c/b5cea/advanced_options.png&quot; srcSet=&quot;/static/758312d75fe96ab9f32acea0af4f6f9c/0e2fe/advanced_options.png 285w,/static/758312d75fe96ab9f32acea0af4f6f9c/432e7/advanced_options.png 570w,/static/758312d75fe96ab9f32acea0af4f6f9c/b5cea/advanced_options.png 1140w,/static/758312d75fe96ab9f32acea0af4f6f9c/09ede/advanced_options.png 1710w,/static/758312d75fe96ab9f32acea0af4f6f9c/d50e7/advanced_options.png 2280w,/static/758312d75fe96ab9f32acea0af4f6f9c/0bd07/advanced_options.png 2898w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 7 - Advanced workflow options&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;&lt;p&gt;Scroll down to see the medaka model selection drop-down. Select either:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;High accuracy - &lt;code&gt;r1041_e82_400bps_hac_variant_g615&lt;/code&gt;&lt;/li&gt;&lt;li&gt;Super high accuracy - &lt;code&gt;r1041_e82_400bps_sup_variant_g615&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position:relative;display:block;margin-left:auto;margin-right:auto;max-width:1140px&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom:70.17543859649122%;position:relative;bottom:0;left:0;background-image:url(&amp;#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABYlAAAWJQFJUiTwAAACB0lEQVR42pXS20/aUBwH8P6Ly+YDrgQpBXdFLmoUX7ZkUaKbCEvMbvK4FxeB6eIetsTFBzOdUBiXFxctpW5KlEuhUFr47rRKXEyc9SSf/E5yfv3m15xDff6yjn2RB88XIYiHEEoiiqUS2YvghRL2iwJRBE8cnFf9/F+8IOC4XMbG5iaoVC4HqSFDkhq4vFRVRbvTQUvV0O12oWkaVOLy0s/0lSsUQCW5DE4rdfw+KqNSk1CtNQjJUKuf7eVWG035avpAvR6Q5NJngU1ZwWm1jkazZTT0a5/cUv5L7yd5SKUz5xNWJRz+KaMuNa/9+KpA9AO5TAa9roaOohi1q6k3pnaUi8Ct7V3sFET8INJ7RwZu7xjZg4pp3K8TiCdtpMlwVHxtHf5nrwyep4t4MBXCkH8WFvc0BkdmrkV7grg1/ATBxffIkhdDJVZXMer3wOt1w08q47CBsVvBMOY4SL/Fcgeh+efI5smzSax9wvhkAL6xcYxNBOC6dx92lgXjdJrCulyw0DRehBYuAicCUxglgZOkuj1e2B0sHE6XKaxrGIO0FfMkMP2T/PJK4iNGfD48fPTYCBtiGNJA467VaorVZsPtgQHMzs2Rh82B+rrxDe+Wonj95i2WolGEFsIIhyOmRSIvMT0TRDyeIJeSB/UhFkMylTKuPJfP35h+s1vft7Gzm8LySgx/AUjEePuBKNawAAAAAElFTkSuQmCC&amp;#x27;);background-size:cover;display:block&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;wf-artic medaka models&quot; title=&quot;Figure 8 - Available medaka variant calling models&quot; src=&quot;/static/d656925f995318eeccdac16cc8adfee2/b5cea/medaka_model_choices.png&quot; srcSet=&quot;/static/d656925f995318eeccdac16cc8adfee2/0e2fe/medaka_model_choices.png 285w,/static/d656925f995318eeccdac16cc8adfee2/432e7/medaka_model_choices.png 570w,/static/d656925f995318eeccdac16cc8adfee2/b5cea/medaka_model_choices.png 1140w,/static/d656925f995318eeccdac16cc8adfee2/09ede/medaka_model_choices.png 1710w,/static/d656925f995318eeccdac16cc8adfee2/d50e7/medaka_model_choices.png 2280w,/static/d656925f995318eeccdac16cc8adfee2/62b00/medaka_model_choices.png 2708w&quot; sizes=&quot;(max-width: 1140px) 100vw, 1140px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;/&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Figure 8 - Available medaka variant calling models&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/0c994c86a92eeef32eb6a2614dff1fe6/59ccf/AdobeStock_447465507.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/0c994c86a92eeef32eb6a2614dff1fe6/AdobeStock_447465507.jpeg</content:toenail></item><item><title><![CDATA[EPI2ME Labs 23.04-01 Release]]></title><description><![CDATA[Highlights of this release include updating the wf-metagenomics report to our latest fresh styling, and further speed improvements to wf-pore-c.]]></description><link>https://labs.epi2me.io/epi2me-labs-23.04-01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-23.04-01-release</guid><pubDate>Thu, 06 Apr 2023 00:00:00 GMT</pubDate><enclosure url="https://labs.epi2me.io/static/fe7ba47d625cdbd1bf8270ad21143d4d/59ccf/release-the-vegetables.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;p&gt;We are pleased to release a collection of updates and technical improvements to the EPI2ME Labs bioinformatics workflows.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell&quot;&gt;wf-single-cell&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-single-cell/blob/master/CHANGELOG.md&quot;&gt;v0.2.2&lt;/a&gt;) is our workflow for the analysis of single-cell transcriptome data. &lt;ul&gt;&lt;li&gt;A more stringent filtering of isoforms is now applied - this uses an approach similar to that performed by the &lt;a href=&quot;https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02525-6&quot;&gt;FLAMES&lt;/a&gt; software. The change will reduce the number of reads incorrectly assigned to transcripts and thus decrease the number of false positive transcript assignments. &lt;/li&gt;&lt;li&gt;The colours used by the UMAP and gene saturation plots have been updated.&lt;/li&gt;&lt;li&gt;Gene IDs were erroneously reported (instead of gene names) in the previous release. This update fixes this issue and gene names are again reported in the result files.&lt;/li&gt;&lt;li&gt;Fixed incorrect stripping of barcode insertion characters.&lt;/li&gt;&lt;li&gt;Fixed a workflow error when processing contigs that contain no sequence alignments following transcriptome mapping.&lt;a href=&quot;https://github.com/epi2me-labs/wf-pore-c&quot;&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-pore-c&quot;&gt;wf-pore-c&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-pore-c/blob/master/CHANGELOG.md&quot;&gt;v0.0.3&lt;/a&gt;) is our workflow for the analysis of sequence data from chromosome conformation capture experiments. &lt;ul&gt;&lt;li&gt;The workflow is considerably faster - input BAM files are now indexed instead of split during the analysis.&lt;/li&gt;&lt;li&gt;The workflow now accepts both FASTQ and BAM sequences as input.&lt;/li&gt;&lt;li&gt;An HTML report is now produced.&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot;&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics&quot;&gt;wf-metagenomics&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-metagenomics/blob/master/CHANGELOG.md&quot;&gt;v2.1.0&lt;/a&gt;) provides a workflow for the analysis of metagenomic data. This update includes: &lt;ul&gt;&lt;li&gt;More options to input data: a single file, a directory containing fastq files, or a directory of directories containing fastq files.&lt;/li&gt;&lt;li&gt;The HTML report has been updated to use our &lt;a href=&quot;https://github.com/epi2me-labs/ezcharts&quot;&gt;ezCharts&lt;/a&gt; library with new features like:&lt;ul&gt;&lt;li&gt;an export button to download tables from the report&lt;/li&gt;&lt;li&gt;a stacked barplot which shows the most abundant taxa&lt;/li&gt;&lt;li&gt;information of all the taxonomic ranks&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;The workflow now uses the latest version of &lt;a href=&quot;https://github.com/epi2me-labs/kraken2-server&quot;&gt;kraken2-server&lt;/a&gt; and has a new option:&lt;ul&gt;&lt;li&gt;&lt;code&gt;threads_server&lt;/code&gt; for setting &lt;code&gt;--thread-pool&lt;/code&gt; on the server&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Fixes a workflow error when &lt;code&gt;--batch-size=1&lt;/code&gt;.&lt;a href=&quot;https://github.com/epi2me-labs/wf-flu&quot;&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/wf-flu&quot;&gt;wf-flu&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/wf-flu/blob/master/CHANGELOG.md&quot;&gt;v0.0.7&lt;/a&gt;) provides a workflow for the analysis of influenza genomes.&lt;ul&gt;&lt;li&gt;The included &lt;a href=&quot;https://insaflu.insa.pt/&quot;&gt;INSaFLU&lt;/a&gt; database has been updated to v10 – this is used for annotation.&lt;/li&gt;&lt;li&gt;The HTML report has been updated to use our &lt;a href=&quot;https://github.com/epi2me-labs/ezcharts&quot;&gt;ezCharts&lt;/a&gt; library and has a new feature: &lt;ul&gt;&lt;li&gt;inclusion of abricate/INSaFLU results for cases where mixed results are discovered&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;The workflow now performs a &lt;a href=&quot;https://clades.nextstrain.org/&quot;&gt;Nextclade&lt;/a&gt; assessment for certain influenza strains.&lt;/li&gt;&lt;li&gt;The &lt;a href=&quot;https://github.com/nanoporetech/medaka&quot;&gt;medaka&lt;/a&gt; models for kit14 have been included; sequencing runs with the latest flowcells and sequencing reagents are now supported.&lt;a href=&quot;https://github.com/epi2me-labs/kraken2-server&quot;&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/epi2me-labs/kraken2-server&quot;&gt;kraken2-server&lt;/a&gt; (&lt;a href=&quot;https://github.com/epi2me-labs/kraken2-server/blob/master/CHANGELOG.md&quot;&gt;v0.1.3&lt;/a&gt;) provides a server-client architecture for the &lt;a href=&quot;https://github.com/DerrickWood/kraken2&quot;&gt;kraken2&lt;/a&gt; software. This update addresses an idiosyncratic issue and allows the streaming of input FASTQ through STDIN.&lt;/li&gt;&lt;/ul&gt;</content:encoded><content:thumbnail>https://labs.epi2me.io/static/fe7ba47d625cdbd1bf8270ad21143d4d/59ccf/release-the-vegetables.jpg</content:thumbnail><content:toenail>https://labs.epi2me.io/static/fe7ba47d625cdbd1bf8270ad21143d4d/release-the-vegetables.jpeg</content:toenail></item></channel></rss>