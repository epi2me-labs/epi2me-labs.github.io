<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[EPI2ME Labs's RSS Feed]]></title><description><![CDATA[Tutorials and workflows for nanopore sequencing.]]></description><link>https://labs.epi2me.io</link><generator>GatsbyJS</generator><lastBuildDate>Thu, 01 Dec 2022 00:56:10 GMT</lastBuildDate><item><title><![CDATA[EPI2ME Labs 22.11.01 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-22.11.01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-22.11.01-release</guid><pubDate>Wed, 16 Nov 2022 00:00:00 GMT</pubDate><content:encoded>

Dear Nanopore Community,

With this month’s update to our EPI2ME Labs product we are delighted to introduce a new sequence basecalling workflow. We also include maintenance updates to several other workflows.

*   Our new workflow, [wf-basecalling](https://github.com/epi2me-labs/wf-basecalling &quot;https://github.com/epi2me-labs/wf-basecalling&quot;), uses a containerised [Dorado](https://github.com/nanoporetech/dorado &quot;https://github.com/nanoporetech/dorado&quot;) installation to prepare aligned CRAM from FAST5/POD5 signal data. The workflow is released with tagged version v0.0.1 and additional information may be found from the product’s GitHub pages.
    
*   The [wf-human-variation](https://github.com/epi2me-labs/wf-human-variation/ &quot;https://github.com/epi2me-labs/wf-human-variation/&quot;) workflow now also includes [Dorado](https://github.com/nanoporetech/dorado &quot;https://github.com/nanoporetech/dorado&quot;) as a bundled step for the basecalling of FAST5 (and POD5) format signal data. This replaces the earlier Guppy subworkflow. The update also includes a number of bug fixes and improvement to the documentation. Please see the project’s [CHANGELOG](https://github.com/epi2me-labs/wf-human-variation/blob/master/CHANGELOG.md &quot;https://github.com/epi2me-labs/wf-human-variation/blob/master/CHANGELOG.md&quot;) for further information. This workflow is now tagged with version v0.4.0.
    
*   The [wf-single-cell](https://github.com/epi2me-labs/wf-single-cell &quot;https://github.com/epi2me-labs/wf-single-cell&quot;) workflow now uses a modified transcript assignment method. The Salmon method for transcript mapping has been replaced with minimap2 - this change leads to the characterisation of additional gene transcripts. The update also includes a PCA preprocessing step prior to the UMAP transformation. Further information on these changes may be found in the [CHANGELOG](https://github.com/epi2me-labs/wf-single-cell/blob/master/CHANGELOG.md &quot;https://github.com/epi2me-labs/wf-single-cell/blob/master/CHANGELOG.md&quot;). The workflow is now tagged with version v0.1.5.
    
*   [wf-metagenomics](https://github.com/epi2me-labs/wf-metagenomics &quot;https://github.com/epi2me-labs/wf-metagenomics&quot;) has been updated with bug fixes. The workflow is now tagged with version v2.0.2 and further information is described in the [CHANGELOG](https://github.com/epi2me-labs/wf-metagenomics/blob/master/CHANGELOG.md &quot;https://github.com/epi2me-labs/wf-metagenomics/blob/master/CHANGELOG.md&quot;).
    
*   The tagged v0.1.6 release of [wf-transcriptomes](https://github.com/epi2me-labs/wf-transcriptomes &quot;https://github.com/epi2me-labs/wf-transcriptomes&quot;) includes both updates and fixes for the differential gene expression (DE) functionality. There are improvements to the DE documentation and an option to run DE analyses independently has been included. Please see the [CHANGELOG](https://github.com/epi2me-labs/wf-transcriptomes/blob/master/CHANGELOG.md &quot;https://github.com/epi2me-labs/wf-transcriptomes/blob/master/CHANGELOG.md&quot;) for further information.
    
*   The deprecated workflows wf-human-snp and wf-human-sv have been removed from GitHub. Please use the [wf-human-variation](https://github.com/epi2me-labs/wf-human-variation/ &quot;https://github.com/epi2me-labs/wf-human-variation/&quot;) workflow instead.
    
*   We will be deprecating the option to run workflows using the `conda` profile
    
    *   Users are advised to run our workflows using our container images via Docker \[`--profile standard`\] or Singularity instead \[`--profile singularity`\]</content:encoded><content:thumbnail>https://labs.epi2me.io/static/4f770ae58622a663e7901cc842cc8de2/thumbnail.jpeg</content:thumbnail></item><item><title><![CDATA[Copy Number Calling Workflow]]></title><description><![CDATA[A post to describe our new copy number calling workflow.]]></description><link>https://labs.epi2me.io/copy-number-calling</link><guid isPermaLink="false">https://labs.epi2me.io/copy-number-calling</guid><pubDate>Wed, 05 Oct 2022 00:00:00 GMT</pubDate><content:encoded>
## Introduction

We are pleased to add to our human analysis workflow repertoire with wf-cnv. This new workflow enables copy number calling from Oxford Nanopore Technologies sequencing data.

Best practices for human copy number calling are actively being investigated by the ONT applications team, and this workflow puts some of that work into something that can be easily used by our community.

wf-cnv also utilises our new reporting and plotting package [ezcharts](https://github.com/epi2me-labs/ezcharts). This uses python [dominate](https://github.com/Knio/dominate) and an apache [echart](https://echarts.apache.org/en/index.html) api to allow us to make modern, responsive layouts and plots with relative ease.

With the release of wf-cnv we also include a new ideogram plotting component for ezcharts.

![ezcharts ideogramns](./chr_bands.png &quot;Figure 1 - ezcharts ideogram plotting&quot;)

## Pre-requisites

Along with the usual requirements to run our workflows (nextflow &amp; docker) you will need a reference genome in FASTA format, which can be downloaded from UCSC using rsync:

- hg19:
```
rsync -a -P rsync://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz .
```

- hg38:
```
rsync -a -P rsync://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz .
```

## How to run

Example command:

```
nextflow run epi2me-labs/wf-cnv --fastq &lt;PATH_TO_FASTQS&gt; --fasta &lt;PATH_TO_REFERENCE&gt; --genome &lt;hg19|hg38&gt; --bin_size &lt;BIN_SIZE&gt;
```

## Workflow

### CNV calling methods

There are a few different approaches taken by CNV detection algorithms, such as read pair, split read, and assembly-based methods. The algorithm used here, QDNAseq, is based on the commonly-used read depth strategy, which seeks to correlate the copy number of a region with the depth of coverage, so for example, a gain in copy number would have a higher depth than expected. Typically this is achieved by dividing the genome into fixed size bins, and the number of reads within each bin counted and normalised. In addition, it is common for most tools to hold an internal &apos;blacklist&apos; of problematic regions, to improve variant calling.

### Workflow details

QDNAseq is an R package which determines the copy number status of bins, the size of which can be tuned by using the `--bin_size` parameter at run time. Pre-calculated bin annotations are available for hg19 and hg38 for a range of bin sizes (1, 5, 10, 15, 30, 50, 100, 500, and 1000 kbp).

Following alignment of raw reads to a reference, the resulting BAM is used to generate a raw copy number profile using the selected bin size. This is filtered to remove blacklisted bins in problematic genomic regions. The raw profile is further refined by estimating and applying the correction for GC content and mappability, and performing smoothing and normalisation. Segmentation (merging of regions with similar read count to estimate a CNV segment) and CNV calling are carried out using [DNAcopy](http://bioconductor.org/packages/release/bioc/html/DNAcopy.html) and [CGHcall](http://bioconductor.org/packages/release/bioc/html/CGHcall.html), respectively.

The test samples included were WGA amplified from genomic DNA and sequenced with either rapid or native barcoding for 180mins, and are cell line samples obtained from the NIGMS Human Genetic Cell Repository at the [Coriell Institute for Medical Research](https://www.coriell.org/): NA01920, NA03225, and NA03623. These samples demonstrate that with low coverage and short Nanopore reads an accurate picture of large scale copy number variations can be detected. The read length and bin size affects the results of QDNASeq analysis. In future versions we will provide preset parameters based on detected read length.

|  Barcode | Sample Name | Details | Genotypic Sex |
|----------|-------------|---------|---------------|
| barcode01 | NA01920    | Trisomy 21 | XY |
| barcode03 | NA03225    | Chr 7 Deletion | XX |
| barcode05 | NA03623    | Trisomy 18 | XXX |

The FASTQs for three test samples are available [here](https://github.com/epi2me-labs/wf-cnv/test_data/fastq) and can be used with the the accompanying sample sheet from [here](https://github.com/epi2me-labs/wf-cnv/test_data/sample_sheet.csv).

Example command with test data:

```
nextflow run epi2me-labs/wf-cnv --fastq &lt;PATH_TO_DOWNLOADED_FASTQ&gt; --sample_sheet &lt;PATH_TO_DOWNLOADED_SAMPLE_SHEET&gt; --fasta /path/to/hg38.fa.gz --genome hg38 --bin_size 500
```

We include two styles of plot on the reports for this workflow. An ideoplot that shows the copy number data from QDNAseq overlaid on a representation of human chromosomes, and a plot of log2 transformed copy number counts per bin.

![Trisomy 21 Ideoplot](./t21-ideoplot.png &quot;Figure 2 - XY Ideoplot Indicating Trisomy 21&quot;)

![Trisomy 21 Scatterplot](./t21-scatterplot.png &quot;Figure 3 - XY Ideoplot Indicating Trisomy 21&quot;)

### Output files

The workflow outputs several files per sample:

* `&lt;SAMPLE_NAME&gt;_wf-cnv-report.html`: HTML CNV report containing chromosome copy summary, ideoplot, plot of read counts per bin, links to genes in detected CNVs, and QC data: read length histogram, noise plot (noise as a function of sequence depth) and isobar plot (median read counts per bin shown as a function of GC content and mappability)
* `&lt;SAMPLE_NAME&gt;.stats`: Read stats
* `BAM/&lt;SAMPLE_NAME&gt;.bam`: Alignment of reads to reference
* `BAM/&lt;SAMPLE_NAME&gt;.bam.bai`: BAM index
* `qdna_seq/&lt;SAMPLE_NAME&gt;_plots.pdf`: QDNAseq-generated plots
* `qdna_seq/&lt;SAMPLE_NAME&gt;_raw_bins.bed`: BED file of raw read counts per bin
* `qdna_seq/&lt;SAMPLE_NAME&gt;_bins.bed`: BED file of corrected, normalised, and smoothed read counts per bin
* `qdna_seq/&lt;SAMPLE_NAME&gt;_calls.vcf`: VCF file of CNV calls



## Feedback

We welcome feedback on this, and any of our workflows, either in the nanopore community or as GitHub issues on the workflow repository.

### Useful Links

- QDNASeq GitHub: https://github.com/ccagc/QDNAseq
- QDNASeq Bioconductor: https://bioconductor.org/packages/release/bioc/html/QDNAseq.html

### Reference

- Scheinin I, Sie D, Bengtsson H, van de Wiel MA, Olshen AB, van Thuijl HF, van Essen HF, Eijk PP, Rustenburg F, Meijer GA, Reijneveld JC, Wesseling P, Pinkel D, Albertson DG, Ylstra B. DNA copy number analysis of fresh and formalin-fixed specimens by shallow whole-genome sequencing with identification and exclusion of problematic regions in the genome assembly. Genome Res. 2014 Dec;24(12):2022-32. doi: 10.1101/gr.175141.114. Epub 2014 Sep 18. [PMCID](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4248318/).
</content:encoded><content:thumbnail>https://labs.epi2me.io/static/8f0c6cd10421c178f23342561ce2015a/AdobeStock_348188744.jpeg</content:thumbnail></item><item><title><![CDATA[EPI2ME Labs 22.08.02 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-22.08.02-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-22.08.02-release</guid><pubDate>Wed, 21 Sep 2022 00:00:00 GMT</pubDate><content:encoded>
Dear Nanopore Community

We have released a [Kraken2 server](https://github.com/epi2me-labs/kraken2-server) software that has been implemented to enable [Kraken2](https://ccb.jhu.edu/software/kraken2/)\-based taxonomic classification of DNA sequences in a gRPC server-client architecture. This architectural change to the Kraken2 software means that the typically large and memory intensive database only has to be loaded once - this ameliorates a substantial bottleneck when sequence data is being processed in real-time. The Kraken2 server software is available through GitHub and pre-compiled binaries are available through conda. The software is tagged at revision v0.0.5 and will be preinstalled in appropriate workflow containers. This metagenomics server is a critical component for a new real-time implementation of wf-metagenomics that will be released in the next few days.
  
The real-time [wf-metagenomics](https://github.com/epi2me-labs/wf-metagenomics) workflow mentioned above is pending with an imminent release. The re-engineered workflow steps away from the post-run data analysis dogma and encourages the analysis of sequence data as it is being produced - the workflow is thus more akin to the EPI2ME WIMP product. The technical implementation of these reinvigorated analysis capabilities is described in detail in an accompanying [blog post](https://labs.epi2me.io/progressive-kraken2/). In summary, the new functionality leverages the `watchPath` capabilities of Nextflow and utilises the [kraken2 server](https://github.com/epi2me-labs/kraken2-server) mentioned above. We hope that this workflow might be used as a template for the development of other workflows that can explore datasets as they are generated. The updated workflow will be available in a few days time with the tagged version v2.0.0.

Our workflow for whole human genome analysis, [wf-human-variation](https://github.com/epi2me-labs/wf-human-variation/), has been updated to v0.2.0. This release includes the functionality to report regions of 5mC base-modification in the CpG sequence context. When base-calling is performed using a model that includes base-modifications, the modified base information is written to the BAM output files. These per base per read information are now distilled into a more informative BED file of genomic regions using the [modbam2bed](https://github.com/epi2me-labs/modbam2bed/) software. The BED information can be presented as tracks using genome browsers such as [IGV](https://software.broadinstitute.org/software/igv/) or [JBrowse](https://jbrowse.org/jb2/). Please also see the workflow [CHANGELOG](https://github.com/epi2me-labs/wf-human-variation/blob/master/CHANGELOG.md) for additional information.

Our [wf-single-cell](https://github.com/epi2me-labs/wf-single-cell) workflow (previously known as sockeye) has been updated and now provides a matrix of gene transcript x cell barcodes counts. The inclusion of transcript resolution data further demonstrates the value of long-read transcriptomic sequence data in the study of single-cells. Further information on the update is provided in the workflow’s [CHANGELOG](https://github.com/epi2me-labs/wf-single-cell/blob/prerelease/CHANGELOG.md). The workflow has been released with tagged version v0.1.2.

The EPI2ME Labs team would welcome recommendations for new workflows, tutorials,
and functionality that you would like to see included in the product.
</content:encoded><content:thumbnail>https://labs.epi2me.io/static/99f06f6c537ad8f0fa3eb841524bd0a9/thumbnail.jpeg</content:thumbnail></item><item><title><![CDATA[Real-time bioinformatics with Nextflow and Kraken2]]></title><description><![CDATA[An explanation of how we implemented progressive workflow using nextflow for wf-metagenomics.]]></description><link>https://labs.epi2me.io/progressive-kraken2</link><guid isPermaLink="false">https://labs.epi2me.io/progressive-kraken2</guid><pubDate>Thu, 15 Sep 2022 00:00:00 GMT</pubDate><content:encoded>
Oxford nanopore is the only sequencing platform to offer the possibility of real-time analysis during a sequencing experiment. This functionality is complemented nicely by unique features of the [Nextflow](https://www.nextflow.io/) workflow manager. The combination of Nextflow&apos;s use of data streaming and recursive operations allows us to write continuous pipelines that output progressive analysis as data becomes available. This makes it possible to gain insights in real-time using many bioinformatics tools.

## Background

There are many possible applications of progressive workflows. In this post we will explain how we implemented the real-time ability in a forthcoming release of our [wf-metagenomics](https://github.com/epi2me-labs/wf-metagenomics) workflow. We chose this workflow as there are many usecases where the ability to accumulate knowledge as data become available allows decisions in real-time before sequencing has completed. Clearly this is not possible with sequencing technologies which are inherently a batch process.

If you have worked with Nextflow you are probably aware of its dataflow paradigm: data files move around in [Channels](https://www.nextflow.io/docs/latest/channel.html). These channels can be manipulated as streams of data by [Operators](https://www.nextflow.io/docs/latest/operator.html) and [Processes](https://www.nextflow.io/docs/latest/process.html). In terms of the functionality of Nextflow, this paradigm is really just a method of programming workflows — it doesn&apos;t in many cases grant users of Nextflow any new abilities to create workflows that couldn&apos;t be created with other tools. That is until we leverage the power of `watchPath`.

## Path Watching superpowers!

The channel factory method [`watchPath()`](https://www.nextflow.io/docs/latest/channel.html#watchpath) allows us to extend the data streaming paradigm from an abstraction used in Nextflow programming to a literal input stream without a preknown input length. We can start our workflow without any data present, wait for inputs, and process them as they become available.

The `watchPath()` method allows Nextflow to watch an input directory or set of directories for files being created, modified or deleted and process them. In wf-metagenomics we watch the input directories for new `.fastq` files. This creates a Nextflow Channel without a terminator: our workflow will continue to execute indefinitely processing inputs into outputs. To stop a workflow we need to inject a signal into the input Channel. This can be achieved with `watchPath().until()` which allows specification of a stopping condition. A useful method to stop a workflow externally to the Nextflow process is to inject a specially named file into the input directory, for example we can write:

```
Channel
      .watchPath(&apos;/some/input/path/*.fastq&apos;)
      .until { file-&gt;file.name == &apos;STOP.fastq&apos; }
```

to signal Nextflow to stop waiting for data. Additionally, if desired, its possible to write the above stop signal from the workflow itself based on workflow results. This is simple but effective, however in the future we hope that Nextflow will incorporate nicer methods to signal an input channel should close.

## Aggregating results

With `watchPath()` and Nextflow `process`es we can create an output stream with items in a one-to-one correspondence with the input stream. In the context of wf-metagenomics this means creating batches of per-read classifications for each input `.fastq` file produced in real-time by an Oxford Nanopore Technologies&apos; sequencing device. More usefully however we&apos;d like to accumulate workflow outputs into an updating stream summarising all historic data. In the language of reactive programming, such an operation is called a [scan](https://reactivex.io/documentation/operators/scan.html).

We tried in vain to implement such a pattern in Nextflow before realising its not possible to encode recursive operations such as a scan. So we reached out to Paolo at SeqeraLabs and asked if this pattern could be implemented. After understanding why we would want to implement such a beast it didn&apos;t take Paolo long to provide us with the `scan()` and `recurse()` process [methods](https://github.com/nextflow-io/nextflow/discussions/2521).

The `recurse()` operator isn&apos;t terribly interesting for our use cases. Applying the operator to a process allows us to pass a single value to a process and iteratively run the process on its own output until a condition is met. The condition could be simply to apply the recursion a set number of times or a more complex function of the data.

As discussed above the `scan()` operator is what we want: it allows us to process not a single input in a recursive fashion but to process an input stream merging new input items with previous results. For the case of wf-metagenomics this means we can at each iteration:

1. accept a new `.fastq` file containing reads
2. perform metagenomic classification of the reads
3. merge the classification results with a previous classification summary 
4. output a new classification summary in real-time with the arrival of new data

As it turns out, in order to allow additional parallelism at the classification step we break apart the above into a distinct classification process and a second aggregating scan process.

As a model of the above, the code below processes an input data stream to create a cumulative output stream ([full code](./main.nf) with additional commentary). In this toy example the metagenomic classification is replaced with a simple file summary, and our cumulative output is a growing list of these summaries formatted as JSON.

```
nextflow.enable.dsl=2
nextflow.preview.recursion=true
import nextflow.util.BlankSeparatedList

/* Classify the input data stream. */
process classify {
    input: path &quot;input.txt&quot;
    output: path &quot;summary.json&quot;
    script:
    &quot;&quot;&quot;
    #!/usr/bin/env python3
    # just count the characters in the file in this example
    import json
    data = {&apos;data&apos;: [len(open(&quot;input.txt&quot;, &quot;r&quot;).read())}
    json.dump(data, open(&quot;summary.json&quot;, &quot;w&quot;))
    &quot;&quot;&quot;
}

/* Accumulate results from `classify` process
process accumulate {
    input: path data
    output: path &quot;output_${task.index}.json&quot;
    script:
        output = &quot;output_${task.index}.json&quot;
        if (data instanceof BlankSeparatedList)
            { new_input = data.getAt(0); state = data.getAt(-1) }
            else { new_input = &quot;null&quot;; state = data }
    &quot;&quot;&quot;
    #!/usr/bin/env python3
    import json
    state_data = json.load(open(&quot;${state}&quot;))
    if &quot;${new_input}&quot; != &quot;null&quot;:
        new_data = json.load(open(&quot;${new_input}&quot;))
        state_data[&quot;data&quot;].extend(new_data[&quot;data&quot;])
    json.dump(state_data, open(&quot;${output}&quot;, &quot;w&quot;))
    &quot;&quot;&quot;
}

workflow {
    // watch a directory, classify inputs, aggregate results progressively
    data = channel.watchPath(&quot;inputs/*.txt&quot;)
    classifications = classify(data)
    cumulative = accumulate.scan(classifications)
    cumulative.view(it -&gt; it.text)  // show contents of files
}
```

The `classify` process here is responsible for taking the input data stream, performing calculations on each item, and creating output that can be readily aggregated. The `accumulate` process takes the output of `classify` and recursively aggregates the data; on each iteration it outputs a new summary of the data observed thus far.


## Letting the data flow

In implementing streaming workflows with `watchPath()` and `scan()` there&apos;s at least one important &quot;gotcha&quot; of which to be aware. We must ensure that there are sufficient compute resources available to service all steps of the workflow. It&apos;s no good performing the data processing, in real-time as data arrives, if our reporting and output processes become queued behind the heavy lifting. We want our output to be updating regularly and so our reporting processes to be executing regularly.

To achieve this some optimization of input batching is required and use of one little Nextflow feature buried in the documentation.

Within wf-metagenomics we have implemented the `--batch_size` parameter. By setting this to 1, each input file is processed independently giving maximum possible granularity in how frequently (in principle) output reports can be updated. However, this can create very many small compute jobs and so larger batch sizes, coalescing some compute jobs, can be beneficial.

In order to avoid the problem highlighted above where our report processes can become starved of resource through very many classification tasks, we also control the flow by making use of the [maxForks](https://www.nextflow.io/docs/latest/process.html#maxforks) process directive. In a non-real-time workflow we would set the total executor CPU resource to say 16, and the process CPU directive to 2, with the effect that up to 8 instances of a process could run simultaneously. In a real-time workflow we want to further ensure some resources are kept available for the summary processes in order to produce updating outputs. In wf-metagenomics we constrain `maxForks` such that the total CPU resource consumed by compute steps is less than that available to the executor.


## Kraken2 Server

With everything in place for real-time streaming workflows in Nextflow we had one other problem to solve: metagenomic classification.

Of course there are excellent community tools out there already for performing this task. Our chosen tool is [Kraken2](https://ccb.jhu.edu/software/kraken2/). There is however one issue with using the Kraken2 command-line tool in a real-time setting. We&apos;d like to be able to perform classifications against large databases. [Kraken2 databases](https://benlangmead.github.io/aws-indexes/k2) can be tens of gigabytes in size and therefore take a significant amount of time to load into computer memory.

For real-time analysis we wish to avoid loading the database for every classification task streamed through the workflow. We therefore implemented a server-client architecture around the algorithms of Kraken2. The [Kraken2-server](https://github.com/epi2me-labs/kraken2-server) package is available on conda and very easy to use. It allows one-time loading of databases into persistent memory to be queried at a later time, and can also expose the databases for use in low-resource settings as the databases can be queried remotely.

Within wf-metagenomics a Kraken2 server is initiated within a Nextflow process and left to run indefinitely. Our classification process uses the Kraken2 client to send incoming sequencing data to the server without having to load the Kraken2 database for each input. The output of this process is the classification results which are fed to a downstream process which utilises Nextflow&apos;s scan to provided a continually updating classification summary for all data received thus far.

## Summary

Most sequencing platforms operate in a batch fashion: a sequencing run is started, the user waits until the run is complete and their sequencing data is output. Oxford Nanopore Technologies&apos; sequencing devices are unique in their ability to output sequencing data continuously throughout an experiment as soon as reads are complete (and with [adaptive sampling](https://nanoporetech.com/resource-centre/adaptive-sampling-oxford-nanopore), even before reads are complete!). This opens a world of data analysis possibilities, enhanced review of data by users, and opportunities for automated and manual feedback with the sequencer to adapt the experiment on-the-fly for maximum scientific value.

In this post we have shown how the reactive dataflow paradigm, used in Nextflow to write explicit data processing pipelines, naturally leads to the possiblity of processing data in real-time. With Nextflow we can perform on-line analyses within a standard bioinformatics workflow manager, using standard bioinformatics tooling, without having to resort to writing bespoke data streaming pipelines. Any suitably written Nextflow workflow can be transformed from a batch processing pipeline to a real-time workflow with minimal changes.

We are particularly excited by the prospect of using sequencing device APIs to amend sequencing parameters dynamically as a result of on-going data analysis orchestrated from within Nextflow. At a simple level this could be changing the targets of an adaptive sampling experiment to supress sequencing of common taxons, or more elaborately to feedback results of an ongoing progressive genome assembly to search for reads overlapping contig ends or poorly resolved graph structures.

Of course we are also intrigued to see what creations the community can create with the combination of real-time data analysis provided through Nanopore sequencing and Nextflow.

</content:encoded><content:thumbnail>https://labs.epi2me.io/static/d09a2c5f35316810f432b1088c8a1595/thumbnail.jpeg</content:thumbnail></item><item><title><![CDATA[Two years with Nextflow]]></title><description><![CDATA[A installation guide to getting started running Nextflow workflows on Windows.]]></description><link>https://labs.epi2me.io/two-years-of-nextflow</link><guid isPermaLink="false">https://labs.epi2me.io/two-years-of-nextflow</guid><pubDate>Sat, 27 Aug 2022 00:00:00 GMT</pubDate><content:encoded> 

import usersPlot from &quot;./users.jsx&quot;
import committersPlot from &quot;./committers.jsx&quot;

The EPI2ME Labs team has now been using [Nextflow](https://nextflow.io/) for
almost two years. In that time we have built and released around twenty
[bioinformatics workflows](https://github.com/orgs/epi2me-labs/repositories?language=&amp;q=wf-&amp;sort=&amp;type=all)
for Oxford Nanopore Technologies sequencing data analysis. Along the way there&apos;s
been a lot of head scratching, bashing of heads on desks, and some swearing.
However we&apos;ve also established a set of practices that allow us to rapidly
create, build, and deploy workflows within days. Nextflow has become the missing
piece in our analysis of DNA and RNA sequencing data.

In this post we will describe our experience of using Nextflow compared to other
workflow managers, enumerate some of the issues we&apos;ve encountered, and present
our views of how Nextflow ought to evolve to stay relevant.

## Introduction

Discussions around workflow managers have a tendency to turn into flame wars
akin to classic [vim vs. emacs](https://stackoverflow.com/questions/1430164/differences-between-emacs-and-vim)
debates.  We&apos;ve [previously](/snakemake-vs-nextflow) written about why we chose
to use Nextflow as our workflow manager of choice. Briefly to recap, and with
some confirmation bias from the last two years:

* **explicit data flow**: tasks are stitched together by the developer in a
clear manner without relying on implicit matching of filepaths (as with
make-like systems),

* **decoupling of filenames**: relatedly, tasks are isolated from each other and
communication of data is decoupled from the naming of files,

* **online analysis**: unlike many workflow managers within the informatics
space Nextflow is not limited to batch compute on static inputs. It borrows
some ideas from modern, continuous
[ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load) systems to
provide a reactive system,

* **excellent support for a multitude of compute engines**: to a large extent
Nextflow has made distributed compute a solved problem, including support for
a variety of software distribution mechanisms.

But this post isn&apos;t about extolling the virtues of one workflow manager or
another; more simply stating our experience with using the manager that we have
chosen to use. The post was motivated by a [Nextflow Language Discussion](https://github.com/nextflow-io/nextflow/discussions/3107) on GitHub
to which we&apos;ve contributed. Here we will elaborate on some of the comments made
there and provide further background.


## Discussion

The EPI2ME Labs team at Oxford Nanopore Technologies has made an investments in
Nextflow, which has paid off through the ease at which we can release new
analysis workflows to accompany a line of commercial sequencing products. 
[Our workflows](/wfindex) are primarily distributed through Nextflow&apos;s integration
with Git repositories, and use [conda](https://docs.conda.io/en/latest/) and
[Docker](https://www.docker.com/) to distribute the software used by the
workflows. The recently released [wf-flu](/influenza-workflow) workflow was
initially written in a single day from scratch, and could have been released the
same day. Our [wf-single-cell](https://GitHub.com/epi2me-labs/wf-single-cell)
was ported from a [snakemake version](https://github.com/nanoporetech/sockeye)
created by the Applications Division at Oxford Nanopore Technologies in less
than a week. Workflows authored by the EPI2ME Labs team are not only available
openly through [GitHub](https://github.com/epi2me-labs) but are also installed
on our GridION devices, and are available for use through our real-time cloud
analysis EPI2ME platform, as well as our EPI2ME Labs [Desktop Application](/downloads).

### The power of Nextflow

We&apos;ve been able to achieve this rapid pace of development through the batteries
included functionality that Nexflow contains. These are too numerous to list in
their entirety (and any who knows me knows I&apos;m mostly a miserable realist; it&apos;s
not in my nature to start waxing lyrical about Nextflow here). There are however
a few features that we have come to rely on which are worth mentioning.

It&apos;s not obvious from the GitHub repositories serving our workflows the extent
to which we leverage some of Nextflow&apos;s integrations and incidental
functionality. The first one of note is the way in which Nextflow handles cloud
compute resource. For us this is through [AWS batch](https://aws.amazon.com/batch/). Within our continuous integration system
we run our workflows with sample data on many compute environments including
local Linux servers, Apple hardware running macOS, Windows 10 systems, as well
as launching workflows into the cloud straight from our test servers with AWS
batch. This is a completely trivial affair, we simply provide Nextflow with our
access credentials and stuff happens. Nextflow handles moving local data to the
cloud, making requests for the required compute resources and downloading
results back to us. Admittedly setting up an AWS batch environment itself is not
a completely trivial affair, but is within reach of most bioinformaticians or IT
system admins.

The second aspect we leverage heavily is Nextflow&apos;s integrations with numerous
software deployment
[environments](https://www.nextflow.io/docs/latest/container.html).  All our
workflows use [conda](https://docs.conda.io/en/latest/) as a base for software
package management. We typically recommend that users make use of the container
images created with conda packages and deployed through
[Dockerhub](https://hub.docker.com/u/ontresearch). These can be used with a
variety of container runtimes; in our testing environment all our workflows are
tested with at least [Docker](https://www.docker.com/) and
[Singularity](https://sylabs.io/singularity/).  Again Nextflow manages the use
and excution of software in containers for us with no fuss. We were surprised to
find just how easy this was with various niceities such as informal script code
from the workflow being mounted into the container and available for use within
jobs.

### Intuition and dataflow

Nextflow has a bit of a reputation for having a steep learning curve to get
started. Indeed [in the words](https://github.com/nextflow-io/nextflow/discussions/3107)
of one Nextflow developer:

&gt; _&quot;Like, it’s not hard to learn, but there’s always this sinking feeling that
&gt; you’re not doing something right, and that feeling never completely goes
&gt; away.&quot;_ &lt;div style=&quot;text-align: right&quot;&gt; - Ben Sherman, Seqera Labs &lt;/div&gt;

This comment resonates very much with our experience. Despite the very fulsome
[documentation](https://www.nextflow.io/docs/latest/index.html) and
[patterns](https://nextflow-io.github.io/patterns/index.html) website there have
been several occasions where we&apos;ve not been able to understand easily how to
implement logic in a workflow that we&apos;ve wanted. We have to admit that some of
this difficulty has been our own misconceptions; but it should be commented from
experience in helping others that there are aspects of Nextflow that simply
aren&apos;t intuitive to many.

_This section assumes some knowledge of authoring Nextflow workflows and gets
technical fast! Stay with us._

Take the following example, which acts as a model for a fairly common pattern in
data analysis. We have a workflow that can process all chromosomes of a human
genome independently (Figure 1.). There are several steps however where we can
break a per-chromosome task into many smaller tasks computed independently,
before the results are gathered together for a single per-chromosome task.
Finally the workflow outputs a single result across the whole genome.
Traditionally we would think of this as a hierarchy of parallel processes: the
workflow forks to produce child processes (one per chromosome), which fork
further to produce grand-child processes.

![Map reduce workflow](./flow1.png &quot;Figure 1. Workflow pattern involving hierarchical map-reduce operations. A) Traditional view of the process, with child sub-workflows spawning further grand-child workflows. B) In Nextflow the pattern is conceptualised as a single task stream with tracking information attached to tasks allowing downstream grouping.&quot;)


It is tempting to think that such a pattern can be achieved in Nextflow through
the use of [workflow composition](https://www.nextflow.io/docs/latest/dsl2.html?highlight=workflow%20scope#workflow-composition),
and it can but not in the way one might expect by mapping workflow scopes to
map-reduce operations. Workflow scopes act merely as wrappers around a set of
linked processes, for the purposes of the data flow its simply as if the script
inside the workflow scope has been cut-and-paste into the main workflow. They
cannot be used as a function mapped across a single level of the task hierarchy,
i.e. in the current example be used to perform all work for each independent
chromosome in an independent manner, with an outer loop over the chromosomes.

To achieve the desired effect in Nextflow we instead decorate the items in our
Nextflow data Channels with information for all levels of the task hierarchy, such
that their results can be grouped back together later on. A complete example can be
found [here](https://gist.github.com/cjw85/d334352e49ddd2e8bf2bd8e3891f3fe5).
The required code turns out to be somewhat simple when you know how: everything
hinges on [line 129](https://gist.github.com/cjw85/d334352e49ddd2e8bf2bd8e3891f3fe5#file-chrom_reg-nf-L129)
where the number of sub-chromosomal regions is used to create a key alongside
the chromosome, which is used on [line 155](https://gist.github.com/cjw85/d334352e49ddd2e8bf2bd8e3891f3fe5#file-chrom_reg-nf-L155)
in a rather baroque Channel join operation.

To return the original point, the computational pattern used as an example above
is fairly common but needs a bit of insider knowledge to implement. How do we
know that we&apos;re supposed to perform the task like this? We don&apos;t. For instance
there&apos;s only a very small remark in the Nextflow documentation regarding the the
`groupKey` function of Groovy found in the
[groupTuple](https://www.nextflow.io/docs/latest/operator.html?highlight=groupkey#grouptuple)
section. Having established the correct conceptual model and knowing the
solution somehow hinges on `groupKey`, how to write the Nextflow code is not
entirely obvious. In trying to implement the pattern we found no fewer than
three historical GitHub issues where users were asking seemingly this question
(or related scenarios) with fragmented responses. We eventually contructed the
solution by piecing together fragments of answers and through trial an error.

It&apos;s interesting to note that number of experienced Nextflow users from the
community did not know how to accomplish the task. We start to think why would
this be, for such an obviously useful pattern? Our feeling is that there is a
lack of more in-depth knowledge in the community around how the internals and
scripting layer of Nextflow work and so how anything other that simple linear
workflows can be implemented. We&apos;ve experienced a similar phenomenom to that
found when dealing with the [pandas](https://pandas.pydata.org/) library in
Python: people fumble around with code, copying other snippets until it
something works the way they want, and then move on without really understanding
why. The cause of this is that various parts of Nextflow are not intuitive to
its user base. This is similar to the ideas raised by Ben Sherman that you&apos;re
never quite sure whether you&apos;re doing things as intended. The lack of real
understanding in the community is something we&apos;ll come back to.


### The elephant in the room

Groovy. There, we said it.

Before anyone gets angry, allow us to qualify this observation. For the most
part, there is little reason why most people casually writing a Nextflow
workflow for their own use need to become experts in Groovy. More important
might be understanding rudimentary concepts from functional programming. This is
perhaps the key to where many bioinformaticians struggle when starting out with
Nextflow. Those with a more biology orientated background and less computer
science must grapple with the ideas of higher order functions, iterators and
closures whilst they are simultaneously dealing with Nextflow&apos;s idiosyncrasies.
When users struggle, they are left with a feeling about what exactly their lack
of knowledge is: is it in how they are using Nextflow or their understanding of
functionality inherited from Groovy? Where should they go to read up?

We should point out that the [Nextflow Scripting](https://www.nextflow.io/docs/latest/script.html) section of the
documentation serves as a very good primer on the things most users are going to
need to know. 

A particular point to note is that writing Nextflow script is not synonymous
with writing Groovy; that is to say, the Nextflow language is not an extension
of Groovy. Some valid Groovy code is not valid Nextflow. Until
[recently](https://github.com/nextflow-io/nextflow/issues/2447#issuecomment-1171318985)
one somewhat confusing difference is that Nextflow functions do not behave as
Groovy functions. We first came across this issue, but didn&apos;t realise, a few
months into using Nextflow. The issue revolves around the fact that passing
arguments to functions in Nextflow does not work the same as in Groovy. Almost
the first time we tried to write a Nextflow function we hit the error described
in the GitHub issue. As we were new to Nextflow and Groovy we thought it was
something that we were doing wrong. Eventually we gave up, bodged the code and
moved on with our lives.

Almost a year later we happened to stumble upon the GitHub issue above and
realised that other users had had similar issues. After this we were motivated
to find a solution. It didn&apos;t take long trawling through the Nextflow source
code to realise that the error was indeed in Nextflow and not anything we had
been doing. This was somewhat surprising to us: Nextflow is a fairly well used
tool at this point, but we had come across a seemingly obvious foible in the
language.

To be more constructive in our criticism, it is not that we believe that
Nextflow is bad because it is written in Groovy but that it suffers because of
it. There has historically been a lack of developers to maintain, support, and
extend Nextflow (Figure 2.); it is approximately the product of a single person.
This has limited the rate of bug fixes, addition of new features and removal of
historical warts for new ideas.

&lt;Bokeh plotJson={committersPlot} plotName=&quot;committers-plot&quot; plotCaption=&quot;Figure
2. Unique contributers to Nextflow on GitHub. There has historically been very
few community contributions. (Commits including the phrases &apos;docs&apos; and &apos;typo&apos;
were excluded along with those that changed exclusively &apos;.rst&apos; files).&quot;/&gt;

The [nf-core](https://nf-co.re/) project is in some ways another example
of this. In addition to providing a set of practices for authoring workflows,
the nf-core project adds a suite of tools for maintaining projects and runtime
tools adding functionality to the base Nextflow experience. It is notable that
these tools are not written in Groovy; it is not the language of choice for even
seasoned Nextflow professionals. The existence of the runtime tools begs the
question why are such things not simply in the base experience?

The most notable runtime functionality found in nf-core is the creation of
command-line help information from a specification of workflow parameters. Yes,
it really is the case that Nextflow currently provides no mechanism to validate
parameters passed from the user. A developer cannot for example indicate that a
certain parameter should be an integer between 1 and 10. It does however try to
perform automatic coercion of parameters to types, with sometimes frustrating
[consequences](https://github.com/nextflow-io/nextflow/issues/2083).

So should Nextflow be rewritten in a less esoteric language? Probably not, but
we do think many of rough edges and questionable design choices in the language
would have been smoothed off by now if it were written in a language that
promoted a higher level of engagement with developers. Unfortunately we do not
believe there is an easy solution to this problem: Groovy is not a commonly
known language either within Nextflow&apos;s audience or the wider data analysis and
computing community.

### Growing pains

With the above considered we wonder if Nextflow is in someway a victim of its
own success. We have encountered various examples of issues we&apos;d have expected
to have not occurred or to have better solutions by this point in Nextflow&apos;s
life. Afterall, we are coming to Nextflow somewhat late in the game. Is it that
there are not a sufficient number of active developers to keep pace with user
requests for new functionality and the fixing of issues? Certainly examining
contributions from the community, other that contributions to the documentation,
show many add extensions of a discrete type such as adding the ability to run
jobs on a new type of job scheduler. Many do not affect the codebase in a more
extensive manner.

Examining issues reported through GitHub does not provide conclusive evidence of
this hypothesis, Figure 3. We searched GitHub for the author of all issues on
both the `nextflow-io/nextflow` project and all projects under the `nf-core`
organisation. We included the latter as we presumed that users may have started
to ask questions through nf-core that they would have otherwise ask through
nextflow. As it transpires, nf-core contributes a large part to the total
community: we find at most a 10% overlap between the two groups of issues for
any one half-year.

&lt;Bokeh plotJson={usersPlot} plotName=&quot;users-plot&quot; plotCaption=&quot;Figure 3. Unique
users reporting issues (or discussions) on GitHub. We interpret this data as a
proxy to the size of the Nextflow user community. The intersection between the
two groups is at most 10%.&quot;/&gt;

We do however admit that perhaps early 2018 serves as a turning point in at
least a figurative sense. The number of users at this time became more than one
user per three days reporting at least one issue. That is not an insignificant
number of users, especially considering the very few developers (even fewer
working full-time on the project) revealed by of analysis of contributors
(Figure 2.). 

### Final words

We would be remiss to not mention what we believe is the biggest issue currently
facing Nextflow as a language. This is something already mentioned on the
[Nextflow Language Discussion](https://github.com/nextflow-io/nextflow/discussions/3107).  _*Our
biggest pain point with Nextflow is the debugging experience when writing
Nextflow scripts*_. It is not uncommon to see errors like the following:

```
Script compilation error
- file : workflow.nf
- cause: Unexpected input: &apos;{&apos; @ line 19, column 10.
   workflow {
            ^

1 error
```

What does this mean? Simply that we have an error somewhere in our workflow
script: we get no more help! This leads to a very frustrating debugging
experience, especially when taken in context with the other issues raised in
this post.


## Summary

In this post we&apos;ve explored our experience of using Nextflow over the last two
years. We&apos;ve spent a lot of time highlighting some of the larger difficulties
we&apos;ve experienced and tried to rationalise why these may have occured in terms
of the number of active users and developer using and working with the software.
Some of the issues we&apos;ve had have been conceptual in nature, heightened by a
lack of knowledge and support in the community. Others were rather more
surprising considering the age and maturity of Nextflow &amp;#150 historical warts
and rough edges exist of which users need to be aware.

So what are we to do? The first is that we encourage Nextflow users to submit
issues on GitHub, and contribute to the Nextflow language improvement
[discussion](https://github.com/nextflow-io/nextflow/discussions/3107). For
those who are able (and those who wish to learn), we urge you to get involved
with Nextflow and expand community contributions. We will certainly keep using
Nextflow and continue to break it!

Nextflow provides us and our users with many benefits portable execution on many
compute platforms to ease of distribution and explicit data flow and automatic
implicit parallel compute (when you know how). We look forward to a time when
with a better debugging experience, the parameter system is rationalised, and
the language simplified and cleaned up. Who knows, might we one day soon have a
DSL3?</content:encoded><content:thumbnail>https://labs.epi2me.io/static/d1d13df7c944811e2a70283b06dcc26d/thumbnail.jpeg</content:thumbnail></item><item><title><![CDATA[Influenza Workflow]]></title><description><![CDATA[Details of a workflow for Influenza typing using Oxford Nanopore Technologies sequencing.]]></description><link>https://labs.epi2me.io/influenza-workflow</link><guid isPermaLink="false">https://labs.epi2me.io/influenza-workflow</guid><pubDate>Wed, 24 Aug 2022 12:30:00 GMT</pubDate><content:encoded>
We are pleased to offer a new workflow for the analysis of targeted Oxford Nanopore Technologies sequencing of Influenza virus.

Influenza is a single stranded RNA virus and contains a 13.5-14.5kb genome which is split into 8 segments encoding 10-14 proteins (dependent on strain).

The virus is classified using two proteins found on the outer surface of the viral capsid. You&apos;ve probably heard of H1N1 Influenza for example. The H represents hemagglutinin and the N is neuraminidase.

The Oxford Nanopore Technologies protocol listed [here](https://community.nanoporetech.com/docs/prepare/library_prep_protocols/ligation-sequencing-influenza-whole-genome) amplifies segments of the Influenza Type A and Type B genomes. Using the analysis workflow described here users can determine the most likely strain of Influenza to which the sample being sequenced belongs.

As with all our workflows we welcome your comments and suggestions for improvements or any reports of isssues on the [nanopore community](https://community.nanoporetech.com/) or on [GitHub](https://github.com/epi2me-labs/wf-flu/issues).

## How To Run

You have two options for local analysis of your Influenza data.

### 1. Command Line

```
nextflow nextflow run epi2me-labs/wf-flu --fastq &lt;PATH_TO_DEMULTIPLEXED_FASTQS&gt;
```

Optional parameters include
-  `--sample_sheet &lt;PATH_TO_SAMPLESHEET&gt;` (Example below)
-  `--downsample &lt;NUMBER_OF_READS&gt;` (default: None, suggested: 500)
-  `--min_qscore &lt;MINIMUM_READ_QSCORE&gt;` (default: 9)

#### Sample Sheet

A sample sheet allows you to name your samples, it must be a comma or tab separated file like below (note: this is equivalent to that which can be provided to MinKNOW, the workflow makes use of only a subset of the columns as shown below):

```
barcode,sample_id,type
barcode02,H1N1_strain_A-PR-8-34,test_sample
barcode03,H1N1_strain_A-Virginia-ATCC1-2009,test_sample
barcode04,H3N2_strain_A-Virginia-ATCC6-2012,test_sample
barcode08,fluB-BY-massachusetts-2-2012,test_sample
barcode09,fluB_B-taiwan-2-62,test_sample
barcode10,fluB-lee-40,test_sample
barcode31,fluB-yamagata-florida-4-2006_1,test_sample
barcode91,fluB-yamagata-florida-4-2006_2,test_sample
barcode55,fluA_H3N2_A-wisconsin-15-2009,test_sample
```

### 2. EPI2ME Labs

You can also use EPI2ME Labs, our straightforward application for the point and click execution of Nextflow workflows. This is available for Windows, MacOS and ubuntu on our [downloads](https://labs.epi2me.io/downloads) page.

![EPI2ME Labs Application](labs.png &quot;EPI2ME Labs Application&quot;)
![EPI2ME Labs Influenza Workflow Settings](labs-flu.png &quot;EPI2ME Labs Influenza Workflow Settings&quot;)

## Data Analysis Details

The workflow analyses all samples on a multiplexed Influenza sequencing run and provide an easy to interpret report.

The workflow carries out the following steps:
1. Concatenate reads from the same sample &amp; filter out short reads &lt; 200 bases long
2. Filter reads below median qscore of 9 (can be adjusted by changing `--min_qscore parameter`)
3. Align reads to reference (minimap2)
4. Optional downsampleing to speed up execution (activate by using `--downsample 500`)
5. Coverage calculations (samtools)
6. Call variants with medaka
7. Make a (coverage masked) consensus with bcftools
8. Perform Influenza typing with abricate

### Reference

We align to the CDC&apos;s multi-fasta Influenza reference which contains FluA + FluB segments, as well as including alternative segment sequences from disparate strains:

`A_MP, A_NP, A_NS, A_PA, A_PB1, A_PB2, A_HA_H1, A_HA_H10, A_HA_H11, A_HA_H12, A_HA_H13, A_HA_H14, A_HA_H15, A_HA_H16, A_HA_H2, A_HA_H3, A_HA_H4, A_HA_H5, A_HA_H6, A_HA_H7, A_HA_H8, A_HA_H9, A_NA_N1, A_NA_N2, A_NA_N3, A_NA_N4, A_NA_N5, A_NA_N6, A_NA_N7, A_NA_N8, A_NA_N9, B_HA, B_MP, B_NA, B_NP, B_NS, B_PA, B_PB1, B_PB2`

### Downsampling

Downsampling is optional and will speed up workflow execution.

For every segment in the reference genome, the workflow:
1. Finds the length of the segment
2. Finds reads within ±10% of the segment length
3. Collates a balanced set of reads from forward and reverse strands to achieve the desired read count
### Typing

Typing is carried out using [abricate](https://github.com/tseemann/abricate) with the INSaFLU database containing the  sequences in the table below.

| Database|Gene|Accession|Details|
|----|----|----|----|
|insaflu|M1|MK576795|Type_A MK576795 A/England/7821/2019 2019/01/03 7 (MP)
|insaflu|M1|AF100378|Type_B AF100378.1 Influenza B virus B/Yamagata/16/88 segment 7 M1 matrix protein (M) and BM2 protein (BM2) genes, complete cds
|insaflu|HA|FJ966974|H1 FJ966974.1 Influenza A virus (A/California/07/2009(H1N1)) segment 4 hemagglutinin (HA) gene, complete cds
|insaflu|HA|L11142|H2 L11142.1 Influenza A virus (A/Singapore/1/57 (H2N2)) hemagglutinin (HA) gene, complete cds
|insaflu|HA|MK576794|H3 MK576794 A/England/7821/2019 2019/01/03 4 (HA)
|insaflu|HA|AF285883|H4 AF285883.2 Influenza A virus (A/Swine/Ontario/01911-2/99 (H4N6)) segment 4 hemagglutinin (HA) gene, complete cds
|insaflu|HA|EF541403|H5 EF541403.1 Influenza A virus (A/Viet Nam/1203/2004(H5N1)) segment 4 hemagglutinin (HA) gene, complete cds
|insaflu|HA|AB295613|H15 AB295613.1 Influenza A virus (A/duck/Australia/341/83(H15N8)) HA gene for haemagglutinin, complete cds
|insaflu|NA|GQ377078|N1 GQ377078.1 Influenza A virus (A/California/07/2009(H1N1)) segment 6 neuraminidase (NA) gene, complete cds
|insaflu|NA|MK576796|N2 MK576796 A/England/7821/2019 2019/01/03 6 (NA)
|insaflu|NA|AB295614|N8 AB295614.1 Influenza A virus (A/duck/Australia/341/83(H15N8)) NA gene for neuraminidase, complete cds
|insaflu|HA|AY338459|H7 AY338459.1 Influenza A virus (A/Netherlands/219/2003(H7N7)) segment 4 hemagglutinin (HA) gene, complete cds
|insaflu|HA|CY014659|H8 CY014659.1 Influenza A virus (A/turkey/Ontario/6118/1968(H8N4)) segment 4, complete sequence
|insaflu|HA|CY014694|H13 CY014694.1 Influenza A virus (A/gull/Maryland/704/1977(H13N6)) segment 4, complete sequence
|insaflu|HA|CY018765|Yamagata CY018765.1 Influenza B virus (B/Yamagata/16/1988) segment 4, complete sequence
|insaflu|HA|CY103892|H17 CY103892.1 Influenza A virus (A/little yellow-shouldered bat/Guatemala/060/2010(H17N10)) hemagglutinin (HA) gene, complete cds
|insaflu|NA|CY103894|N10 CY103894.1 Influenza A virus (A/little yellow-shouldered bat/Guatemala/060/2010(H17N10)) neuraminidase (NA) gene, complete cds
|insaflu|NA|CY125730|N3v2 CY125730.1 Influenza A virus (A/Mexico/InDRE7218/2012(H7N3)) neuraminidase (NA) gene, complete cds
|insaflu|HA|CY125945|H18 CY125945.1 Influenza A virus (A/flat-faced bat/Peru/033/2010(H18N11)) hemagglutinin (HA) gene, complete cds
|insaflu|NA|CY125947|N11 CY125947.1 Influenza A virus (A/flat-faced bat/Peru/033/2010(H18N11)) neuraminidase-like protein (NA) gene, complete cds
|insaflu|HA|CY130078|H12 CY130078.1 Influenza A virus (A/duck/Alberta/60/1976(H12N5)) hemagglutinin (HA) gene, complete cds
|insaflu|HA|CY130094|H14 CY130094.1 Influenza A virus (A/mallard/Astrakhan/263/1982(H14N5)) hemagglutinin (HA) gene, complete cds
|insaflu|NA|CY130096|N5 CY130096.1 Influenza A virus (A/mallard/Astrakhan/263/1982(H14N5)) neuraminidase (NA) gene, complete cds
|insaflu|HA|DQ376624|H6 DQ376624.1 Influenza A virus (A/chicken/Taiwan/0705/99(H6N1)) hemagglutinin (HA) gene, complete cds
|insaflu|HA|EU293864|H16 EU293864.1 Influenza A virus (A/black-headed gull/Turkmenistan/13/76(H16N3)) hemagglutinin (HA) gene, complete cds
|insaflu|HA|FJ183474|H10 FJ183474.1 Influenza A virus (A/mallard/Bavaria/3/2006(H10N7)) segment 4 hemagglutinin (HA) gene, complete cds
|insaflu|NA|FJ183475|N7 FJ183475.1 Influenza A virus (A/mallard/Bavaria/3/2006(H10N7)) segment 6 neuraminidase (NA) gene, complete cds
|insaflu|NA|GQ907296|N3v1 GQ907296.1 Influenza A virus (A/black headed gull/Mongolia/1756/2006(H16N3)) segment 6 neuraminidase (NA) gene, complete cds
|insaflu|HA|GU052203|H11 GU052203.1 Influenza A virus (A/duck/England/1/1956(H11N6)) segment 4 hemagglutinin (HA) gene, complete cds
|insaflu|NA|KC853765|N9 KC853765.1 Influenza A virus (A/Hangzhou/1/2013(H7N9)) segment 6 neuraminidase (NA) gene, complete cds
|insaflu|HA|KX879589|H9 KX879589.1 Influenza A virus (A/swine/Hong Kong/9/98(H9N2)) segment 4 hemagglutinin (HA) gene, partial cds
|insaflu|HA|M58428|Victoria M58428.1 Influenza B/Victoria/2/87, hemagglutinin (seg 4), RNA
|insaflu|NA|EU429793|N4 EU429793.1 Influenza A virus (A/turkey/Ontario/6118/1968(H8N4)) segment 6 neuraminidase (NA) mRNA, complete cds
|insaflu|NA|EU429795|N6 EU429795.1 Influenza A virus (A/duck/England/1/1956(H11N6)) segment 6 neuraminidase (NA) mRNA, complete cds

### Output Files

The workflow outputs several files that are useful for interpretation and analysis:

* Per run:
  * `wf-flu-report.html`: Easy to use HTML report for all samples on the run
  * `wf-flu-results.csv`: Typing results in CSV format for onward processing
* Per sample:
  * `&lt;SAMPLE_NAME&gt;.stats`: Read stats
  * `&lt;SAMPLE_NAME&gt;.bam`: Alignment of reads to reference
  * `&lt;SAMPLE_NAME&gt;.bam.bai`: BAM index
  * `&lt;SAMPLE_NAME&gt;.annotate.filtered.vcf`: medaka called variants
  * `&lt;SAMPLE_NAME&gt;.draft.consensus.fasta`: Consensus FASTA
  * `&lt;SAMPLE_NAME&gt;.insaflu.typing.txt`: abricate typing results
  * `&lt;SAMPLE_NAME&gt;.depth.txt`: samtools depth, columns are contig, postion, and coverage


## Useful Links

- INSaFLU: https://insaflu.insa.pt/
</content:encoded><content:thumbnail>https://labs.epi2me.io/static/43cc1b71b5c0a30813a78e5c60045c59/AdobeStock_290465641.jpeg</content:thumbnail></item><item><title><![CDATA[EPI2ME Labs 22.08.01 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-22.08.01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-22.08.01-release</guid><pubDate>Wed, 24 Aug 2022 00:00:00 GMT</pubDate><content:encoded>
Dear Nanopore Community

We are delighted to introduce two new bioinformatics workflows and a collection
of updates and fixes to our EPI2ME Labs product.

New workflow, [wf-single-cell](https://github.com/epi2me-labs/wf-single-cell) for the analysis of single-cell transcriptomics
data. This is based on the GitHub research tool, [sockeye](https://github.com/nanoporetech/sockeye), developed by our
Applications team. This new workflow provides usability through its pre-packaged
Docker containers and EPI2ME Labs graphical user interface. The latest updates
to Sockeye have been included. The sockeye software will be deprecated and we
would recommend that issues are reported through this project’s GitHub. The
workflow is currently available at version v0.1.0.

New workflow, [wf-flu](https://github.com/epi2me-labs/wf-flu), for the analysis of FluA and FluB genomes prepared using
the ligation protocol for whole flu genome sequencing - see protocol [release
note](https://community.nanoporetech.com/posts/protocol-for-whole-genome). This workflow prepares whole genome consensus sequences following FASTQ
read mapping to the CDC multi-fasta Influenza reference. Strain typing is
performed using the [abricate](https://github.com/tseemann/abricate) software with an [INSaFLU](https://insaflu.insa.pt/) database. This experimental protocol is
also based on work developed by our Applications team. More information
can be found in a separate [post](https://labs.epi2me.io/influenza-workflow/).

Our workflow for transcriptome analysis, [wf-transcriptomes](https://github.com/epi2me-labs/wf-transcriptomes/), has been updated to
include functionality for differential gene expression in addition to the
previous reference and de novo based isoform detection and fusion gene detection
functionality. The differential expression workflow uses the analysis logic from
the EPI2ME Labs tutorial - the tutorial will now be deprecated in favour of the
workflow. Please see the workflow’s [CHANGELOG](https://github.com/epi2me-labs/wf-transcriptomes/blob/prerelease/CHANGELOG.md) for additional information on
fixes included in this release. The wf-transcriptomes workflow is tagged at
version v0.1.5. 

Our workflow for whole human genome variation analysis, [wf-human-variation](https://github.com/epi2me-labs/wf-human-variation/), has
been updated to version v0.1.1. This update includes minor bug fixes and offers
better control for alignment performance. Please see the [CHANGELOG](https://github.com/epi2me-labs/wf-human-variation/blob/master/CHANGELOG.md) for further
details.

[wf-clone-validation](https://github.com/epi2me-labs/wf-clone-validation/), the workflow for assembling complete plasmids from rapidly
sequenced plasmid preparations has been updated to version v0.2.4. This release
includes a couple of minor fixes and the software now runs on computers
installed with Windows/WSL. Please see the [CHANGELOG](https://github.com/epi2me-labs/wf-clone-validation/blob/master/CHANGELOG.md) for additional information.

The epi2melabs-notebook container image used by the EPI2ME Labs desktop
application has been updated to version v1.1.34. This release includes the new
wf-flu and wf-single-cell workflows. Please update your EPI2ME Labs installation
with the “Environment update” button provided through the settings cog at the
bottom left of your EPI2ME Labs panel.

Our workflow for whole human genome SNP analysis, wf-human-snp, is to be
deprecated and will be removed from the EPI2ME Labs product in a future release.
The SNP analysis software is now contained within the wf-human-variation
software where it will continue to be developed and supported. 


The EPI2ME Labs team would welcome recommendations for new workflows, tutorials,
and functionality that you would like to see included in the product.
</content:encoded><content:thumbnail>https://labs.epi2me.io/static/381808f1423828f22dde6e7878b45aa7/yellow-helix.jpeg</content:thumbnail></item><item><title><![CDATA[EPI2ME Labs 22.07.02 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-22.07.02-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-22.07.02-release</guid><pubDate>Wed, 27 Jul 2022 00:00:00 GMT</pubDate><content:encoded>
Dear Nanopore Community,

Our workflow for SARS-CoV-2 genome analysis, [wf-artic](https://github.com/epi2me-labs/wf-artic/), has been updated to
version v0.3.18. This release includes updates to the Pangolin and Nextclade
software components and ensures that the most recent SARS-CoV-2 strains can be
reported. Further details may be found from the workflow [changelog](https://github.com/epi2me-labs/wf-artic/blob/master/CHANGELOG.md). This update
is synchronised with an EPI2ME release. EPI2ME Fastq QC+ARTIC+NextClade is now
available with version v2022.07.19.

We have released a freshly named workflow, [wf-transcriptomes](https://github.com/epi2me-labs/wf-transcriptomes), with version
v0.1.4 that replaces the earlier wf-isoforms product. Included in this release
is new functionality for fusion transcript detection - these analyses are
performed using the [JAFFA](https://github.com/Oshlack/JAFFA) software.

Example datasets are now provided through the EPI2ME Labs GUI for the wf-artic
and wf-transcriptomes workflows. Press the “Workflow demo” button at the top of
the workflow page and the workflow design will be populated with a small
pre-packaged dataset. These updates require an update to both the EPI2ME Labs
software (v3.1.3) and the epi2melabs-notebook container (v1.1.30).

We look forwards to any feedback and welcome suggestions and recommendations for
future workflows, tutorials, or new functionality to include in EPI2ME Labs.

</content:encoded><content:thumbnail/></item><item><title><![CDATA[Reduced Representation Methylation Sequencing (RRMS)]]></title><link>https://labs.epi2me.io/rrms2022.07</link><guid isPermaLink="false">https://labs.epi2me.io/rrms2022.07</guid><pubDate>Wed, 27 Jul 2022 00:00:00 GMT</pubDate><content:encoded>
We are excited to release the new protocol for [Reduced Representation
Methylation Sequencing (RRMS)](https://community.nanoporetech.com/docs/prepare/library_prep_protocols/ligation-sequencing-gdna-rrms/v/rrms_v3/overview-of-the-protocol?devices=minion), login required, from the Nanopore Applications Team.

## Background

CpG dinucleotides frequently occur in high-density clusters called CpG islands
(CGI) and &gt;60% of human genes have their promoters embedded within CGIs.
Determining the methylation status of cytosines within CpGs is of substantial
biological interest: alterations in methylation patterns within promoters is
associated with changes in gene expression and disease states such as cancer.
Exploring methylation differences between tumour samples and normal samples can
help to elucidate mechanisms associated with tumour formation and development.
Nanopore sequencing enables direct detection of methylated cytosines (e.g. at
CpG sites), without the need for bisulfite conversion.

## Adapative sampling for CpG containing regions

Oxford Nanopore’s Adaptive Sampling (AS) offers a fast, flexible and precise
method to enrich for regions of interest (e.g. CGIs) by depleting off-target
regions during the sequencing run itself with no requirement for upfront sample
manipulation. Here we introduce Reduced Representation Methylation Sequencing
(RRMS), which combines Oxford Nanopore’s methylation API, Remora, with AS, to
target 310 Mb of the human genome including regions which are highly enriched
for CpGs including ~28,000 CpG islands, ~50,600 shores and ~42,700 shelves as
well as ~21,600 promoter regions.

To read more about how the method works, and how it compares to other techniques
for analysing methylation (e.g. EPIC arrays, bisulfite), please see our
[Introduction to Reduced Representation Methylation Sequencing](https://community.nanoporetech.com/attachments/7599/download) (login required).

## Benchmarks

To benchmark the performance of RRMS, we performed RRMS on five replicates of a
metastatic melanoma cell line and its normal pair for a male individual
([COLO829](https://www.atcc.org/products/crl-1974)/[COLO829_BL](https://www.atcc.org/products/crl-1980)) and a triple negative breast cancer cell-line pair
([HCC1395](https://www.atcc.org/products/crl-2324)/[HCC1935_BL](https://www.atcc.org/products/crl-2325)) and ran each sample on a separate MinION flow cell (one
sample per flow cell). RRMS resulted in high-confidence methylation calls for
7.3 - 8.5 million CpGs per sample (covering ~90+% of promoters, CGIs, shores and
shelves, and covering 28% of all CpGs in the human genome). For comparison we
also performed Reduced Representation Bisulfite Sequencing (RRBS), which
typically yields 1.7 - 2.5 high-confidence calls per sample (approximately 10%
of the total number of CpGs in the human genome). Methylation frequencies called
by RRMS and RRBS are highly similar for those CpGs which are covered by both
technologies (R &gt; 0.967). Furthermore, with RRMS we were able to detect ~62 Mb
of differentially methylated regions (DMRs) between tumour and normal pairs, of
which a high proportion overlapped with cancer census genes: this demonstrates
the value of RRMS for providing key information for tumour characterisation as
well as methylation status. In comparison RRBS yielded ~20 Mb of DMRs. For more
information about benchmarking the performance of RRMS, please see our [RRMS
performance document](https://community.nanoporetech.com/attachments/7600/download) (login required).

## Data Availability

Data, including BAM files with per-read modification calls
as well as the raw data (FAST5s) for all samples is available in our ONT Open Datasets
S3 bucket:

```
s3://ont-open-data/rrms_2022.07/
```

We also have available the matched reduced representation bisulfite sequencing
data for the four samples, processed with Bismarck to produce methylation
frequencies:

```
s3://ont-open-data/rrms_2022.07/bisulfite/
```


## Summary

Combined with its ease of use and ability to scale to a high number of samples,
RRMS is well suited to investigating methylation differences in large cohorts,
as well as providing deeper insights into the mechanisms behind diseases such as
cancer and monitoring tumour progression. For more information, please see our
[poster](https://nanoporetech.com/resource-centre/reduced-representation-methylation-sequencing-rrms-captures-100-cpg-islands-and).</content:encoded><content:thumbnail>https://labs.epi2me.io/static/cc2e8d2ae04dbdf6e1b3606f3102280d/methylation.png</content:thumbnail></item><item><title><![CDATA[EPI2ME Labs 22.07.01 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-22.07.01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-22.07.01-release</guid><pubDate>Wed, 13 Jul 2022 00:00:00 GMT</pubDate><content:encoded>
Dear Nanopore Community

We are pleased to release a collection of EPI2ME Labs updates that include new
workflows, improved workflows, new functionality, and a couple of bug fixes.

A new human genetic variation workflow, wf-human-variation, has been released
with version v0.1.0. This integrated workflow identifies both single nucleotide
polymorphisms and structural variations from human whole genome sequencing data.
The workflow supports “uBAM” (unaligned BAM) as an input data type and
additional functionality, including methylation analysis, will be included in a
future update.

[wf-human-sv](https://github.com/epi2me-labs/wf-human-sv), the workflow for the detection of human SV has been updated to
version v1.0.0. With this release the sequence mapping method has been updated
to minimap2 and the SV detection step now uses the [Sniffles 2](https://github.com/fritzsedlazeck/Sniffles) software. More
information may be found in the workflow’s [CHANGELOG](https://github.com/epi2me-labs/wf-human-sv/blob/master/CHANGELOG.md).

[wf-human-snp](https://github.com/epi2me-labs/wf-human-snp) for the analysis of germline human SNPs has been updated to version
v0.3.2. This update includes an update to the bundled version of longphase and
improvements to the workflow’s online documentation. Please see the [CHANGELOG](https://github.com/epi2me-labs/wf-human-snp/blob/master/CHANGELOG.md)
for additional information.

[wf-mpx](https://github.com/epi2me-labs/wf-mpx/), our workflow for the analysis of human monkeypox genome sequences, has
been updated to version v0.0.4. This includes a minor update to accommodate a
minimum depth of coverage for calling genetic differences relative to the
reference sequence. Please see the workflow [CHANGELOG](https://github.com/epi2me-labs/wf-mpx/blob/master/CHANGELOG.md) for further information on
the changes.

[wf-clone-validation](https://github.com/epi2me-labs/wf-clone-validation), the cloning plasmid sequence validation workflow, has been
updated to version v0.2.2. This release includes the latest version of the
[pLannotate](https://github.com/barricklab/pLannotate) software. This version includes additional plasmid feature
annotations from the protein sequence source databases e.g. RFAM and SWISSPROT.
Please see the [CHANGELOG](https://github.com/epi2me-labs/wf-clone-validation/blob/master/CHANGELOG.md) for further details on the changes. This workflow has
also been updated in our EPI2ME product for bioinformatics analysis in the
cloud; the updated workflow is available as Fastq Clone Validation v2022.07.12.

EPI2ME Labs workflows now support “canned datasets”. In our earlier EPI2ME Labs
tutorials, we provided minimal example datasets that could be used to test the
tutorials and explore the results produced. The schema behind our bioinformatics
workflows now supports such example datasets and example demonstration data will
be rolled out across all workflows over the next few weeks. This functionality
is available with the [EPI2ME Labs](https://labs.epi2me.io/downloads) v3.1.2 software and with the versions of the
epi2melabs-notebook container &gt;= v1.1.27.

![canned-data.png](./canned-data.png &quot;Figure 1. EPI2ME Labs workflows now
support example datasets. In the screenshot above, a new “Workflow demo” button
has been included that facilitates running a workflow with a provided dataset.
In this example a pre-configured wf-alignment workflow can be run to map
sequence reads against a reference genome with just a couple of mouse clicks.&quot;)

Other updates to EPI2ME Labs platform include improvements to file browsing and
input sequence selections on computers running Windows and a bug fix where
result files were not visible within the EPI2ME Labs software.

Deprecation warning for SARS-CoV-2 analysis tutorial. The SARS-CoV-2 analysis
tutorial that demonstrates an ARTIC based analysis of amplicon sequenced
SARS-CoV-2 genomes is being deprecated. Any users who are still using this
tutorial should migrate to the supported and maintained [wf-artic](https://github.com/epi2me-labs/wf-artic) bioinformatics
workflow instead. 


We welcome any suggestions for future tutorials, workflows or usability
improvements.</content:encoded><content:thumbnail/></item><item><title><![CDATA[Monkeypox Workflow]]></title><description><![CDATA[A quick description of a new workflow for basic Monkeypox virus Oxford Nanopore Technologies sequencing analysis.]]></description><link>https://labs.epi2me.io/basic-monkeypox-workflow</link><guid isPermaLink="false">https://labs.epi2me.io/basic-monkeypox-workflow</guid><pubDate>Wed, 01 Jun 2022 12:30:00 GMT</pubDate><content:encoded>
Monkeypox (MPX) is a double-stranded DNA virus. There is an ongoing outbreak of the West African clade of the virus in
multiple countries. Data has been generated for a number of these cases using Oxford Nanopore Technology sequencing
and here we describe `wf-mpx`, a decentralised workflow to analyse this data on device, anywhere.

## Data Analysis

A dive into the many excellent community posts on [virological.org](virological.org) indicated that:
* people were mapping to existing references,
* creating a consensus based on this mapping,
* or they were creating de novo assemblies;
* and in either case, performing some manual review.

We wanted to empower those users who perhaps are keen to sequence MPX using Oxford Nanopore Technologies
devices but don&apos;t have the expertise or resources to throw together an analysis workflow. We have therefore released `wf-mpx`. By releasing this workflow in it&apos;s nascent state anyone with ONT Monkeypox data, be it metagenomics or something more targeted can get a __draft__ consensus using EPI2ME Labs.

`wf-mpx` is by no means a comprehensive workflow for the creation of Monkeypox consensus sequences or assemblies,
but it might get you started analysing your data.

You should be particulalry careful using this workflow if you have amplicon or other targeted data, __no
trimming of adapters or primers is carried out by this workflow__.

If you have any issues, thoughts, or suggestions please don&apos;t hesitate to raise an issue for us on GitHub: [epi2me-labs/wf-mpx](https://github.com/epi2me-labs/wf-mpx).

## Workflow Details

You can run the workflow in two ways:

1. In EPI2ME Labs - you can click the workflow and complete the path to your fastq files. You can download EPI2ME Labs from [here](https://labs.epi2me.io/downloads)
2. On the command line:
   ```
   nextflow run epi2me-labs/wf-mpx --fastq &lt;PATH_TO_FOLDER_OF_FASTQ_FILES&gt;
   ```

### Workflow Steps

The workflow takes a single folder of fastq files (more coming soon) and:

* Maps the reads using `minimap2` to a reference from a choice of:
    * ON568298.1 - German sequence described [here](https://virological.org/t/first-german-genome-sequence-of-monkeypox-virus-associated-to-multi-country-outbreak-in-may-2022/812)
    * MT903344.1 - Monkeypox virus isolate MPXV-UK_P2 [NCBI](https://www.ncbi.nlm.nih.gov/nuccore/MT903344.1)
    * MN648051.1 - Monkeypox virus strain Israel_2018 [NCBI](https://www.ncbi.nlm.nih.gov/nuccore/MN648051)
    * ON563414.1 - USA Center for Disease Control sequence [NCBI](https://www.ncbi.nlm.nih.gov/nuccore/ON563414)
* Assesses coverage
* Keep only reads mapping to reference to exclude potential human reads
* Calls variants with respect to that reference using `medaka`
* Filters variants with &lt;20x depth
* Creates a __draft__ consensus using `bcftools` from the variants and reference:
    * Coverage &lt;20x is masked with &apos;N&apos;
    * Deletions are represented by &apos;-&apos;
    * Insertions are in lowercase1
* Produces an independent de-novo assembly using `flye` and `medaka`

## Sample Report

The report contains a few useful plots to quality control your data which are described in more detail below. An example can be found [here](/workflows/wf-mpx-report.html).

### Read summary

This section contains two basic plots to show your read length distribution and
the read quality scores. These are useful for troubleshooting your experiment.

### Genome coverage

This plot shows the depth of coverage at each position along the Monkeypox virus reference you
chose to align or map read to. This plot also shows the location of:
* SNPs: grey dots
* Insertion/Deletions: blue bars

### Variant Context

It has been noted that the mutations identified in the genome appear to be in a context that
would suggest APOBEC3 host enzyme action. This plot categporises SNPs in their context to help
highlight this observation. More information can be found in [this](https://virological.org/t/initial-observations-about-putative-apobec3-deaminase-editing-driving-short-term-evolution-of-mpxv-since-2017/830) excellent post by Áine O’Toole &amp; Andrew Rambaut

### All Variants

This is simply all of the variants called by `medaka`. This is filtered only by depth (&gt;20x).

### Flye Assembly

This plot shows the contigs produced by `flye` when attemping to assemble the reads.

### Software Versions &amp; Workflow Parameters

These sections details the versions of tools used in this workflow and also the parameters at execution.

## Test Data

The git repository for `wf-mpx` includes test data provided by GSTT; Adela Medina, Luke Snell, Themis Charalampous, Rahul Batra, Jonathon Edgeworth.
This can be found at [wf-mpx/test_data/fastq/barcode01](https://github.com/epi2me-labs/wf-mpx/tree/master/test_data/fastq/barcode01).
The original source data can also be found on SRA [here](https://www.ncbi.nlm.nih.gov/Traces/study/?acc=PRJNA842892&amp;o=acc_s%3Aa).


## Useful Links

- Virological: https://virological.org
- Virological Monkeypox posts: https://virological.org/c/monkeypox/46
- Nextstrain Monkeypox: https://nextstrain.org/monkeypox?c=country&amp;p=grid
- Monkeypox background: https://www.gov.uk/guidance/monkeypox
</content:encoded><content:thumbnail>https://labs.epi2me.io/static/ba3161227865caebdbeca546d6284ee2/AdobeStock_506172741.jpeg</content:thumbnail></item><item><title><![CDATA[EPI2ME Labs 22.06.01 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-22.06.01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-22.06.01-release</guid><pubDate>Wed, 01 Jun 2022 00:00:00 GMT</pubDate><content:encoded>
Dear Nanopore Community,

We are pleased to release a new infectious diseases workflow and a collection of
minor updates to our EPI2ME Labs product.

Release of a new workflow, [wf-mpx](https://github.com/epi2me-labs/wf-mpx), with initial version v0.0.1 for the basic
analysis of sequence data prepared from samples containing monkeypox virus. The
workflow is now available through the EPI2ME Labs application. An article
describing the workflow, and providing some recommendations for how the mapped
sequence data might best be considered, can be found in a separate 
[Monkeypox Analysis](/basic-monkeypox-workflow) post.



![monkeypox-coverage](./bokeh_plot.png &quot;Figure 1. Sequence coverage across the Monkeypox genome. Bars and points at the
bottom of the plot represent the location of INDELS and SNPS respectively.&quot;)

* The [wf-artic](https://www.github.com/epi2me-labs/wf-artic) software for analysis of SARS-CoV-2 sequence data has been updated
to v0.3.16. This update includes an updated primer scheme for the NEB VarSkip
V2b design. There is also a change that allows for the passing of additional
parameters to the Pangolin software component - further details may be found in
the project&apos;s [CHANGELOG](https://github.com/epi2me-labs/wf-artic/blob/master/CHANGELOG.md).
* Several of the tutorials included in EPI2ME Labs have
been deprecated. Tutorials are intended to introduce data analysis concepts but
the Jupyter notebook is not an ideal vehicle for the production analysis of
sequence data. For production analysis of Nanopore sequence data we recommend
only our Nextflow-based workflows - these are simpler to run, have richer
analysis reports and are more actively maintained and developed. A separate [post](https://labs.epi2me.io/tutorial_trimming)
explains more about the rationale behind this decision and
lists the tutorials that have been deprecated.
* The EPI2ME Labs file browser has
been updated and simplifies connecting your sequence data to a workflow. A
collection of minor fixes have also been included in the update. These changes
have been implemented in the epi2melabs-notebook v1.1.21 container.
* Our [wf-isoforms](https://www.github.com/epi2me-labs/) workflow (v0.1.3) has been updated to use the latest version of
[pychopper](https://github.com/epi2me-labs/pychopper/) (v2.7.0) that now supports the SQK-PCS111 kit. Please see the
[CHANGELOG](https://github.com/epi2me-labs/wf-isoforms/blob/master/CHANGELOG.md) for additional information.


We welcome any feedback and would love to hear recommendations for workflow or
interface improvements. 
</content:encoded><content:thumbnail/></item><item><title><![CDATA[Deprecation of several EPI2ME Labs tutorials]]></title><description><![CDATA[Announcing that several EPI2ME Labs tutorials are being deprecated.]]></description><link>https://labs.epi2me.io/tutorial-deprecation</link><guid isPermaLink="false">https://labs.epi2me.io/tutorial-deprecation</guid><pubDate>Wed, 01 Jun 2022 00:00:00 GMT</pubDate><content:encoded>
In the coming weeks the EPI2ME Labs team will be rejigging our tutorial and workflow offerings to remove duplication 
and provide the best bioinformatics experience possible. 


EPI2ME Labs provides ONT customer with two main offerings that help them get up to speed with long read data analysis:
* Jupyterlab-based [notebook tutorials](/nbindex), which are a great teaching tool for walking a user through concepts 
while at the same time allowing the analysis of small amounts of real data.
* [Nextflow workflows](/wfindex) that can process large amounts of real world
data as described [here](epi2me-labs-embraces-nextflow/).

Over time, some of the tutorials have become a bit stale and now contain outdated methodology. Others have been 
identified as being more suited to being implemented as a workflow. These tutorials have been removed from EPI2ME 
Labs. Please let us know if you rely on any of these tutorials, and we can point you to relevant alternative resources.

The remaining tutorials can still be found [here](/nbindex).

## Deprecated tutorials

The following are being removed from EPI2ME Labs:

* **Structural Variant Calling** - our [wf-human-sv](https://github.com/epi2me-labs/wf-human-sv) performs the same task
  in a more streamlined fashion 
* **Human Variant Calling with Medaka** - this tutorial used an now deprecated methodology, users should instead use
  the [wf-human-snp](https://github.com/epi2me-labs/wf-human-snp) workflow. 
* **Benchmarking GM24385 Small Variant Calling** - this short tutorial introduced the [hap.py](https://github.com/Illumina/hap.py) tool for 
  benchmarking small variants. We feel the tutorial did not add much over the documentation of the tool and
  the tool is now an option in wf-human-snp.
* **Cas9 Targeted Sequencing Tutorial** - please see [wf-cas9](https://github.com/epi2me-labs/wf-cas9)
* **ERCC Workflow** - see [wf-alignemnt](https://github.com/epi2me-labs/wf-alignment),
  which is able to compare expected to observed counts of spike-in controls.
* **Introduction to Pychopper** - see the instructions for using Pychopper [here](https://github.com/epi2me-labs/pychopper),
  pychopper is a preprocessing tool used within our wf-transcriptomes workflow.
* **Isoform Tutorial** - see [wf-transcriptomes](https://github.com/epi2me-labs/wf-transcriptomes) for a more fullsome
  and automated cDNA/RNA preprocessing and classification workflow.
* **Metagenomic classification tutorial** - please see [wf-metagenomics](https://github.com/epi2me-labs/wf-metagenomics)
* **Viral and Bacterial Variant Calling** - please see the workflow [wf-bacterial-genomes](https://github.com/epi2me-labs/wf-bacterial-genomes)

We are always keen to here feedback from our users. Were you using this notebooks? What aspects of them were useful to
you? Would you like to see them make a return. Get in touch through the [contact](/contactus) page.</content:encoded><content:thumbnail/></item><item><title><![CDATA[CliveOME 5mC dataset]]></title><description><![CDATA[5mC basecall dataset from Nanopore's CTO, including cfDNA and .]]></description><link>https://labs.epi2me.io/cliveome_5mc_cfdna_celldna</link><guid isPermaLink="false">https://labs.epi2me.io/cliveome_5mc_cfdna_celldna</guid><pubDate>Tue, 31 May 2022 12:30:00 GMT</pubDate><content:encoded>
import cliveome2206length from &quot;./ulk_length_cliveome.jsx&quot;
import cliveome2206accuracy from &quot;./accuracy_cliveome.jsx&quot;

Following on from our [CliveOME 2022.05](/cliveome_cfdna) data release we are
now excited to present 5mC basecalls for both the original cfDNA reads and reads
from a second ultralong cellular DNA sample preparation.

Aligned reads are available in BAM format with modified base tags as defined in
the [hts-specs](https://samtools.github.io/hts-specs/SAMtags.pdf) document. The
cfDNA basecalls are available at (see below for more details):

```
s3://ont-open-data/cliveome_kit14_2022.05/cfdna/basecalls/bonito_mod
```

with three flowcells of cellular WGS data available at:

```
s3://ont-open-data/cliveome_kit14_2022.05/gdna/basecalling/
```

Figure 1. shows the data output for one of the three ULK flowcells as a function
of read length. This ULK sample preparation has yielded a total of 70 gigabases
of data with over 20 gigabases of data contained with reads over 100kb.  Figure
2. presents the read accuracy as measured by alignment to the GRCh38 reference
sequence for both DNA samples. Note that the cfDNA distribution here appears
rather broad as explained [previously](/cliveome_cfdna): the single-read base
accuracy density is broadened by the appearance of 1 or 2 errors in reads of
length ~100 bases.


&lt;Bokeh plotJson={cliveome2206length} plotName=&quot;CliveOME Ultralong Read Lengths&quot;
  plotCaption=&quot;Figure 1. Cumulative data volume for one of the three flowcells used to sequence the ULK cellular DNA sample. This flowcell produced 20Gbases of reads greater than 100 kbase in length.&quot; /&gt;


&lt;Bokeh plotJson={cliveome2206accuracy} plotName=&quot;CliveOME Read Accuracies&quot;
  plotCaption=&quot;Figure 2. Kernel density estimate depicting the read accuracy for short fragment mode cfDNA sequencing as well as long-read cellular DNA data.&quot; /&gt;

### Data Availability

The FAST5 files from the sequencing run have been placed within our Amazon S3
bucket publicly available at:

    s3://ont-open-data/cliveome_kit14_2022.05/
    
More information on downloading the data from `s3://ont-open-data` may be found
on our [Open datasets Tutorials](/tutorials) page.

### Sample extraction

The cellular DNA sample was prepared for sequencing using Oxford Nanopore&apos;s
Ultra-Long DNA Sequencing Kit, the details of which can be found on the [ONT
Store](https://store.nanoporetech.com/productDetail/?id=ultra-long-dna-sequencing-kit).

Details of the cfDNA sample preparation can be found on the [previous
post](/cliveome_cfdna).

&gt; The samples taken for DNA extractions of cfDNA and cellular-DNA were not
contemporaneous.

### Data processing.

In both cases, cfDNA and cellular-DNA,
[bonito](https://www.github.com/nanoporetech/bonito) was used to perform
basecalling straight to BAM files with modified base tags. Bonito was chosen
over Guppy because at the time of writing bonito implements a slightly more
accurate algorithm for 5mC calling which is thought to help particularly in the
case of short fragment mode.

&gt; **Ordinarily users should use Guppy for obtaining
&gt; 5mC calls, which can be performed in real-time on the sequencing instrument
&gt; to further lower the barrier to obtaining such data.**

This extremely simple workflow is in contrast to the laborious sample
preparation and data processing required for techniques such as bisulfite
sequencing. We previously discussed these differences in our [5mC
GM24385](/gm24385-5mc) blog post. To recap, all that is required to obtain 5mC
calls from the primary sequencing data is to run:

```
bonito basecaller \
    dna_r10.4.1_e8.2_sup@v3.5.1 \
    &lt;input location&gt;
    --recursive \
    --modified-bases 5mC \
    --reference &lt;reference fasta&gt; \
    | samtools sort -@16 \
    &gt; bonito_calls.bam
samtools index bonito_calls.bam
```

Aggregation of 5mC information by genomic position can be performed by our
[modbam2bed](https://www.github.com/epi2me-labs/modbam2bed) program:

```
modbam2bed \
    -e -m 5mC --cpg -t 10 \
    &lt;reference fasta&gt; bonito_calls.bam \
    &gt; bonito.cpg.bed
```

to obtain per-site methylation frequencies in 

The `modbam2bed` program can accept a BAM file with additional tags specifiying
the haplotype to which each read belongs. In this manner it is possible to
simply acquire haplotype specific methylation frequencies for CpG sites in human
samples, greatly accelerating research into phenomena controlled by genetic
imprinting.  We will leave these tasks for another blog post, in the meanwhile
please do download and explore the dataset.</content:encoded><content:thumbnail>https://labs.epi2me.io/static/b4203e35095cfe8040afa657bb78a771/sample-tubes.jpg</content:thumbnail></item><item><title><![CDATA[Data access within EPI2ME Labs]]></title><description><![CDATA[A simple guide to setting up the data folder within EPI2ME Labs.]]></description><link>https://labs.epi2me.io/mount-folder-labslauncher</link><guid isPermaLink="false">https://labs.epi2me.io/mount-folder-labslauncher</guid><pubDate>Mon, 23 May 2022 00:00:00 GMT</pubDate><content:encoded> 

The `Data Mount` folder is the location within notebooks tutorials available for reading and writing data. 
It provides an isolation from the rest of the file system, ensuring that any coding foibles
do not result in the accidental interference with files stored elsewhere on your computer. It also serves as output folder for 
workflows (workflow inputs can be located anywhere on your computer).

By default, the `Data Mount` directory is set to the home directory. In order to set a different `Data Mount` directory, 
just follow these simple steps:

1) Open the settings dialog
![Open the settings window](./settings.png &quot;The setting icon.&quot;)

2) Browse to the folder you would like to set as your `Data Mount` and press OK.
![Browse to a directory](./browse.png &quot;Selected folder.&quot;)

3) The EPI2ME Labs environment will restart for the changes to take effect.

![Restart](./dialog.png &quot;Restart dialog.&quot;)
 

After the EPI2ME Labs environment has restarted, you will see a new folder within the `Data Mount` folder you specified called `epi2melabs-data`.
This is where the output data from tutorials and workflows will be saved. It is also where you must place any data files you wish to be available within the tutorials.

![directory](./directory.png &quot;Contents of the new data directory with a tutorial and workflow output.&quot;)

To enable notebook tutorials to access your input data, move it to `epi2melabs-data` and it will become visible within the
notebook interface file browser.
![Labs browser](./data_mount_in_launcher.png &quot;Contents of Data Mount folder in the EPI2ME Labs browser.&quot;)


### Summary
This small guide has shown you how to set up the `Data Mount` folder in EPI2ME Labs. The option is used to expose
only a specified location on your computer to the notebook environment to ensure any coding experiments you
undertake cannot make unwanted changes to important parts of your system. Experiment away!
</content:encoded><content:thumbnail/></item><item><title><![CDATA[CliveOME cfDNA dataset]]></title><description><![CDATA[Cell free DNA (cfDNA) dataset from Nanopore's CTO.]]></description><link>https://labs.epi2me.io/cliveome_cfdna</link><guid isPermaLink="false">https://labs.epi2me.io/cliveome_cfdna</guid><pubDate>Wed, 18 May 2022 17:30:00 GMT</pubDate><content:encoded>
import readLengthPlotcfDNA from &quot;./read_length.jsx&quot;
import accuracyPlotcfDNA from &quot;./accuracy.jsx&quot;

We are pleased to release a cell free DNA (cfDNA) dataset to our `s3://ont-open-data` resource.

Normal blood plasma contains background amounts of short degraded extracellular DNA - these DNA fragments typically range from 50bp to 200bp in length. The DNA is of cellular origin but has been released into the blood during cell lysis. The fragments may be either of nuclear or mitochondrial origin and are collectively referred to as cfDNA. Much of the cfDNA circulates still packaged with nucleosomes; 147bp of DNA are wound around the nucleosome.

The concentration of cfDNA fragments correlate both with age and disease. Cancer patients often have elevated cfDNA levels that also reflect the mutations that have been acquired within a tumour&apos;s genome. This observation has enabled the techniques of liquid biopsy. Isolated cfDNA may be used to non-invasively screen for genetic biomarkers associated with cancer types and stages.

Our recent updates to enable short-fragment sequencing on Nanopore devices open exciting new horizons for cfDNA sequencing. This cfDNA dataset release has been prepared from a blood sample provided by our CTO, Clive Brown. The cfDNA was isolated from 7 ml fresh plasma using then QIAGEN ccfDNA Midi Kit. The manufacturer&apos;s instructions were followed. QC was performed using both the Agilent Bioanalyzer and the Qubit dsDNA high sensitivity assay. 30 ng of cfDNA was used to prepare sequencing libraries using the new SQK-LSK114 kit. The end prep SPRI concentrations were increased by 3x.

&lt;Bokeh plotJson={readLengthPlotcfDNA} plotName=&quot;cfDNA Read Lengths&quot;
  plotCaption=&quot;Kernel density estimate depicting the read length distribution for short fragment mode cfDNA sequencing.&quot; /&gt;


&lt;Bokeh plotJson={accuracyPlotcfDNA} plotName=&quot;cfDNA Read Accuracies&quot;
  plotCaption=&quot;Kernel density estimate depicting the read accuracy for short fragment mode cfDNA sequencing. Peaks in to plot correspond to 0, 1, 2, etc. errors per read.&quot; /&gt;

The FAST5 files from the sequencing run have been placed within our Amazon S3 bucket publicly available at:

    s3://ont-open-data/cliveome_kit14_2022.05/cfdna.
    
More information on downloading the data from `s3://ont-open-data` may be found on our [Open datasets Tutorials](/tutorials) page.

We aim to enhance this dataset in the coming days with a standard genomic DNA dataset, and provide 5mC basecalls.

We hope that you have some interesting explorations within this dataset.

</content:encoded><content:thumbnail>https://labs.epi2me.io/static/7d9cb78eb078cac55058564e4159bbfe/human-dna-bg.jpeg</content:thumbnail></item><item><title><![CDATA[EPI2ME Labs Quick Start]]></title><description><![CDATA[A guide illustrating the use of EPI2ME Labs.]]></description><link>https://labs.epi2me.io/labsquickstart</link><guid isPermaLink="false">https://labs.epi2me.io/labsquickstart</guid><pubDate>Tue, 17 May 2022 00:00:00 GMT</pubDate><content:encoded>
EPI2ME Labs extends the JupyterLab notebook framework with a pre-configured analysis environment and the ability to run Nextflow workflows all in a single
desktop application. Our Nextflow workflows can be used to routine analyses,
while our notebooks provide an introduction to nanopore sequencing bioinformatics
and provide next steps for a selection of our workflows.


## EPI2ME Labs

The first time EPI2ME Labs is started the environment components must be
downloaded. This will not happen on subsequent server starts. After the components
have been downloaded the Lab environment will start.

## Running a Workflow

1. On the landing page select the workflow you would like to open.
   ![Workflow launch](./workflows-01.png &quot;Launch a workflow&quot;)
2. Scroll through the form and fill out required fields, required fields will be indicated with a red cross, which will turn to a green tick when filled out correctly. Click Launch workflow.
   ![Workflow launch](./workflows-02.png &quot;Launch a workflow&quot;)
3. The workflow should start running and give an indication it is has launched and when complete it should provide output files.
   ![Workflow launch](./workflows-03.png &quot;Launch a workflow&quot;)
4. Click on instance details for further information about the run and for an option to re-run.

## Running a Tutorial

1. On the landing page select notebooks in the top right corner and select the notebook template you wish to open. The template will create a copy of the file as a new document for you to edit.
   ![Notebook launch](./start-notebooks-01.png &quot;Launch a notebook&quot;)
3. You can see all your data in the location you provided in settings by 
clicking **/epi2melabs** in the sidebar:
   ![epi2melabs mount](./epi2melabsmount.png &quot;Data access from epi2melabs folder&quot;)
4. To copy your own data into the tutorial, right-click on the data file and
select **Copy path**.
![Copy filepaths](./copypath.png &quot;Copying filepaths&quot;)
5. Follow the guidance provided in the tutorial to perform data analysis.

### Running code cells

Running code cells requires clicking the **play** button in the
navigation bar at the top of the screen (or to the right-hand side of a code cell).
There is also a **&quot;Run&quot;** tab that can be used to run multiple cells.

![Run code cells](./runcodecell.png &quot;Running code cells&quot;)

### Bookmarks and navigating notebooks

The environment provides a table of contents menu in the left-hand
sidebar. These navigation prompts can also be used to *fold* content visibility
in the notebook.

![Bookmarks](./bookmarks.png &quot;Notebook Bookmarks&quot;)

## Updating the Environment

The EPI2ME Labs application automatically detects when updates are
available to the notebok server. Oxford Nanopore Technologies&apos; may
occassionally provide updates to provide new features or improved performance.

When an update is available attempting to start or restart the notebook server
when an update is available will result in the following being displayed:

![Update available](./updateavailable.png &quot;Server update available&quot;)

To update the server or launcher select update from the setting menu (bottom left hand cog), The new components will be downloaded:

![Update download](./downloadupdate.png &quot;Server updating&quot;)

## Application settings

To access the application settings click the bottom left hand cog and click **Settings**. 

![option screen](./optionscreen.png &quot;The EPI2ME Labs application required minimal configuration. Users are advised to change the Security Token setting to a unique value.&quot;)

The **Data Mount** option controls the path where data from Nextflow workflows
and Jupyter notebooks is save.

We strongly encourage updating the security token to a unique value. The token
provides an authentication method that allows only trusted users access to the
EPI2ME Labs server and is used as an additional security layer to help prevent
other users on the network from running code and accessing data. This token
will be used later in the workflow to connect Google Colaboratory to the
server.

Changing these settings will cause the Lab environment to restart.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[London is Calling EPI2ME Labs]]></title><description><![CDATA[How to get your EPI2ME Labs questions answered at London Calling 2022.]]></description><link>https://labs.epi2me.io/london-calling-2022</link><guid isPermaLink="false">https://labs.epi2me.io/london-calling-2022</guid><pubDate>Thu, 12 May 2022 00:00:00 GMT</pubDate><content:encoded>
London Calling 2022 is now only a week away and we wanted to give you some pointers on how you can get in touch with EPI2ME Labs team both online and in person in the Live Lounge. Some of the team are very excited to be attending their 1st conference in person since before the start of the pandemic and we have our shiny new local bioinformatics platform, &quot;EPI2ME Labs&quot; to show you all. If you&apos;re not attending in person then the team will be available online for the duration of the conference and can be reached through the LC2022 platform.

More information about London Calling 2022, how to register, and the jam packed agenda can be found on the conference website: https://londoncallingconf.co.uk/lc22. Please take a look!

## The Team

### Chris Wright

* _In Person_: Thursday, Friday

Our esteemed leader, talk to him about anything Oxford Nanopore, he&apos;s been around for 10 years and has a wealth of knowledge about the platform and how to get the best out of your Oxford Nanopore data analysis.

### Stephen Rudd

* _In Person_: Thursday, Friday

Often found somewhere in the woods poking bears, he&apos;s taking a break and will be in London for both Thursday and Friday. Stephen is your go to for any bioinformatics related enquiries.

### Tom Rich

* _Online_: Wednesday, Thursday, Friday

Want to know why we chose Nextflow? Or what the internal name for EPI2ME Labs is? Talk to Tom. He&apos;s been working with Chris on development of the EPI2ME Labs interface. If you have questions on use or suggestions for improvements don&apos;t hesitate to contact him online.

### Sarah Griffiths

* _Online_: Wednesday, Thursday, Friday

Love plasmid/construct sequencing? So does Sarah! Talk to her about how you can replace Sanger sequencing with Oxford Nanopore sequencing and get full length assemblies of your plasmids with wf-clone-validation. Sarah is also our Windows expert, so if you&apos;re having trouble with EPI2ME Labs and Windows, Sarah will be able to help!

### Matt Parker

* _Online_: Wednesday, Friday
* _In Person_: Thursday

Come talk to me about your SARS-CoV-2 sequencing workflows, clinical bioinformatics workflows and other general workflow related matters.   

### Neil Horner

* _Online_: Wednesday, Thursday, Friday

Got RNA data? Talk to Neil. He&apos;s been developing some of our RNA sequencing workflows. He can also help you with general workflow related questions.

### Sam Nicholls

* _Online_: Wednesday, Thursday
* _In Person_: Friday

Of COG-UK fame, Sam joins us hot off the SARS-CoV-2 sequencing production line. You can talk to him about all things workflow related, and how he&apos;s getting on with the transition to macOS.
</content:encoded><content:thumbnail>https://labs.epi2me.io/static/ab62278e0c878056ad735c92573dfab6/AdobeStock_85090086.jpeg</content:thumbnail></item><item><title><![CDATA[EPI2ME Labs 22.05.01 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-22.05.01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-22.05.01-release</guid><pubDate>Wed, 04 May 2022 00:00:00 GMT</pubDate><content:encoded>
Dear Nanopore Community,

We are pleased to release a new version of the EPI2ME Labs software that
introduces an updated and simplified user experience. EPI2ME Labs is now a
standalone application that continues to provide a clean interface to accessing
bioinformatics workflows and tutorials for the analysis of nanopore sequence
data. This release also brings our workflow capabilities to users working from
Windows computers.

![epi2melabs-interface](./fig1.png &quot;Figure 1. The new version of EPI2ME Labs has an updated and simplified
graphical interface. The software brings our workflow capabilities to our
Windows users and the analytical focus is now targeted towards the
bioinformatics workflows.&quot;) 

There are a number of key changes within the updated EPI2ME Labs software that
are summarised below:

The EPI2ME Labs Launcher software has been deprecated - the new EPI2ME Labs
software automatically orchestrates the download, start and stop of the
required Docker containers.  The EPI2ME Labs software functions as a standalone
application and now delivers the Jupyter interface itself.  The EPI2ME Labs
software is focusing on our collection of [Nextflow](https://nextflow.io/) based workflows. These have
been improved with additional in-application documentation to better describe
the workflows, their parameters, and configuration.  Updates to the workflow
integration make it simpler to re-run workflows that have been pre-configured
with preferred parameters, databases, or reference genomes.  The command-line
access to the software has been maintained and for advanced users, the ability
to start multiple sessions on a server remains possible.  The installer
software has been updated to simplify the installation for Windows users - the
installer scripts now orchestrate the installation of most dependencies (WSL2,
an Ubuntu VM, Nextflow and its Java dependencies). The only unmanaged software dependency
is now Docker, which can be downloaded as [Docker Desktop](https://www.docker.com/products/docker-desktop/).


![epi2me-labs-workflows](./fig2.png &quot;Figure 2. The workflow functionality in EPI2ME Labs has been enriched and
additional run metadata and logging is now presented. The &quot;Rerun workflow&quot;
button shown in the screenshot allows an analysis to be re-run; preferred
parameters and e.g. reference genomes are copied. This encourages
reproducibility and simplifies the analysis of data from larger studies.&quot;)


The EPI2ME Labs software is available from our [Downloads](/downloads) page.
Additional information is provided in the project&apos;s CHANGELOG.

&gt; It is recommended that a previous installation of the EPI2ME Labs Launcher is first
&gt; removed - the new EPI2ME Labs software cannot
&gt; uninstall the now deprecated EPI2ME Labs Launcher software.


### New workflow releases


The EPI2ME Labs release also includes updates to some of our workflows.
The template for our Nextflow based workflows can be found at [wf-template](https://www.github.com/wf-template).

Notably [wf-tb-amr](https://www.github.com/wf-tb-amr) has been updated to version v1.0.0. This version now uses the
bcftools mpileup software for variant calling and uses a larger collection of
WHO defined variants in antibiotic resistance association. Please see the
changelog for additional information on the changes.  wf-metagenomics has been
updated to version v1.1.3. This update includes in-application documentation
improvements required for functionality with the EPI2ME Labs software.


We look forwards to any feedback and would welcome recommendations for new
workflows, tutorials, and features.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[SARS-CoV-2 Midnight Scheme Update]]></title><description><![CDATA[Description of the new scheme naming and structure for the wf-artic workflow.]]></description><link>https://labs.epi2me.io/ont-midnight-scheme-update</link><guid isPermaLink="false">https://labs.epi2me.io/ont-midnight-scheme-update</guid><pubDate>Fri, 15 Apr 2022 00:00:00 GMT</pubDate><content:encoded>
## Introduction

This blog post will explain the changes we are making to the primer scheme naming in `wf-artic` due to underlying changes in the ONT Midnight SARS-CoV-2 sequencing kit. We will also describe a new feature of `wf-artic` allowing specification of custom primer schemes.

### SARS-CoV-2 Genome Sequencing by Tiled Amplicon

Tiled amplicon sequencing allows us to cover small genomes with short slightly overlapping PCR amplicons. This approach has a number of advantages, possibly the most important being that we can still recover full or nearly full genomes from samples with a low viral load or degraded RNA.

To achieve this tiled amplicon sequencing approach, pools of primer pairs are used to create the amplicons that cover the whole of the genome. For SARS-CoV-2 we analyse this data using the Network Artic FieldBioinformatics tool using `wf-artic`. This tool requires a bed file of the primer locations used in the assay. These primer &quot;schemes&quot; provide the start and end position of each primer but do not include the primer sequences as these are not required by the workflow.

The primer scheme bed files are required for a couple of reasons:
* Determining which amplicon a read has been generated from
* Trimming of primer sequences from the ends of reads to prevent issues during data analysis

Previously the `wf-artic` workflow included a single Midnight primer scheme called `V1200`. Previous additions of primers to ONT Midnight have been sequence only changes and have therefore required no changes to the primer scheme. Version 3 of the ONT Midnight kit adds a new primer to tackle the drop out of amplicon 21 as a result of a 9bp deletion in this region in BA.2 samples. This requires a new scheme. We are therefore, after consulting members of the community, renaming our Midnight schemes to be in line with our physical kit versions.

## New schemes

We are making it clearer in the `wf-artic` repository which scheme should be used with each physical assay in addition to making versioning for Midnight schemes more transparent.

The schemes now included in `wf-artic` are:

* ARTIC: Original Network Artic primer scheme of 400bp amplicons (Network ARTIC &amp; Josh Quick &amp; Nick Loman)
* Midnight-IDT: This represents the assay produced by IDT and designed by the community (Nikki Freed &amp; Olin Salander) including the recent additions and changes to deal with new variants.
* Midnight-ONT: This represents the assay originally designed by the community (Nikki Freed &amp; Olin Salander) and produced by ONT, but with primer additions and improvements to deal with recent variants of SARS-CoV-2 that differ from those contained in Midnight-IDT.
* NEB-VarSkip: No changes here other than a clearer folder structure.

So when running ARTIC or `wf-artic` we specify the version as `&lt;ASSAY&gt;/&lt;VERSION&gt;` where assay could be, for example `Midnight-ONT` and the version could be `V2`, this would be written as `Midnight-ONT/V2`

The rule of thumb for Midnight kits bought from ONT is that **the kit version will always pair with the scheme version** e.g. `EXP-MRT001.30` should be paired with primer scheme `Midnight-ONT/V3`. If you use the V2 Midnight primers from IDT then you would use `Midnight-IDT/V2` as the scheme name.

### ONT Midnight kit version 3 primer additions

The table shows the *additional* (i.e. original V1&amp;V2 primers remain in the mix) primers added to Midnight-ONT kit V3 to address BA.2 amplicon 21 drop-out and provide general performance improvements.

| Primer | Start V1&amp;V2     | End V1&amp;V2               | Sequence V1&amp;V2 | Start V3 | End V3 | Sequence V3 |
|--------|-----------|-------------------|----------|-------|-----|----------|
| 21R    | 21621     | 21642             | CCAGAACTCAATTACCCCCTGC | 21615 | 21645 | CTTACAACCAGAACTCAATCATACAC |
| 22R    | 22591     | 22612             | CGCCACCAGATTTGCATCTGTT | 22591 | 22612 | CGCCACCAaATTTGCATCTGTT |
| 23L    | 22512     | 22537             | ACTTTAGAGTCCAACCAACAGAATCT | 22512 | 22537 | ACTTTAGAGTtCAACCAACAGAATCT |
| 24L    | 23519     | 23544             | GCTGAACATGTCAACAACTCATATGA | 23519 | 23544 | GCTGAAtATGTCAACAACTCATATGA |

While the additional 22R, 23L and 24L primers represent single base changes (lowercase), 21R has been redesigned to deal with a 9bp deletion found in the primer region in BA.2.

![Primer 21R](./primer-21R.png &quot;Figure 1 - Primer 21R additional BA.2 primer.&quot;)

The improvements are clear on a BA.2 sample of ct 20 sequenced using Midnight V2 and Midnight V3:
![Midnight-ONT Version 3 Improvements](./improvements-to-coverage.png &quot;Figure 2 - Coverage using Midnight V2 and V3 on a BA.2 sample of ct 20.&quot;)

To account for the addition of a primer 21R with a different position we decided to use a scheme containing a single entry for 21R but expand this primer region to cover the whole of the additional new 21R and the original 21R primer. If you use an older version of the scheme it could affect your analysis, 7 bases that could result from primers might be included in your final consensus so you should update your workflows.

![Trimming at 21R](./trimming.png &quot;Figure 3 - The result of using an old scheme with V3 primers.&quot;)

It is advisable to update to the latest version of `wf-artic` when you start using the new kit version.

### Folder structure

The growing number of supported schemes mean that we needed to reorganise the `primer_schemes` folder. The new structure is shown below. Briefly, under each scheme name of `SARS-CoV-2` or `spike-seq` a folder exists that separates the schemes into the main assays, be that `ARTIC`, `Midnight-ONT`, or the newly added `Midnight-IDT`. Within each of these folders are the primer versions available for that assay. The scheme name therefore remains `SARS-CoV-2`, but the version now includes the assay and version of that assay, e.g `Midnight-ONT/V3`.

```
data/primer_schemes
├── SARS-CoV-2
│   ├── ARTIC
│   │   ├── V1
│   │   │   ├── SARS-CoV-2.reference.fasta
│   │   │   └── SARS-CoV-2.scheme.bed
│   │   ├── V2
│   │   │   ├── SARS-CoV-2.reference.fasta
│   │   │   └── SARS-CoV-2.scheme.bed
│   │   ├── V3
│   │   │   ├── SARS-CoV-2.reference.fasta
│   │   │   └── SARS-CoV-2.scheme.bed
│   │   ├── V4
│   │   │   ├── SARS-CoV-2.reference.fasta
│   │   │   └── SARS-CoV-2.scheme.bed
│   │   └── V4.1
│   │       ├── SARS-CoV-2.reference.fasta
│   │       └── SARS-CoV-2.scheme.bed
│   ├── Midnight-IDT
│   │   └── V1
│   │       ├── SARS-CoV-2.reference.fasta
│   │       └── SARS-CoV-2.scheme.bed
│   ├── Midnight-ONT
│   │   ├── V1
│   │   │   ├── SARS-CoV-2.reference.fasta
│   │   │   └── SARS-CoV-2.scheme.bed
│   │   ├── V2
│   │   │   ├── SARS-CoV-2.reference.fasta
│   │   │   └── SARS-CoV-2.scheme.bed
│   │   └── V3
│   │       ├── SARS-CoV-2.reference.fasta
│   │       └── SARS-CoV-2.scheme.bed
│   └── NEB-VarSkip
│       ├── v1a
│       │   ├── SARS-CoV-2.reference.fasta
│       │   └── SARS-CoV-2.scheme.bed
│       ├── v1a-long
│       │   ├── SARS-CoV-2.reference.fasta
│       │   └── SARS-CoV-2.scheme.bed
│       └── v2
│           ├── SARS-CoV-2.reference.fasta
│           └── SARS-CoV-2.scheme.bed
└── spike-seq
    └── ONT
        ├── V1
        │   ├── spike-seq.reference.fasta
        │   ├── spike-seq.scheme.bed
        │   └── spike-seq.vcf
        └── V4.1
            ├── spike-seq.reference.fasta
            ├── spike-seq.scheme.bed
            └── spike-seq.vcf
```

### Running wf-artic

To facilitate the existence of potentially multiple primers schemes `wf-artic` allows for the specification of the scheme at run time.

```
nextflow run epi2me-labs/wf-artic \
  --fastq &lt;PATH_TO_MIDNIGHT-ONT_V3_FASTQ&gt; \
  --scheme_name SARS-CoV-2 \
  --scheme_dir primer_schemes \
  --scheme_version MIDNIGHT-ONT/V3
```

To list the schemes that are packaged with `wf-artic` you can run:

```
nextflow run epi2me-labs/wf-artic \
  --fastq &lt;PATH_TO_FASTQ&gt; \
  --list_schemes
```

## Custom primer schemes

We understand that users may want to supply their own schemes for use with `wf-artic` analysis. To enable this we have added the command line argument `--custom_scheme`. This must be the *full path* to the directory containing your appropriately named scheme bed and fasta files; `&lt;SCHEME_NAME&gt;.bed` and `&lt;SCHEME_NAME&gt;.fasta`.

*You must specify the scheme name (`--scheme_name`) and the min (`--min_len`) and max (`--max_len`) length parameters based on your expected amplicons for read length filtering.*

You must provide both a fasta of your genome and a primer bed file detailing your primers, please use the ones included in `wf-artic` as examples; These must be readable and contained within the directory you specify for `--custom-scheme`. We do not validate your bed or fasta file so please ensure these are in the correct format.
</content:encoded><content:thumbnail>https://labs.epi2me.io/static/0be72d91de3e75aabbac10d36291da9e/AdobeStock_322667262.jpeg</content:thumbnail></item><item><title><![CDATA[EPI2ME Labs 22.03.01 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-22.03.01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-22.03.01-release</guid><pubDate>Wed, 09 Mar 2022 00:00:00 GMT</pubDate><content:encoded>
Dear Nanopore Community

This week the EPI2ME Labs team is delighted to release a new workflow,
wf-bacterial-genomes. We have also released minor updates to wf-artic,
wf-alignment and wf-isoforms.

* [wf-bacterial-genomes](https://github.com/epi2me-labs/wf-bacterial-genomes) is a Nextflow workflow implemented to simplify bacterial
genome assembly. The analysis pipeline performs a quality review on the starting
sequence collection prior to assembling the provided sequencing data using [Flye](https://github.com/fenderglass/Flye).
The consensus sequence is polished using [Medaka](https://github.com/nanoporetech/medaka) and some basic annotation is
performed using [Prokka](https://github.com/tseemann/prokka). If a reference genome is provided, the workflow will map
sequence reads to this reference using minimap2 and will call variants using
medaka variant. This workflow replaces our earlier wf-hap-snp product that
performed variant calling in haploid genomes.
* [wf-artic](https://github.com/epi2me-labs/wf-artic) for the analysis of
SARS-CoV-2 sequencing data has been updated to version v0.3.12. This release
includes an update to the included version of [Nextclade](https://clades.nextstrain.org/) (1.10.3) and
improvements to the Nextclade visualisation included in the HTML report. Please
note that the workflow now uses the r941_min_hac_variant_g507 Medaka model by
default. The corresponding EPI2ME workflow, Fastq QC + ARTIC + NextClade, also
includes these updates and is available as version v2022.03.08. Please see the
product [changelog](https://github.com/epi2me-labs/wf-artic/blob/master/CHANGELOG.md) for additional details.


![nextclade](./nextclade.png &quot;Figure 1. The Nextclade SARS-CoV-2 visualisation in the EPI2ME and EPI2ME Labs
Workflow has been updated. The example panel shown at the top of the figure
lists the genetic variants observed between a  SARS-CoV-2 genome&apos;s consensus
sequence and the reference. The SNPs observed are ranked as nucleotide variants,
non synonymous peptide variants and private mutations described only in this
sample. This panel is presented when a user performs a mouse-over of the
variants column in the nextclade report.&quot;)

* [wf-alignment](https://github.com/epi2me-labs/wf-alignment), our workflow for mapping sequence reads to a reference genome, has
been updated to version v0.1.1. This minor update introduces a collection of
performance changes - for more details please review the workflow [changelog](https://github.com/epi2me-labs/wf-alignment/blob/master/CHANGELOG.md).
* [wf-isoforms](https://github.com/epi2me-labs/wf-isoforms), for analysis of gene isoforms from cDNA sequencing data, has been
updated to version v0.1.1. This update fixes a couple of issues associated with
transcript counting and the de novo approach to identifying genes and their
isoforms. More information on these updates is provided in the product&apos;s
[changelog](https://github.com/epi2me-labs/wf-isoforms/blob/master/CHANGELOG.md). 
* Our metagenomic classification tutorial has been fixed. The bundled
Centrifuge software can now find the required indices.

We would welcome any recommendations for future workflows, tutorials and dataset
releases.</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs 22.02.01 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-22.02.01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-22.02.01-release</guid><pubDate>Wed, 09 Feb 2022 00:00:00 GMT</pubDate><content:encoded>
Dear Nanopore Community,

We are pleased to introduce an update to our EPI2ME Labs product that now
includes the functionality to launch our Nextflow pipelines directly from the
EPI2ME Labs session. These new capabilities have been implemented to facilitate
the launching of integrated workflows in an efficient manner whilst avoiding
the requirement for using a terminal session. 

### Updated EPI2ME Labs interface

EPI2ME Labs has been updated to provide access to both tutorials and workflows
through the JupyterLab interface. The inclusion of workflows within EPI2ME
Labs (Figure 1) now facilitates the launch and execution of our Nextflow
workflows through a graphical user interface (Figure 2) - using the command
line interface is no longer mandatory.

![workflows](./workflows.png &quot;Figure 1. EPI2ME Labs now also provides access to our collection of Nextflow
pipelines for bioinformatics data analyses. The screenshot illustrates a
collection of available workflows and the instances listed at the bottom of the
screenshot correspond to either running of complete analyses. &quot;ArabidopsisSeq&quot;
corresponds in this case to an instance of the wf-alignment workflow that has
been launched and is not yet complete.&quot;)

![workflow-options](./workflow-options.png &quot;Figure 2. The integration of our Nextflow workflows inside the EPI2ME Labs
Jupyter environment means that workflows can be specified using a convenient
web form instead of preparing a potentially scary command in the terminal. In
the screenshot above the reference genome for a sequence mapping analysis has
been defined. This interface is part of the wf-alignment workflow.&quot;)

The tutorials provided in EPI2ME Labs are not recommended for the routine or
production analyses of sequence data; for the analysis of sequence data we
recommend our workflows that are intended to be more scalable and portable
between computer systems and servers.

This first beta release is available for both macOS and Ubuntu Linux. The
product will be updated to implement functionality on computers running Windows
over the coming weeks. More details are available in our [EPI2ME Labs
Nextflow](/jupyterflow) post.


### Nextflow workflow for assessment of non-target Cas9 depletion experiments

We have released wf-cas9, a Nextflow workflow aimed to assist in the assessment
of non-target depletion from a Cas9 targeted sequencing experiment. The
workflow is based on our Cas9 Targeted Sequencing Tutorial but is intended for
more routine and expedited analyses. The logic within the analysis is unchanged
but the pyranges software has been deprecated in favour of bedtools. The
workflow is available through its [GitHub page](https://github.com/epi2me-labs/wf-cas9) and an example report is
available from the EPI2ME Labs [Workflows page](https://labs.epi2me.io/wfindex). The wf-cas9 software is
supported through our EPI2ME Labs product.

![wf-cas9](./wf-cas9.png &quot;Figure 3. An excerpt from the wf-cas9 report showing results from a
multi-target enrichment experiment. The report shows summary information that
describes the target regions and their number of on-target reads, coverage and
bias. Additional graphical data is plotted that may be used to identify targets
and their primers that could be further optimised.&quot;)


### Nextflow workflow for assessment of tuberculosis antibiotic resistance

EPI2ME Labs now provides a workflow, [wf-tb-amr](https://github.com/epi2me-labs/wf-tb-amr), that may be used for the
assessment of antibiotic resistance from Mycobacterium tuberculosis amplicon
sequencing data.  The workflow looks for genetic variants associated with
antibiotic resistance from sequence reads that have been mapped to the
reference tuberculosis reference genome. More information on the workflow may
be found from its [GitHub page](https://github.com/epi2me-labs/wf-tb-amr).


### SARS-CoV-2 analysis pipeline, wf-artic, updated to version v0.3.11

wf-artic, our Nextflow workflow for the analysis of SARS-CoV-2 sequence data has
been updated to version v0.3.11. This update includes a correction to the NEB
VarSkip V2 bed coordinates and database updates to both the Pangolin and
NextClade components. Please see the wf-artic
[changelog](https://github.com/epi2me-labs/wf-artic/blob/master/CHANGELOG.md)
for further details.


### EPI2ME Labs maintenance

The EPI2ME Labs Launcher has been updated to fix an issue where system
usernames containing non-ASCII characters led to a failure to start the
epi2melabs-notebook container.

Please note that we will no longer provide Centos 8 RPM builds for the
LabsLauncher software. Centos 8 has [reached end of life](https://www.centos.org/centos-linux-eol/); as such it is no
longer maintained by its authors with no further feature or security updates.
Furthermore, package repositories have started to remove packages with security
issues meaning that other dependent packages will no longer function. It is
therefore no longer possible for us to maintain a secure and reliable CentOS 8
LabsLauncher package. 

[Aplanat](https://github.com/epi2me-labs/aplanat) is the Python library that we use for preparing the tabular and
graphical data within our workflow and tutorial HTML reports. Aplanat has been
updated to version v0.6.1 and includes a variety of updates that accommodate
the presentation of tabbed-tabular data. Please see the [aplanat changelog](https://github.com/epi2me-labs/aplanat/blob/master/CHANGELOG.md) for
further details.

We look forwards to any feedback and comments and would welcome insight as to
workflows and tutorials that you would like to see in the future.

</content:encoded><content:thumbnail/></item><item><title><![CDATA[Nextflow and Jupyter Labs]]></title><description><![CDATA[Running Nextflow workflows from EPI2ME Labs notebook environment.]]></description><link>https://labs.epi2me.io/jupyterflow</link><guid isPermaLink="false">https://labs.epi2me.io/jupyterflow</guid><pubDate>Wed, 09 Feb 2022 00:00:00 GMT</pubDate><content:encoded> 

We are pleased to introduce an update to our EPI2ME Labs product that now
includes the functionality to launch our Nextflow pipelines directly from the
EPI2ME Labs session. These new capabilities have been implemented to facilitate
the launching of integrated workflows in an efficient manner whilst avoiding
the requirement for using a terminal session. 

### Getting started

Install the new version of the EPI2ME Labs Launcher; this can be downloaded
from our [Downloads](/downloads) page. The LabsLauncher
interface is largely unchanged – there is one small, but critical, requirement
– we need to specify the path to the Nextflow binary.

 
![jupyterflow](./nextflow-path.png &quot;Figure 1. To set the Nextflow path, open
the EPI2ME Labs Launcher &apos;Settings&apos; dialog and enter the path into the
&apos;nextflow_path&apos; field. In the figure above the nextflow binary is located
within the author’s Documents folder. More information on the Nextflow
installation may be found in the EPI2ME Labs Workflows quickstart guide.&quot;)


### The new EPI2ME Labs landing page

When EPI2ME Labs is launched a new landing page is now presented. The landing
page still provides the list of recently opened and available tutorials but now
also provides a link to the EPI2ME Labs Workflows. The landing page is shown in
Figure 2.

![landing-page](./landing-page.png &quot;Figure 2. EPI2ME Labs now has an updated
landing page that is presented in your web browser. This still provides access
to the recently run and other available tutorials (or notebooks as they are
referred to in the Jupyter environment).  EPI2ME Workflows can be selected by
clicking on the &apos;Workflows&apos; link provided in the navigation bar at the top of
the screen.&quot;)

When the &quot;Workflows&quot; link is selected from the navigation bar at the top of the
landing page, the available workflows will be presented. The workflows page
also shows the list of workflows that have been run, or are still running.
Figure 3 shows a screenshot of the Workflows page – this shows three of the
available workflows that may be selected and shows that one workflow,
ArabidopsisSeq is currently running.
   
![workflows-page](./workflows-page.png &quot;Figure 3. Screenshot showing the EPI2ME
Labs workflows page. This page shows both available workflows and a history of
workflows that have either been run in the past or are currently running on the
host computer system.&quot;)


### Running a workflow

To run a workflow is now simple: select a workflow from the list of available
workflows. This will open a new dialog page that will encourage you to specify
a nextflow analysis. Figure 4 shows how an analysis can be built for the
wf-alignment workflow – we need to specify both the collection of FASTQ files
to map and the FASTA format reference genome. The analysis is started with a
click of the &quot;RUN COMMAND&quot; button and the analysis will commence.
 
![workflow-options](./workflow-options.png &quot;Figure 4. Selecting the
wf-alignment workflow presents a dialog that asks for the relevant experimental
parameters. This includes the starting FASTQ sequences (under Input/output
options, not shown) and the reference genome sequence. Starting the analysis is
performed by clicking on the &apos;RUN COMMAND&apos; button shown at the bottom of the
screen.&quot;)

Once a workflow has been started the experiment information becomes available
as a workflow instance. Each instance has its own page and this provides both
the output from the Nextflow command, links to the result files and additional
metadata on the version of the workflow called, the parameters passed and the
actual command run on the computer. Figure 5 shows an example of a workflow
instance screen for the wf-alignment workflow that we have building.

![workflow-instance](./workflow-instance.png &quot;Figure 5. Screenshot taken from
the workflow instance report for an analysis that is running. The report shows
the parameters that have been passed to the workflow, the intermediate output
from the Nextflow command (and potentially error messages) and the result files
(bottom panel).&quot;)

When the analysis has completed the results are made available through the same
instance report page. The analysis report is accessible by clicking on the
&quot;OPEN REPORT&quot; button.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs 22.01.01 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-22.01.01-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-22.01.01-release</guid><pubDate>Wed, 26 Jan 2022 00:00:00 GMT</pubDate><content:encoded>
Dear Nanopore Community,

This Wednesday we have a trio of announcements; these include a new
Nextflow pipeline for gene isoform characterisation, updates to our wf-artic
software and an updated dataset release that includes Remora-called 5mC
information from GM24385.

### New workflow for gene isoform characterisation from transcriptomic sequencing data

We are delighted to introduce a new EPI2ME Labs workflow, [wf-isoforms](https://github.com/epi2me-labs/wf-isoforms). This
workflow provides a robust pipeline for the characterisation of gene isoforms
from transcriptomic sequence collections. The workflow is based on, and now
supersedes, the [pipeline-nanopore-ref-isoforms](https://github.com/nanoporetech/pipeline-nanopore-ref-isoforms) and
[pipeline-nanopore-denovo-isoforms](https://github.com/nanoporetech/pipeline-nanopore-denovo-isoforms).

wf-isoforms can accommodate single or multiplexed sequence collections and
provides a simplified and scalable product for the analysis of gene isoforms.
The workflow is best run using available genome annotation information (GTF
files) to both assign sequenced reads to known gene isoforms and to aid in the
discovery of potentially novel isoforms. The workflow can also be run using
experimental de novo parameters to assist in the annotation of genes and their
isoforms from organisms where little prior genome annotation is available.

The workflows both use the [pychopper](https://github.com/nanoporetech/pychopper) software to select for appropriate
full-length sequence reads from the starting sequence collection. The workflow
produces an HTML format report that summarises the analysis and results
obtained. When a reference genome annotation has been used, the results include
[GffCompare](https://ccb.jhu.edu/software/stringtie/gffcompare.shtml) assignments for the observed transcripts; these assignments can be
used to identify the potentially novel isoforms as shown in Figure 1.


![wf-isoforms](./isoforms.png &quot;Figure 1. Screenshot from the wf-isoforms HTML report showing information on the
number of isoforms observed and their novelty. Novelty is determined using the
GffCompare software. These results were obtained from the analysis of just two
million D. melanogaster cDNA sequences subsampled from a larger PromethION
sequencing run.&quot;)


### Updates to wf-artic workflow for SARS-CoV-2 sequence analysis

A new version of
our wf-artic software has also been released. The wf-artic v0.3.10 update
includes support for NEB primersets and includes updates for both Pangolin
(v3.1.17) and Nextclade (v.1.8.0). This update is available through the
project&apos;s github pages and through our EPI2ME product. This release also allows
you to specify `–update_data` at runtime, which will provide you with the latest
Pangolin and Nextclade tools and datasets. Please also have a review of our blog post
on lineage and clade assignment using wf-artic: [SARS-CoV-2 Midnight Analysis](/sarscov2-midnight-analysis).



### Remora for 5mC analysis and associated data release

We have also released an
ont-open-data dataset that can be used to evaluate and benchmark the 5mC
basecalling results obtained using the new Remora algorithm as implemented in
Bonito. This dataset and instructions for how it may be used are included in an
EPI2ME Labs blog post. The EPI2ME Labs modified bases tutorial has been updated
and now uses modbam2bed for the preparation of bedMethyl format data. The
tutorial will demonstrate how to produce the beautiful plots as presented in the
blog post.

![Phased 5mC Calls](./igv_5mc.png &quot;Figure 2. Phased 5mC calls in the vicinity of the Prader-Willi gene SNRPN, depicted in IGV. The presence of 5mC is highlighted in red.; the paternal and maternal copies are differentially methylated.&quot;)


We look forwards to any feedback and comments and would welcome insight as to
workflows and tutorials that you would like to see in the future.



</content:encoded><content:thumbnail/></item><item><title><![CDATA[SARS-CoV-2 Midnight Analysis]]></title><description><![CDATA[Background article on the choice of Nextflow for authoring bioinformatics workflows for nanopore sequencing.]]></description><link>https://labs.epi2me.io/sarscov2-midnight-analysis</link><guid isPermaLink="false">https://labs.epi2me.io/sarscov2-midnight-analysis</guid><pubDate>Mon, 24 Jan 2022 00:00:00 GMT</pubDate><content:encoded>
## Introduction

I wanted to write a blog post about SARS-CoV-2 sequencing data analysis to achieve two things:

1. Clarify the changes needed to the underlying code for ARTIC analysis to accommodate midnight and,
2. Discuss lineage and clade assignment and how we keep up to date with new versions of these.

### Oxford Nanopore Technologies Midnight

Sequencing of SARS-CoV-2 has been pivotal in the ongoing pandemic. First the rapid publication of the original sequence from Wuhan to ongoing surveillance to identify and track the emergence of variants of the virus that could be a public health concern.

There are many methods to sequence the genome of SARS-CoV-2 but one of the most popular remains the [ARTIC Network protocol](https://artic.network/). This protocol relies on 98 overlapping ~400bp amplicons split into two pools. This enables full genome coverage even in situations where viral RNA might be more degraded or is present at low number of copies.

The amended Midnight protocol uses longer 1200bp amplicons first proposed in a publication by [Freed *et. al.*](https://academic.oup.com/biomethods/article/5/1/bpaa014/5873518?login=false). These primers were designed using [primalscheme](https://primalscheme.com/) by [Quick *et. al.*](https://www.nature.com/articles/nprot.2017.066).

In addition to the amplicon length the key difference between the Midnight protocol and the original ARTIC protocol with Nanopore sequencing is the library preparation method. ARTIC original used the ONT ligation sequencing kit and therefore all reads produced are equivalent to the amplicon length. Midnight however uses the rapid library preparation chemistry, improving turnaround time but due to the transposase tagmentation employed in this method read lengths are less than or equal to the intact amplicon length.

***

## 1. Changes to ARTIC Bioinformatics Analysis

The ARTIC bioinformatics analysis workflow is globally recognised as the gold standard for the processing of ARTIC tiled amplicon SARS-CoV-2 genomes. The SOP can be found [here](https://artic.network/ncov-2019/ncov2019-bioinformatics-sop.html).

Because of the differences between the original ARTIC method and Midnight, amendments were made to the underlying assumptions in the ARTIC FieldBioinformatics package used to analyse data generated by tiled amplicon sequencing of SARS-CoV-2 and so our `wf-artic` Nextflow workflow was born.


### 1a. Read lengths are longer (*and SHORTER!*)

This might seem like an obvious point, but because the Midnight amplicons are longer than the standard Artic amplicons *and* we have the existence of fragmented amplicons, we need to adjust the read length cut-offs used to filter reads. The standard ARTIC bioinformatics SOP recommends using reads &gt;=400bp and &lt;=700bp.

It is obvious that we need to include longer reads to account for the increased amplicon size, but because of the library preparation method we also need to allow for the presence of shorter reads. **Therefore we use reads &gt;=150bp and &lt;=1200bp**.


### 1b. Reads length is not amplicon length

In addition due to the tagmentation library preparation in Midnight read lengths will no longer always be the same as the length of the intact amplicons defined by primer pairs. Steps in the underlying analysis code for ARTIC original data assume that the read length will equal that of the amplicon. We have therefore made changes to this code under guidance from the original authors to allow for shorter read lengths. These changes can be summarised as:
- The code no longer requires that reads fully span an amplicon region,
- The code tags a read as belonging to an amplicon region simply by largest overlap,
- The read selection code to achieve desired coverage was rewritten to account for incomplete amplicons, whilst retaining the longest reads.


### Changes to ARTIC FieldBioinformatics

Changes have been applied to the `align_trim.py` Python program in the ARTIC network FieldBioinformatics package:

- For each read we find the amplicon from which it originated by selecting the amplicon with the largest overlap with the read, we also find the next closest match.
- We discard the read if the next closest match is a large proportion of the mutual overlap of the two amplicons. This is a guard against chimeric reads, either from library preparation or faults in the sequencing platform control software.
- (There is an option to only allow those reads that extend across the whole amplicon, if set to true then we check whether the alignment extends to the primer at each end, this is a &quot;correctly paired&quot; read.)
- To normalise we take the passing reads, sort by the amount of coverage they provide and take the 1st *n* reads.

The modified `align_trim.py` program can be found [here](https://github.com/epi2me-labs/fieldbioinformatics/blob/align_trim/artic/align_trim.py).

### Other points of note

- Like ARTIC original, we downsample reads, using only 200 in each direction for each amplicon. Reads above this coverage threshold are discarded.
- Twenty reads covering a position are required for a mutation call.
- `wf-artic` uses medaka as opposed to the default in ARTIC original which is nanopolish, although there is an option to use the faster medaka. Using medaka also negates the need for fast5 files.


## 2. Lineage and clade assignment

For many users the classification of the SARS-CoV-2 sample being sequenced into a clade or lineage is often the primary end point of analysis workflows described here. These classifications help us put the sequence into the context of the global pandemic and create a shared language we can use when discussing the sample. Further, the identification of genomic changes which differ from the definitions of these clades and lineages might help the identification of important changes that could help define new clades and lineages. We realise that timely updates to lineage calling tools and the data they use is an important consideration for those analysing SARS-CoV-2 sequence data.

There are some excellent publications and blog posts which discuss lineages and clades that you may wish to read, including:

- https://www.nature.com/articles/s41564-020-0770-5
- https://nextstrain.org/blog/2020-06-02-SARSCoV2-clade-naming


### The Problem

The rapid generation of sequencing data and the emergence of variants of SARS-CoV-2 with new constellations of mutations requires that the data underlying the tools used to classify a SARS-CoV-2 sequence into a clade or lineage are in a constant state of flux. We must therefore balance rapid releases, resources, and ensuring the most important clades/lineages are identified by our workflow. The most important clades/lineages are those that have been deemed Variants of Concern (VOCs) or Variants Under Investigation (VUIs) by WHO or UKHSA as often our users are sequencing to inform public health decisions in the field.

We have opted for a model that we think helps satisfy those users who have no- or intermittent- internet access, or for reasons of security have no internet access on certain facets of their computing infrastructure.

We have automated processes that run daily on our continuous integration servers that check our analysis software images hosted on Dockerhub ([Pangolin](https://hub.docker.com/r/ontresearch/pangolin/tags) and [Nextclade](https://hub.docker.com/r/ontresearch/nextclade/tags)) for the latest versions from their authors. These images are then kept up to date automatically. Users of our `wf-artic` Nextflow workflow can specify the version of Pangolin `--pangolin_version` or Nextclade `--nextclade_version` to use when they run `wf-artic` - but the version you select *must* be available from our Dockerhub registry. These are static images and (usually) are never updated again.


### Nexclade

Data used by Nextclade to determine the clade to which your SARS-CoV-2 sample belongs is provided in a GitHub repository: https://github.com/nextstrain/nextclade_data. This repository also contains data for other viruses so you need to navigate to `data/datasets/sars-cov-2/references/MN908947/versions` to see the data packages available. These are helpfully organised by date and time. We maintain a copy of this data in the `wf-artic` repository in `data/nextclade`. If no `--nextclade_data_tag` (i.e. `2021-12-16T20:57:35Z`) is specified at `wf-artic` run time then the most recent contained within our repository will be used. You may specify any tag that we have in the `data/nextclade` directory.

If you want the absolute latest version just specifying `--update_data` at runtime will download the latest Nextclade dataset with the command `nextclade dataset get` before Nextclade is executed.

If you also specify the `--nextclade_data_tag` then that version will be downloaded by `nextclade dataset get`


### Pangolin

In general the data available to Pangolin is determined at the time when our continuous integration systems build our docker analysis images. We don&apos;t release a new docker container image unless the version of Pangolin itself is increased. But like Nextclade we can update when we run `wf-artic`.

Pangolin data updates are organised slightly differently to Nextclade. Pangolin can update both itself and the data files it uses with the command `pangolin --update`. If you specify `--update_data` at runtime, the update will be executed before the lineage assignment takes place and you will run the latest version of this lineage classification tool.

Again you can specify the version of Pangolin you would like to run within `wf-artic`, as long as we have a docker image in our Dockerhub registry by specifying `--pangolin_version`. If you also specify `--update_data` then the data used by Pangolin will also be upgraded to the latest available at runtime.

### Advanced: Manually updating a local Pangolin or Nextclade docker container image

If you would like to update the Pangolin docker container used by `wf-artic` follow the instructions below:

```
docker run ontresearch/pangolin:3.1.17 pangolin --update
```

Your output should look something like this:

```
pangolin already latest release (v3.1.17)
pangolearn updated to 2022-01-20
constellations already latest release (v0.1.1)
scorpio already latest release (v0.3.16)
pango-designation already latest release (v1.2.123)
```

Now note the image identifier of the container just fetched and run:

```
docker ps -a
```

Your output should look like this:

```
CONTAINER ID   IMAGE                                 COMMAND                  CREATED              STATUS                      PORTS                                NAMES
068f67cb118e   ontresearch/pangolin:3.1.17           &quot;pangolin --update&quot;      About a minute ago   Exited (0) 29 seconds ago                                        keen_poincare
```

and commit your update:

```
docker commit &lt;CONTAINER_ID&gt;  ontresearch/pangolin:3.1.17-updated
```

Where `&lt;CONTAINER_ID&gt;` = `068f67cb118e` in this case.

Then when you next run `wf-artic` specify `--pangolin_version 3.1.17-updated` at runtime and it will use the local container you just created.

You can follow a similar procedure to upgrade the Nextclade data in the Nextclade docker container image.

### Useful Links

- ARTIC Network: https://artic.network/
- Online Pangolin Lineage Assignment: https://pangolin.cog-uk.io/
- Online Nextclade Clade Assignment: https://clades.nextstrain.org/
- UKHSA Varinat Definitions: https://github.com/phe-genomics/variant_definitions
- SARS-CoV-2 Codon to Nucleotide: https://codon2nucleotide.theo.io/
</content:encoded><content:thumbnail>https://labs.epi2me.io/static/0be72d91de3e75aabbac10d36291da9e/AdobeStock_322667262.jpeg</content:thumbnail></item><item><title><![CDATA[Phased CpG Methylation Calling in GM24385 with Remora and Clair3]]></title><description><![CDATA[Reanalysing our matched Bisulfite-Nanopore Open Dataset release with Remora and Clair3.]]></description><link>https://labs.epi2me.io/gm24385-5mc-remora</link><guid isPermaLink="false">https://labs.epi2me.io/gm24385-5mc-remora</guid><pubDate>Thu, 20 Jan 2022 00:00:00 GMT</pubDate><content:encoded>
import coorPlotPhase from &quot;./coorplot-phase.jsx&quot;

[Previously](/gm24385-5mc) we released a nanopore dataset comprising 5-methylcytosine basecalls using the
Guppy basecaller tailored to the specific task of identifying 5mC. In this post we
present fresh set of basecalls using the new algorithms for the [Remora](https://github.com/nanoporetech/remora) project,
and integrated into the research-grade basecaller [Bonito](https://github.com/nanoporetech/bonito).

For more information and help downloading data from our open dataset archive
see the [Datasets Tutorials](https://labs.epi2me.io/tutorials) page. All the data
referred to in this blog can be accessed under:

    s3://ont-open-data/gm24385_mod_2021.09/extra_analysis/bonito_remora

The most relevant files stored under this top level and referred to below are:

    240.6 GiB all.bam
     76.9 MiB all.bam.bai
    233.6 GiB all.hp.bam
     76.9 MiB all.hp.bam.bai
     75.2 MiB all_contigs.vcf.gz
      1.5 MiB all_contigs.vcf.gz.tbi
    891.4 MiB bonito.cpg.bed.gz
      1.9 MiB bonito.cpg.bed.gz.tbi
    727.9 MiB bonito.hp1.cpg.bed.gz
      1.8 MiB bonito.hp1.cpg.bed.gz.tbi
    722.6 MiB bonito.hp2.cpg.bed.gz
      1.8 MiB bonito.hp2.cpg.bed.gz.tbi

Please also refer to the [original post](/gm24385-5mc) introducing this dataset.

&gt; The GM24385 cell line samples were obtained from the NIGMS Human Genetic Cell
Repository at the Coriell Institute for Medical Research.


### Bonito-Remora Basecalling

Previous iterations of modified-base basecalling in the Guppy basecaller have required
use of a basecalling model specific to the task of identifying modified bases. These
models have typically traded the ability to call base modifications for a slight decrease
in canonical basecalling accuracy. The advent of the new algorithms of Remora allow
highest accuracy basecalling and identification of modified bases in a single basecalling
process, reducing the computation requirements to obtain such results.

As with the analysis workflow using Guppy, the Bonito basecaller is capable of outputting
[BAM](https://en.wikipedia.org/wiki/SAM_(file_format)) files annotated with methylation calls as described in the 
[SAM tags](https://samtools.github.io/hts-specs/) specification found at: https://samtools.github.io/hts-specs.
Bonito can be provided with a reference genome and instructed to output BAM files with the `MM`
and `ML` tags described in the specification documents:

    bonito basecaller dna_r9.4.1_e8_sup@v3.3 \
        &lt;fast5 input directory&gt; \
        --modified-bases 5mC \
        --reference &lt;minimap2 reference index&gt; \
        --recursive \
        --alignment-threads 8 \
        | samtools view -u | samtools sort -@ 8 &gt; &lt;output.bam&gt;
    samtools index &lt;output.bam&gt;

Similarly the resultant BAM file can be summarized to a [bedMethyl](https://www.encodeproject.org/data-standards/wgbs/)
using the [modbam2bed](https://github.com/epi2me-labs/modbam2bed) program
(available through conda for both Linux and MacOS):

    modbam2bed \
        -e -m 5mC --cpg -t 10 \
        GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \
        &lt;bonito bams&gt; ... \
        | bgzip -c &gt; bonito.cpg.bed.gz 

We therefore have obtained highest accuracy basecalls and modification calls in a useful summarised
form with only two computational steps from the primary sequencer measurements.

&lt;Bokeh plotJson={coorPlotPhase} plotName=&quot;Correlation between nanopore and bisulfite sequencing&quot;
    plotCaption=&quot;Figure 1. Heatmaps indicating correlation between CpG site methylation frequencies
    from bisulfite and nanopore sequencing. Limited to sites with 20 or more spanning reads for both
    technologies.&quot;/&gt;


### Phased Methylation Calls

To further demonstrate the enhanced utility of long-read nanopore sequencing for modified-base
identification, we are also providing phased methylation calls. These have been produced de-novo
using our [wf-human-snp](https://github.com/epi2me-labs/wf-human-snp) Nextflow workflow to produce phased small variant calls
using [clair3](https://github.com/HKU-BAL/Clair3) and [whatshap](https://whatshap.readthedocs.io/).
The phased variants were used to tag reads as belonging to one of the two haplotypes, and `modbam2bed`
run to produce a bedMethyl file per haplotype.

The variant calling workflow was run using,

    nextflow run epi2me-labs/wf-human-snp \
        -r v0.1.2 --model r941_prom_sup_g5014 --phase_vcf \
        --bam all.bam \
        --ref GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \
        --out_dir clair3 -w clair3

to produce a VCF file containing phased variants. These phased variants were then used
to tag each read as belonging to one of the two haplotypes, `whatshap` was used for this
task with,

    whatshap haplotag \
        --ignore-read-groups \
        --output all.hp.bam \
        --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \
        all_contigs.vcf.gz all.bam

The above command produced a BAM file with an HP (haplotype) tag for each read. Phased methylation
statistics were obtained using `modbam2bed` with the `--haplotype` option, once for each
haplotype:

    for HP in 1 2; do
        modbam2bed \
            -e -m 5mC --cpg -t 10 --haplotype ${HP} \
            GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \
            all.hp.bam \
            | bgzip -c &gt; bonito.hp${HP}.cpg.bed.gz
    done;


As a well-known and characterised example of differential methylation between maternal
and paternal haplotypes, Figure 2. depicts sequencing data in the region around the
[Prader-Willi Syndrome](https://en.wikipedia.org/wiki/Prader%E2%80%93Willi_syndrome) associated gene [SNRPN](https://en.wikipedia.org/wiki/Small_nuclear_ribonucleoprotein_polypeptide_N). Using the procedure above sequencing
data is tagged as either &quot;haplotype 1&quot; or &quot;haplotype 2&quot;: a striking difference
in the rate of 5mC presence is observed.

![Phased 5mC Calls](./igv_5mc.png &quot;Figure 2. Phased 5mC calls in the vicinity of the Prader-Willi gene SNRPN, depicted in IGV. The presence of 5mC is highlighted in red.; the paternal and maternal copies are differentially methylated.&quot;)


### Discussion

Here we have shown how the latest software tools from Oxford Nanopore Technologies can
be used to obtain simply phased CpG modification calls for the GM24385 human cell-line.
The methods used are applicable to any diploid sample. We hope that these new tools
will greatly accelerate fields of research where DNA methylation is known to play an
important role, and also unlock new insights.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[Phasing and Whatshap]]></title><description><![CDATA[An overview of phasing methods and Whatshap.]]></description><link>https://labs.epi2me.io/phasing-and-whatshap</link><guid isPermaLink="false">https://labs.epi2me.io/phasing-and-whatshap</guid><pubDate>Fri, 26 Nov 2021 00:00:00 GMT</pubDate><content:encoded> 

Phasing, also referred to as haplotyping, relates to methods used to infer which genetic variants occur together, usually distinguishing which genetic variants came from each of the maternal or paternal haplotypes. It may also be used to predict more than two haplotypes in polyploid phasing. It can refer to phasing of the whole genome, or a smaller subsequence.

Understanding which variants are inherited together ‘in phase’, can help give a clearer picture of how certain genes may affect expression and regulation, functional impacts of genes or sets of genes, diseases associated with sets of inherited genes, genetics within populations and gene inheritance (Tewhey et al., 2011). It can also help find variants resulting from compound heterozygosity (when each parent donates one alternate allele located at different loci within the same gene) and find whether 2 heterozygous variants occur in 1 copy of a gene or in both copies (Choi et al.,  2018). Phased genes may be referred to as cis when variants are from the same chromosome or trans when they are from different chromosomes.

There are various methods for phasing. The method used for phasing will depend on the sequencing technology used and what level of detail is required: 

+   Physically separating sequences in the lab, which involves isolating chromosomes, amplification, sequencing and then piecing fragments together – requiring detailed lab protocols (Martin et al., 2016). The later step can introduce errors where segments are phased accurately but pieced back together incorrectly. 

+   Population based phasing methods use large data sets to predict haplotypes based on statistical likelihood, projects such as [1000 genomes](https://www.internationalgenome.org/)  and [Hapmap](https://www.genome.gov/10001688/international-hapmap-project)  catalogue common variants and haplotype blocks. Tools such as [Shape IT](https://mathgen.stats.ox.ac.uk/genetics_software/shapeit/shapeit.html)  and [Beagle](https://faculty.washington.edu/browning/beagle/beagle.html) use a [maximum-likelihood model](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) using the data sets to predict phasing. This method is unable to find variants that are rare or unique to an individual and relies on large datasets being representative of the population (Choi et al., 2018).

+	Trio Phasing or genetic haplotyping where parent(s)&apos; and offspring(s)&apos; heterozygous variant calls can be compared to predict which haploid they come from by considering Mendel’s laws of inheritance, this is a very straight forward method but can be relatively expensive due to the need to sequence 3 genomes and will not, find de-novo mutations, or distinguish variants that are heterozygous in all individuals  (Martin et al., 2016).

+	Read based phasing also known as haplotype assembly relies on long enough reads that span two or more heterozygous variants. (Martin et al., 2016).). If a read spans two variant positions and contains both variants, this suggests that both variants lie on the same allele and vice versa (Hager et al., 2020). A weighted minimum error correction algorithm is used to infer haplotypes. This approach can phase individual-specific variants, providing the most specific detail on haplotypes. 

Sequencing read data from Oxford Nanopore Technologies’ can be used for all the methods but is uniquely suited to read based phasing because the length of reads is enough that they are likely to contain multiple SNVs. The greater sequence context helps align the reads to a reference. Phasing short reads can be difficult because each read will contain fewer SNVs. Here is a nice [animation illustrating this](https://www.youtube.com/watch?v=B88TSfh5aks). It is possible to do with as little as 60x reads and it is also now possible to include methylation information in the phasing step. 

## Whatshap 

There are many tools for read based phasing but we currently favour [Whatshap](https://whatshap.readthedocs.io/en/latest), due to good run time and has low error rates. Whatshap can phase SNP’s, insertions, deletions, multiple adjacent SNP’s and some complex variants. Whatshap has an algorithm to solve a weighted minimum error correction problem and takes into account phred-scaled error probabilities, to find the minimum number of corrections required in order to arrange the reads into two haplotypes. You can read about it [here (Martin et al., 2016)](https://www.biorxiv.org/content/10.1101/085050v2.full.pdf ).

Whatshap requires sequencing reads and an unphased VCF as input. The initial VCF can be created by aligning sequence reads to a reference and using a variant calling tool but it is also integrated in to popular variant calling tools including [Medaka](https://github.com/nanoporetech/medaka) and [Clair3]( https://github.com/HKU-BAL/Clair3).  Phasing information is output in the VCF file using 0/1 1/0 for heterozygous,  1/1 0/0  homozygous and can be visualised in IGV or other tools. Whilst Whatshapp is currently our choice of phasing tool there is still ongoing research and development of tools by algorithm experts that we continually review.

Why not try out our new [Human snp workflow](https://github.com/epi2me-labs/wf-human-snp) that uses Clair3 and have a look at the Phased output VCF.

Choi, Y., Chan, A. P., Kirkness, E., Telenti, A. and Schork, N. J., 2018. Comparison of phasing strategies for whole human genomes. PLOS Genetics [online], 14 (4), e1007308. Available from: https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1007308 [Accessed 26 Nov 2021].

Hager, P., Mewes, H.-W., Rohlfs, M., Klein, C. and Jeske, T., 2020. SmartPhase: Accurate and fast phasing of heterozygous variant pairs for genetic diagnosis of rare diseases. PLOS Computational Biology [online], 16 (2), e1007613. Available from: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007613 [Accessed 26 Nov 2021].

Martin, M., Patterson, M., Garg, S., O Fischer, S., Pisanti, N., Klau, G. W., Schöenhuth, A. and Marschall, T., 2016. WhatsHap: fast and accurate read-based phasing [online]. Bioinformatics. preprint. Available from: http://biorxiv.org/lookup/doi/10.1101/085050 [Accessed 26 Nov 2021].

Tewhey, R., Bansal, V., Torkamani, A., Topol, E. J. and Schork, N. J., 2011. The importance of phase information for human genomics. Nature reviews. Genetics [online], 12 (3), 215–223. Available from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3753045/ [Accessed 26 Nov 2021
</content:encoded><content:thumbnail/></item><item><title><![CDATA[Nextflow in Windows with WSL2]]></title><description><![CDATA[A installation guide to getting started running Nextflow workflows on Windows.]]></description><link>https://labs.epi2me.io/nextflow-on-windows</link><guid isPermaLink="false">https://labs.epi2me.io/nextflow-on-windows</guid><pubDate>Wed, 20 Oct 2021 00:00:00 GMT</pubDate><content:encoded> 

***For EPI2ME Labs version 3 and onward the setup of Windows Subsytem for Linux is automated
provided users have an up-to-date version of Windows 10 or 11. These instructions
are provided to aid users on older versions of Windows that cannot update. For users
who can update we (and [Microsoft](https://learn.microsoft.com/en-us/windows/wsl/install-manual))
recommend doing so before attempting to use the method presented here. The simple
[wsl -\-install](https://learn.microsoft.com/en-us/windows/wsl/install)
method should be used in most cases, which is the method used in the automated setup
within EPI2ME Labs 3 and newer.***

It is easy to run and develop Nextflow workflows on Windows but there are some
additional set up steps required. We have found the set up can be tricky so
have outlined the steps required here with links to trouble shooting and
further help.

&gt; This guide is similar to that presented at [a guide from Nextflow](https://www.nextflow.io/blog/2021/setup-nextflow-on-windows.html)
&gt; which goes in to greater detail and has additional steps useful for developing workflows.
    

You will need Windows 10 and it is critical that you have Admin privileges for
all the steps,

1.	***Check your windows version is sufficient***

    + Start &gt; search ‘Windows update settings’ &gt; at the bottom select ‘OS Build
	  info’
    + Under Windows specifications - Under OS Build info - Your Build number must be
      18362.1049 or 18363.1049 (or higher).
    + Make any Windows updates that are required, this will cause your computer
	  to restart and could take a while.


2.  ***Enable Windows Subsystem for Linux (WSL).***
    
    If you have **Windows 10 May 2020 (2004) update (or later)**
    installed you can install WSL with a single command:
    + Start &gt; search for &apos;powershell&apos; &gt; Right click and run as admin &gt; copy/paste
	the following - 

    ```
    wsl.exe --install
    ```

    + Restart your computer and you will be prompted to set up Ubuntu with a
	username and password and can **skip to step 8**.

    If you have an **earlier Windows version** or the above has not worked continue
    following the steps -

    + Start &gt; search for &apos;powershell&apos; &gt; Right click and run as admin &gt; copy/paste
	the following - 

    ```
    dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart
    ```

    + Copy and paste the following in to Powershell
    
    ```
    dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart
    ```

3. ***Update to WSL2***

    + Download this WSL2 update file and run through the installation wizard:
    [wsl_update.msi](https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi)

    + A guide to WSL installation troubleshooting can be found [here](https://docs.microsoft.com/en-us/windows/wsl/install-manual).

4. ***Set WSL2 as the default version.***

    + Open a new Powershell with admin rights
    + In Powershell copy and paste

    ```
    wsl --set-default-version 2
    ```

5. ***Download WSL Ubuntu***

    + From the start menu open the Microsoft store
    + In search bar type Ubuntu
    + Download and install the Ubuntu 64-bit version: Focal 20.04 (LTS).

6. ***Check WSL version for Ubuntu installation***
    
    WSL installations come in two versions, creatively name 1 and 2. We must
    ensure the Ubuntu installation is set to version 2.

    + In Powershell run:

      ```
      wsl --list –v
      ```

      The response should look something like:
    
      ```
      H:\&gt; wsl --list -v
        NAME                   STATE           VERSION
      * Ubuntu                 Running         2
        docker-desktop-data    Running         2
        docker-desktop         Running         2
        Ubuntu-18.04           Running         1
      ```
    + If a version is listed as 1 and not 2, change the version with 

      ```
      wsl --set-version &lt;distribution name&gt; &lt;version&gt;
      ```
      For example to change the Ubuntu-18.04 installation above to version 2:
      ```
      H:\&gt; wsl --set-version Ubuntu 2
      ```

7.  ***Set up Docker***

    + Download [Docker Desktop](https://www.docker.com/products/docker-desktop) 
    + Run the installation as an administrator and follow the install wizard.
    + Add your computer username to the &quot;docker group&quot; in WSL - in Powershell type: 
    
    ```
    wsl
    sudo groupadd docker
    sudo usermod -aG docker $(whoami)
    ```
    
    + Reboot your PC

8.  ***Enable WSL2 in docker***

    These docker instructions can also be found [here](https://docs.docker.com/desktop/windows/wsl/)
    + Start Docker Desktop from the Windows Start menu.
    + From the Docker menu, select Settings &gt; General
    + The &apos;Use the WSL 2 based engine&apos; box should already be ticked, if not tick
    the box and click Apply &amp; Restart.

    ![Docker settings](./dockersettings.png)

    + Go to Settings &gt; Resources &gt; WSL Integration 
    + &apos;Enable integration with my default WSL distro&apos; should be ticked and select
    Enable integration with the Ubuntu version you have just downloaded.

    ![Docker WSL Integration](./wslintegration.png)

    + If you are unsure of the Ubuntu version go to Powershell and type

    ```
    wsl
    lsb_release -d
    ```

    This will respond with something like:

    ```
    Description:    Ubuntu 20.04.3 LT0
    ```

9. ***Set WSL2 resource limits***

	It is important to configure docker resource limits so it does not use all available memory and cpu.

	+ You will need to update a different file depending on if you want a
      global configurartion for all WSL distributions or a specific one for each
      distribution.
      * *Global*
        + In Windows Explorer navigate to `C:/Users/&lt;USERNAME&gt;`
        + Create or open existing file `.wslconfig`
      
	  * *Specific Distribution*
        + Open specific distribution terminal eg. Ubuntu or type wsl Powershell
	    + Navigate to the `/etc` and open a file:
      
        ```
	    cd /etc
	    sudo vim wsl.conf
	    ```
	  * Copy and paste the following in to the respective file
        ```
	    [wsl2]
        memory=8GB 
	    processors=4
        localhostForwarding=true 
	    ``` 
    + Adjust settings in the config based on your available resources. To find
      out total available on your device you can CTRL-ALT-DELETE to open task
      manager, click performance tab and look at details of CPU and memory. Total no.
      of processors will be &apos;logical processors&apos;. Remember to leave some resource
      available for other programmes on your computer.
    + Save the file (in vim press *Esc* and type `:wq!`).
    + Shut down all running wsl instances via cmd line
    
      ```
      wsl --shutdown
      ```
    
    + Now relaunch wsl and check resources are as expected using cmds 
      ```
      cat /proc/cpuinfo
      cat /proc/meminfo
      ```
    
    For more advanced settings see [here](https://docs.microsoft.com/en-us/windows/wsl/wsl-config#configure-global-options-with-wslconfig)


9. ***Install Java***
    
    To install Java into the WSL2 Ubuntu installation run (after starting WSL
    with the `wsl` command):

    ```
    sudo apt install openjdk-11-jre-headless
    ```

10. ***Download and install Nextflow***
    
    Nextflow can be installed following the instruction in their
    [Getting started](https://www.nextflow.io/docs/latest/getstarted.html) pages:
    
    + Run the following:
    ```
    curl -s https://get.nextflow.io | bash 
    mv nextflow /usr/bin/nextflow
    chmod +x /usr/bin/nextflow
    ```
    
    + Make sure Nextflow is updated to the latest version by typing - 
    
    ```
    nextflow self-update
    ```

11. ***Test nextflow***

    To test Nextflow we can run a simple [workflow template](https://github.com/epi2me-labs/wf-template).

    + To obtain some test data, download the workflow:
    ```
    git clone https://github.com/epi2me-labs/wf-template.git
    ```

    + To see required parameters of the workflow run:
    ```
    nextflow run epi2me-labs/wf-template 
    ```

    + And to run the workflow with the test data run:    
   
    ``` 
    OUTPUT=output
    nextflow run epi2me-labs/wf-template \
        -w ${OUTPUT}/workspace \
        --fastq wf-template/test_data --out_dir wf-template/${OUTPUT}
    ```

If you had any problems with this tutorial or think any additional information
would be useful then please [let us know](https://labs.epi2me.io/contactus) so
we can improve it for other users. 
</content:encoded><content:thumbnail/></item><item><title><![CDATA[Changing Location of Workflow Container Images]]></title><description><![CDATA[A guide to changing the storage location of persistent workflow data, focussing on Docker container images.]]></description><link>https://labs.epi2me.io/docker-storage</link><guid isPermaLink="false">https://labs.epi2me.io/docker-storage</guid><pubDate>Wed, 20 Oct 2021 00:00:00 GMT</pubDate><content:encoded>
We&apos;ve noticed that common struggle from GridION and PromethION users of EPI2ME Labs
Nextflow workflows is that the default location in which Docker stores our workflow
container images is the somewhat small &quot;root partition&quot; of the devices. In this
post we will walk through how to change the location at which Docker stores these
data to the larger `/data` volume on the sequencing devices.

&gt; Users who have a Docker installation provided through installation of recent
&gt; versions of MinKNOW *should not* follow these instructions. If in doubt contact
&gt; support@nanoporetech.com.


### Docker Configuration

The configuration of the Docker daemon, the process that is responsible for starting,
running, managing, and ultimately killing containers is handled through a single JSON
formatted configuration files. Common to other Linux systems this file is located at:

    /etc/docker/daemon.json

on the GridION and PromethION sequencing platforms. To change the location where Docker
(and hence Nextflow) stores the container images associated with workflows it is this
file that we must edit.

&gt; Altering the Docker configuration and stopping and restarting the Docker daemon requires
&gt; root access. The commands in this document are all run with `sudo` for this reason.

The default directory where Docker keeps [persistent data](https://docs.docker.com/config/daemon/#docker-daemon-directory)
under Linux is:

    /var/lib/docker

On GridION and PromethION sequencing devices, this location is stored on a somewhat small
hard drive partition that was not intended for storage of large data. To change the location
we need to edit the file noted above to tell Docker a more appropriate location to use
for storage.

To start, open a terminal window and enter the command:

    sudo gedit /etc/docker/daemon.json


This file should be empty, if it is not it is possible that your Docker installation has been
preconfigured either by an installation of MinKNOW or by another user. Contact support@nanoporetech.com
if the contents of this file are unfamiliar to you.

In the event that the file is empty, let us proceed to set the Docker storage location. Copy and paste
the following into the gedit text editor:

```
{
    &quot;data-root&quot;: &quot;/data/docker/&quot;
}
```

and click the &apos;Save&apos; button in gedit.

![Editing the Docker configuration file](./gedit-edit.png &quot;Editing the Docker configuration file in the gedit text editor.&quot;)

Close gedit and return to the terminal window. Enter the following command to restart the
Docker daemon with the new settings applied:

    sudo systemctl restart docker

The command may take a few seconds to run. To test the new settings let us run a Docker container
and check that Docker uses the new storage location. Run a simple container with:

```
# first remove any existing image
docker image rm ubuntu:latest

# now download and run a fresh copy of the image
docker run --rm ubuntu:latest echo Hello
```

The output of this command should be something like:

```
Unable to find image &apos;ubuntu:latest&apos; locally
latest: Pulling from library/ubuntu
7b1a6ab2e44d: Pull complete 
Digest: sha256:626ffe58f6e7566e00254b638eb7e0f3b11d4da9675088f4781a50ae288f3322
Status: Downloaded newer image for ubuntu:latest
Hello
```

Running the command below should reveal content at the path specified:

    sudo ls /data/docker/

If this is not the case we again recommend contacting support@nanoporetech.com
for further assistance.

### Summary

This guide has walked through configuring the &quot;data-root&quot; configuration item of the
Docker daemon, and restarting the daemon for these changes to take effect. For more
information on configuring Docker on Linux systems see the [official documentation](https://docs.docker.com/config/daemon/).
</content:encoded><content:thumbnail/></item><item><title><![CDATA[October 2021 GM24385 Q20+ Simplex Dataset Release]]></title><description><![CDATA[An updated dataset release of simplex nanopore sequencing from the Genome in a Bottle sample GM24385. Sequencing libraries were prepared using our Kit 12 sequencing chemistry and DNA sequences were produced using R10.4 flowcells (FLO-PRO112)]]></description><link>https://labs.epi2me.io/gm24385_q20_2021.10</link><guid isPermaLink="false">https://labs.epi2me.io/gm24385_q20_2021.10</guid><pubDate>Fri, 08 Oct 2021 00:00:00 GMT</pubDate><content:encoded>
import summaryPlot from &quot;./q20summary.jsx&quot;

We are happy to announce an updated dataset release of *simplex* nanopore sequencing
from the Genome in a Bottle sample GM24385. Sequencing libraries were
prepared using our Kit 12 sequencing chemistry and DNA sequences
were produced using R10.4 flowcells (FLO-PRO112).

Multiple PromethION flowcells were used during data generation. The sequencing output 
is provided both as raw signal data stored in .fast5 files is provided 
and as basecalled data in .fastq files. Super accuracy *simplex* basecalling was performed 
using the Guppy 5.0.15 basecaller.

Additional secondary analyses are included in the dataset release:

* alignments of *simplex* sequence data to the reference genome are provided along with performance statistics,
* whole genome structural variant calls and benchmarking results,
* and small variant calls for chr6 and chr10.

***What&apos;s included?***

The dataset comprises data from multiple R10.4 flowcells with the Kit 12
sequencing chemistry.

The primary sequencing outputs are included as self contained directories.
The derived outputs from the Katuali pipeline are also stored
separately.

The data is available from an Amazon Web Services S3 bucket at:

    s3://ont-open-data/gm24385_q20_2021.10/

See the [tutorials](/tutorials/) page for information on downloading datasets.

The uploaded dataset has been prepared using a snakemake pipeline to:

1. Perform basecalling using the latest Guppy 5.0.15 basecaller,
2. Align basecalls to reference sequence. All primary, secondary and
supplementary alignments are kept,
3. Filter .bam file to list of regions defined in configuration file
retaining only primary alignments,
4. Produce read statistics from per-region .bams.

For more details see our [post](/katuali_human_pipeline/) detailing the
pipeline and its outputs.

***Information about sample preparation***

*The following cell line samples were obtained from the NIGMS Human Genetic Cell
Repository at the Coriell Institute for Medical Research: GM24385*

High molecular weight DNA from GM24385 lymphoblastoid cells was prepared by 
[Evotec](https://www.evotec.com/en). A standard, published protocol was used
to deplete DNA fragments &lt; 35kb in length. DNA was end repaired and dA tailed
prior to LSK based library preparation.


## Dataset Summary

The outputs of the analysis pipeline include summary files which are readily
interrogated using standard data analysis software. The `sequencing_summary.txt`
files prepared by the Guppy basecaller can be used to answer basic questions
concerning read length and sequencing yield, while the `calls2ref_stats.txt`
calculated from read to reference alignments produced by the `minimap2` aligner
can be used visualize, amongst other statistics, the read accuracy. Summary graphs
are shown in *Figure 1*.

&lt;Bokeh plotJson={summaryPlot} plotName=&quot;Dataset Summary&quot;
    plotCaption=&quot;Figure 1. Per-flowcell dataset summary statistics. Experiments were performed &apos;interaction free&apos;, with no nuclease flowcell flushing or library reloading.&quot;/&gt;


## Variant calling

Variant calling for the GM24385 genome has been performed and analysed with respect
to published benchmark callsets.

***Small variant calling***

Small variant calling for Chromosome 20 was performed
using [clair3](https://github.com/HKU-BAL/Clair3) with inference models trained explicitely
for R10.4 flowcells with the Kit 12 chemistry. This model is hosted at our
[rerio](https://github.com/nanoporetech/rerio) model repository and is fully compatible
with the latest version of Clair3. All inputs, outputs, and commands to reproduce the
results presented below are available at:

    s3://ont-open-data/gm24385_q20_2021.10/extra_analysis/small_variants

and can be downloaded by following the instructions provided in our [tutorials](/tutorials/)
page.

The table below shows recall, precision, and f1-scores for both
insertion and deletion, and single nucleotide polymophisms for variant calls reported
by Clair3. The benchmarking was performed with respect to the GIAB v4.2.1 truth set
with no additional filtering.

| Metric                |     INDEL      |     SNP        |
|-----------------------|----------------|----------------|
| Variants in truth set |       11256    |       71333    |
| Recall                |     0.82907    |     0.99913    |
| Precision             |     0.91288    |     0.99902    |
| f1-score              |     0.86896    |     0.99908    |


***Structural variant calling***

In addition to small variant calling we also performed structural variant calling
across the whole genome using our [wf-human-sv](https://github.com/epi2me-labs/wf-human-sv)
pipeline. wf-human-sv is based on [lra](https://github.com/ChaissonLab/LRA)
and [cuteSV](https://github.com/tjiangHIT/cuteSV). All data required to reproduce
the results presented below are available at:

    s3://ont-open-data/gm24385_q20_2021.10/extra_analysis/structural_variants

The table below illustrates key metrics for the variant calls output by the
`wf-human-sv` pipeline. Benchmarking was performed with respect to the GIAB v0.6 Tier1
variant set.

| Metric                | Value          |
|-----------------------|----------------|
| Variants in truth set |     9641       |
| Recall                |     0.9760     |
| Precision             |     0.9473     |
| f1-score              |     0.9615     |

Although the entire dataset was used for these calculations, we note that accurate
structural variation calling can be also be performed at significantly lower depth of coverage.

## Summary

The new R10.4 GM24385 dataset is being released to provide researchers with access to an example
dataset that demonstrates the capabilities of the new sequencing chemistry and can
be used as a resource to support the development of new algorithms.

This dataset may be re-released in future as iterations to the sequencing and flowcell chemistries
are made. Duplex basecalling has not yet been performance, the basecalled reads as all *simplex*.
Continuous upgrades to the Q20+ chemistry and library preparation protocols focused on output
and delivering high duplex yields will follow. A duplex basecalled release of GM24385 is planned.

As always we look forward to your feeback and hope that the dataset provides a valuable
and instructive resource.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[Detection of 5-methylcytosine modification in GM24385]]></title><description><![CDATA[Analysing our matched Bisulfite-Nanopore Open Dataset release.]]></description><link>https://labs.epi2me.io/gm24385-5mc</link><guid isPermaLink="false">https://labs.epi2me.io/gm24385-5mc</guid><pubDate>Fri, 27 Aug 2021 00:00:00 GMT</pubDate><content:encoded>
import coorPlot from &quot;./coorplot.jsx&quot;
import coverPlot from &quot;./coverplot.jsx&quot;

Using nanopore sequencing, researchers have directly identified DNA and RNA base
modifications at nucleotide resolution, including
[5-methylcytosine](https://www.nature.com/articles/nmeth.4184),
[5-hydroxymethylcytosine](https://www.nature.com/articles/nmeth.4189),
[N6-methyladenosine](https://www.nature.com/articles/s41467-019-08289-9), and
[5-bromodeoxyuridine](https://www.nature.com/articles/s41592-019-0394-y) in DNA;
and [N6-methyladenosine](https://www.nature.com/articles/nmeth.4577) in RNA, with detection of other natural or synthetic epigenetic
modifications possible through training basecalling algorithms. One of the most
widespread genomic modifications is 5-methylcytosine (5mC), which most frequently
occurs at [CpG](https://en.wikipedia.org/wiki/CpG_site) dinucleotides. Compared to whole-genome bisulfite sequencing, the
traditional method of 5mC detection, nanopore technology can offer many advantages
which we will explore in this post with the aid of newly released data in our
[ONT Open Datasets](https://labs.epi2me.io/dataindex/) archive.

For more information and help downloading data from our open dataset archive
see the [Datasets Tutorials](https://labs.epi2me.io/tutorials) page. All the data
referred to in this blog can be accessed under:

    s3://ont-open-data/gm24385_mod_2021.09/

In the below we provide direct links to the more interesting analysis outputs.

&gt; The GM24385 cell line samples were obtained from the NIGMS Human Genetic Cell
Repository at the Coriell Institute for Medical Research.


### GM24385 dataset generation

In order to demonstrate the utility and convenience of Oxford Nanopore Technologies&apos;
sequencing platform for performing detection and analysis of 5mC, we have sequenced
the HG002 Genome in a Bottle Sample GM24385 with both traditional bisulfite sequencing
and using nanopore sequencing. Both technologies, old and new, were applied to the
same sample from a single DNA extraction.

*Bisulfite sequencing*

Bisulfite sequencing was performed by a commercial provider and processed with the
commonly used [bismark](https://www.bioinformatics.babraham.ac.uk/projects/bismark/)
package to obtain the proportion of reads displaying methylation
at CpG sites throughout the whole genome. The primary output of this processing is a single
[BedGraph](https://genome.ucsc.edu/goldenPath/help/bedgraph.html)-like file describing these proportions:

    https://ont-open-data.s3.amazonaws.com/gm24385_mod_2021.09/bisulphite/cpg/CpG.gz.bismark.zero.cov.gz

Bismark was installed using the [mamba](https://github.com/mamba-org/mamba) package manager,
the commands used to produce the above file can be found in:

    https://ont-open-data.s3.amazonaws.com/gm24385_mod_2021.09/bisulphite/fastq2bed.sh

with the input read data being located at:

    https://ont-open-data.s3.amazonaws.com/gm24385_mod_2021.09/bisulphite/reads/004_0111_001_R1.fq.gz
    https://ont-open-data.s3.amazonaws.com/gm24385_mod_2021.09/bisulphite/reads/004_0111_001_R2.fq.gz

The running of the bismark analysis pipeline to produce the final BED file took on
the order of 4 days on a desktop computer.

*Nanopore sequencing*

Nanopore sequencing was performed using the same sample of GM24385 material sent for bisulfite
sequencing. Sequencing was performed on the MinION platform, across multiple flowcells, as part
of ongoing platform development activities. The sequencing was not performed explicitly for the
analysis presented here; we are making available all sequencing runs undertaken with this
sample for the benefit of the community.

Sequencing was carried out in October 2020 with results presented here being derived from
fresh basecalling using Guppy version 5.0.1 and the `dna_r9.4.1_450bps_modbases_5mc_hac`
configuration, the optimal choice for methylation calling of CpG sites.

The simplicity of the analysis workflow presented here leverages the ability of the Guppy
basecaller to output [BAM](https://en.wikipedia.org/wiki/SAM_(file_format)) files annotated with methylation calls as described in the 
[SAM tags](https://samtools.github.io/hts-specs/) specification found at: https://samtools.github.io/hts-specs.
Guppy can be provided with a reference genome and instructed to output BAM files with the `Mm`
and `Ml` tags described in the specification documents:

    guppy_basecaller \
        --config dna_r9.4.1_450bps_modbases_5mc_hac.cfg \
        --device cuda:0 \
        --bam_out --recursive --compress \
        --align_ref &lt;reference fasta&gt; \
        -i &lt;fast5 input directory&gt; -s &lt;output directory&gt;

After basecalling, which can be performed live during the sequencing run, a simple one
step process can be used to summarize the BAM files into methylated and unmethylated frequency information
akin to the bismark BED file. Our recently developed [modbam2bed](https://github.com/epi2me-labs/modbam2bed) program
is available through conda for both Linux and MacOS:

    modbam2bed \
        -e -m 5mC --cpg -t 10 \
        &lt;reference fasta&gt; &lt;guppy bams&gt; ... \
        &gt; guppy.cpg.bam

The BED file output by the above conforms to the [bedMethyl](https://www.encodeproject.org/data-standards/wgbs/)
description from the [ENCODE](https://www.encodeproject.org/) project.

This simple one step analysis contrasts with the multistep and time-consuming steps required to process the
raw bisulfite sequencing data to obtain the frequency counts. The program will happily consume
multiple BAM files simultaneously (up to limits imposed by the user&apos;s system) to
produce aggregated counts. One current small wrinkle is that Guppy does not currently
produce a BAM index files alongside its BAM files such that the user must first index
Guppy&apos;s outputs with `samtools`:

    ls &lt;guppy output directory&gt;/*.bam | xargs samtools index

A future version of Guppy will correctly output BAM indices such that this step is
no longer required. For reference the Open Dataset archive includes a single, consolidated
BAM file (and index) for all sequencing runs in the set:

    https://ont-open-data.s3.amazonaws.com/gm24385_mod_2021.09/extra_analysis/all.bam
    https://ont-open-data.s3.amazonaws.com/gm24385_mod_2021.09/extra_analysis/all.bam.csi

The corresponding BED file with methylation frequencies is available at:

    https://ont-open-data.s3.amazonaws.com/gm24385_mod_2021.09/extra_analysis/all.cpg.bed

In contrast to the bisulfite data processing use of `modbam2bed` to aggregate data from
the basecaller outputs is possible in minutes on a desktop computer.


### Technology Comparison

To further demonstrate the power of nanopore sequencing for the detection of 5mC in CpG sites
we will briefly survey the properties of the two sequence datasets. Our aim here is not
to analyse the full biology of the sample sequenced but merely to illustrate that
nanopore sequencing data is a convenient and accurate replacement for bisulfite
sequencing.

Firstly as we have shown [previously](https://nanoporetech.com/resource-centre/benchmarking-nanopore-methylation-analysis-comparison-publicly-available-bisulphite),
nanopore sequencing does not suffer from GC-content contextual bias often associated
with short-read bisulfite sequencing such that accurate methylation frequencies can be obtained throughout
entire genomes. Figure 1. depicts the read coverage of CpG sites in chromosome 1 for
both the nanopore and bisulfite sequencing experiments. The nanopore data show a
tighter coverage distribution with very few sites of low coverage. By contrast the
bisulfite sequencing shows a tail of low coverage (both absolute and relative to
the mean coverage), with a noticeable spike close to zero coverage.

&lt;Bokeh plotJson={coverPlot} plotName=&quot;Coverage comparison&quot;
    plotCaption=&quot;Figure 1. Comparison of sequencing coverage in bisulfite and nanopore sequencing.&quot;/&gt;

Of course simply achieving low coverage bias does not guarantee acceptable results,
we would like that the methylation frequencies obtained are correct. To this end
Figure 2. illustrates the correlation between the calculated bisulfite frequencies
and those obtained from nanopore sequencing.

&lt;Bokeh plotJson={coorPlot} plotName=&quot;Correlation between nanopore and bisulfite sequencing&quot;
    plotCaption=&quot;Figure 2. Heatmap indicating correlation between CpG site methylation frequencies
    from bisulfite and nanopore sequencing.&quot;/&gt;

There is a strong correlation (R=0.943) between the per-site methylation proportions calculated
from the two technologies. We note that commercial bisulfite providers typically quote
bisulfite conversion error of around 2%, which goes some way to explain the lack of perfect
correlation. The use of [megalodon](https://github.com/nanoporetech/megalodon)
can improve the accuracy of 5mC identification further to unprecedented levels of accuracy.

Through examining the data provided in the Open Dataset archive we invite users
to explore both datasets in more detail.


### Discussion

In this short post we have introduced matched bisulfite and nanopore sequencing datasets
of a single DNA extraction from a GM24385 cell line sample. We have shown how 5mC identification
can be performed easily without time consuming or specialised sample preparation or data
analysis by utilising Oxford Nanopore Technologies&apos; sequencing platforms. 

The dataset is provided for use by the community. We hope that it will aid in development
of new and existing tools such as [mbtools](https://github.com/jts/mbtools), [methplotlib](https://github.com/wdecoster/methplotlib),
and [pycoMeth](https://github.com/snajder-r/pycoMeth).
</content:encoded><content:thumbnail/></item><item><title><![CDATA[Assembly tools and Flye]]></title><description><![CDATA[An overview of the Flye assembler, the assembler we recommend for use with nanopore sequencing data.]]></description><link>https://labs.epi2me.io/assembly-flye</link><guid isPermaLink="false">https://labs.epi2me.io/assembly-flye</guid><pubDate>Fri, 06 Aug 2021 00:00:00 GMT</pubDate><content:encoded> 


Assembly is required because sample extraction, processing, handling and sequencing can lead to the fragmentation of DNA molecules comprising whole chromosomes are rarely present. With careful preparation reads in excess of 2 Mbases have been achieve on Oxford Nanopore Technologies&apos; sequencing platforms. Many tools exist which aim to achieve the best assembly whilst balancing computational requirements and time constraints. 

There are commonly two types of whole genome consensus tasks undertaken in bioinformatics: i) reference assisted consensus and ii) de novo assembly. In the first of these we use knowledge of an exisiting reference sequence as a scaffold to piece together reads. De novo assembly by contrast does not use prior knowledge and is therefore a more difficult task that requires greater computation but is arguably of greater utility as it also allows assembly of anything that may not be present in a reference sequence database, removes bias a reference sequence may introduce and allow you to find novel components or structural variations.

Sequencing read data from Oxford Nanopore Technologies&apos; sequencing platforms are ideal for creating de novo assemblies due to the long-read lengths produced, having longer reads means that there are more unique sequences and more easily distinguishable overlaps between reads which makes it easier to piece them together. This also makes them more useful for resolving repetitive elements and other structural ambiguities.

Some popular assembly tools for working with ONT data include [Flye](https://github.com/fenderglass/Flye), [Canu](https://github.com/marbl/canu), [Raven]( https://github.com/lbcb-sci/raven), [Shasta](https://github.com/chanzuckerberg/shasta) and [Miniasm](https://github.com/lh3/miniasm). I don’t plan to explain the algorithms used, as these are explained superiorly in the tool’s respective papers but it’s useful to understand that they make use [graphs](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)) to model relationships between sequence fragments. The Overlap–layout–consensus (OLC), approach is used in part by all mentioned above. This can be presented as a directed overlap graph where each node is a read and edges are where sequence fragments overlap. Scored edges can then be used to find a most likely consensus sequence. Whilst various tools use OLC they differ by including additional steps, using different alignment methods, and scoring systems to come to an ultimate assembly.

## Flye

The assembler that we have been using frequently in our workflows is Flye which constructs a repeat graph where edges represent the genomic sequence and the nodes are sequences overlaps. The edges are labelled as either unique or repetitive. The genome is then predicted by traversing the graph so each of the unique edges appear once. A more in depth explanation can be [found here](https://github.com/fenderglass/Flye#repeat-graph). You can also see from the Github page that Flye has been benchmarked with sequence sets ranging from small bacteria to large human giving it a broad applicability. Flye is still being actively improved and developed with the most recent release being in February this year.

There are a lot of benchmarking papers and it’s important to keep in mind that they are each focused on specific applications. Different assemblers may be better suited to different applications and there is not yet one assembler that far out-weighs the rest for all applications.
One recent independent study where tools were benchmarked for use with prokaryote whole genome sequencing for various aspects found that Flye was one of the top assemblers and made the fewest sequence errors in comparison to other tools for this application but notably used the most RAM: Flye used 8-16 Gb vs. 8 Gb or less for all other assemblers (Wick and Holt 2021). Most people these days are likely to have access to 16 Gb on a regular laptop. Canu had the longest run time of 1 to 6 hours, where as Flye’s average was 15 minutes making it easier to carry out full analysis in a reasonable amount of time. Another paper benchmarking assembly tools for the application of plant genome assemblies concluded that Flye and Canu were best for creating accurate assemblies (Jung et al. 2020).

From our own experience, we have found Flye to be reliable and able to resolve assemblies in most cases. It generally represents an improvement in consensus accuracy and a decrease in assembly time, compared to other tools we have used in the past. We also find it very user friendly, requiring less configuration than some other tools.

For some datasets Flye fails to complete assemblies after a substantial amount of time and consistently failed to assemble certain datasets at all so we have needed to use of other assemblers. Whilst Flye is often our go to assembler for each workflow we research and experiment with various options to find the most suitable in each case. 

Experimental design as well as other tools used before assembling may also have an impact on results for example filtering on read length, quality score filtering, removing adapters or identifying areas within reads that are low quality may help when an assembly fails. When looking to find a robust assembly, it can be worth using more than one assembler and comparing results.

Assembly is an ongoing area of research with improvements to speed and accuracy being made continually by algorithm experts. Flye is often our first choice of assembler due to its broad applicability, it’s speed and reliability in solving an assembly but it is not the only assembler that we use. We continually review tools and look forward to future developments and improvements.


Wick, R.R. and Holt, K.E. 2021. Benchmarking of long-read assemblers for prokaryote whole genome sequencing. F1000Research 8, p. 2138. doi: 10.12688/f1000research.21782.4.

Jung, H. et al. 2020. Comparative Evaluation of Genome Assemblers from Long-Read Sequencing for Plants and Crops. Journal of Agricultural and Food Chemistry 68(29), pp. 7670–7677. doi: 10.1021/acs.jafc.0c01647.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs 21.07.02 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-21.07.02-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-21.07.02-release</guid><pubDate>Thu, 29 Jul 2021 00:00:00 GMT</pubDate><content:encoded>
Dear Nanopore Community,

Our (second!) July 2021 EPI2ME Labs release includes various updates to
workflows and tutorials.

The [wf-clone-validation ](https://github.com/epi2me-labs/wf-clone-validation) workflow has been updated to version v0.1.0 and now
includes the [pLannotate ](https://github.com/barricklab/pLannotate) tool for the visualisation and annotation of plasmid
consensus sequences. The pLannotate software uses a collection of public
sequence databases to explore the plasmid sequence for regions of interest that
include e.g. the origin or replication and antibiotic resistance genes and may
include information on the cloned insert sequence (if orthologues are present in
the sequence database).

![Screenshot from the wf-clone-validation HTML report showing the
pLannotate figure for the assembled plasmid. The coordinates for the annotated
features are also shown in a tabular format along with additional information on
sequence identify and coverage.
](./clone_validation_fig1.png)


The [wf-artic ](https://github.com/epi2me-labs/wf-artic) pipeline v0.3.1 release now reports a per-sample BAM file in the
result directory. These BAM files are a standard component of the
[FieldBioinformatics ](https://github.com/artic-network/fieldbioinformatics) workflow and correspond to the sorted sequence mapping data
that has been structured by readgroup and trimmed of sequencing primers. This
user request addresses the requirement that BAM files are also provided along
with strain identification information.

The [EPI2ME Labs SV tutorial ](https://labs.epi2me.io/notebooks/Structural_Variation_Tutorial.html) has been updated to use LRA for read mapping and
CuteSV for the identification of genomic structural variation; this synchronises
the functionality between our research tool, [pipeline-human-sv](https://github.com/nanoporetech/pipeline-structural-variation), and the EPI2ME
Labs offerings. As a brief reminder, [EPI2ME Labs](https://labs.epi2me.io/) provides a collection of
best-practice tutorials that explore a variety of themes in the analysis of
Nanopore sequence data. The EPI2ME Labs quick start guide provides more
information.

The [SARS-CoV-2 tutorial](https://labs.epi2me.io/notebooks/SARS_CoV_2_Analysis_Workflow.html) in EPI2ME Labs has been repaired - the V1200 primer set
has been added back to the workflow.

We have made a number of minor changes to the way that we prepare and distribute
our nextflow based workflows.

* The documentation for our 11 workflows (all are available through GitHub but a
couple are still in development) now focuses only on the configuration,
execution and results from the pipeline - we have removed the common
introductory and configuration texts; these documents are provided in the [quick
start guide](https://labs.epi2me.io/wfquickstart) and additional information is also included in our [how-to documents](https://labs.epi2me.io/category/how-tos/).
* We have improved our dockerfile housekeeping - this means that there are minor
version updates to all our of [workflows](https://labs.epi2me.io/wfindex) and the corresponding docker images are
now considerably smaller.
* The environment.yaml files that define Conda channels
and corresponding software requirements are now better defined in the
nextflow.config. This update provides a more robust workflow experience for
users who cannot use the recommended Docker software and/or prefer to run
workflows inside a Conda environment (with the -profile conda option).
* [Mamba](https://github.com/mamba-org/mamba) is now included as our preferred software for the installation of software packages
from Conda channels.

We look forwards to feedback and recommendations for future tutorials and
workflows.

</content:encoded><content:thumbnail/></item><item><title><![CDATA[Normalizing fastq data]]></title><description><![CDATA[An introduction to fastcat -- the program we use in our workflows for manipulating fastq files from Nanopore sequencing devices.]]></description><link>https://labs.epi2me.io/introducing-fastcat</link><guid isPermaLink="false">https://labs.epi2me.io/introducing-fastcat</guid><pubDate>Wed, 07 Jul 2021 00:00:00 GMT</pubDate><content:encoded>
import yieldPlot from &quot;./yield.jsx&quot;

Oxford Nanopore Sequencing devices output RNA and DNA read sequences in batched
[fastq](https://en.wikipedia.org/wiki/FASTQ_format) files. This is useful for
real-time analyses like those run by EPI2ME but can be inconvenient for providing
the data to standard bioinformatics tools, many of which only cater for data
provided in a single file. In this post we&apos;ll examine a new tool developed for
use in EPI2ME Labs workflows to provide a first-line data sanitization and
normalization step.

## The problem

The fastq format is ubiquitous for storing sequencing reads. Data to be analysed
by a workflow can however be presented with at least three wrinkles:

1. there might be multiple fastq files to be analysed as a whole,
2. the files may contain reads from a multiplexed experiment to be analysed separately, 
3. or the files may or may not be compressed with [gzip](https://en.wikipedia.org/wiki/Gzip) compression.

We would like a process to handle these situations to format the data in a
standardised form: a single gzip compressed fastq file for each demultiplexed
sample. There are already a variety of programs that can take
as input the data described above and get us most of the way to the desired
result. Indeed for most cases simple shell scripting can do the job. The standard
Linux `cat` program can be used in simple cases with either uncompressed
or gzip compressed data, but not a mixture. The demultiplexing aspect of the
task complicates things further: Oxford Nanopore Sequencing devices output
demultiplexed reads helpfully into distinct directories, but what if this
structure has been lost? We would like a more robust solution.

## The solution

To solve this problem the EPI2ME Labs team developed the [fastcat](https://github.com/epi2me-labs/fastcat)
program. The program handles the three concerns above placing minimal contraints
on the inputs. It performs this task using two key ingredients:

1. the stream-based fastq/fasta parser [kseq](https://github.com/attractivechaos/klib) as used in 
       the ubiquitous [htslib](https://github.com/samtools/htslib) package,
2. the header information written to fastq records by the Guppy basecaller.

The program is not yet a complete replacement to more fully
featured fastq toolkits such as the excellent [seqkit](https://bioinf.shenwei.me/seqkit/) though
does provide a few additional generally useful features in addition to solving
the core normalization functions. These include length and quality filtering, as
well as producing a summary file containing per-read statistics. Being written
in C it performs these tasks [several times faster](https://github.com/lh3/biofast/tree/master/fqcnt)
than any similar programs written in languages such as Python.


&lt;Bokeh plotJson={yieldPlot} plotName=&quot;yield-plot&quot;
    plotCaption=&quot;The fastq header parsing of fastcat allows plotting experimental yield withou resorting to Fast5 or standalone sequencing summary files.&quot;/&gt;


## Summary

Bioinformatics workflows often require using tools which are not designed
to work in a real-time, streaming fashion and require all sequencing data
to be provided in a single input file. fastcat provides a convenient method
to normalize input data for downstream tools.

**Key fastcat features**

* accepts compressed, uncompressed, or a mixture of fastq files,
* filters reads on length and quality score,
* produces per-read and per-input file summaries including information from fastq headers,
* recapitulate standard MinKNOW demultiplexing directory layout from unstructured data. </content:encoded><content:thumbnail/></item><item><title><![CDATA[Quality Scores And Read Accuracy]]></title><description><![CDATA[In this brief post we will examine the concepts of read accuracy and read quality scores, and how they are calculated by Oxford Nanopore Technologies.]]></description><link>https://labs.epi2me.io/quality-scores</link><guid isPermaLink="false">https://labs.epi2me.io/quality-scores</guid><pubDate>Thu, 01 Jul 2021 00:00:00 GMT</pubDate><content:encoded>
import accuracyPlot from &quot;./accuracy.jsx&quot;
import qscorePlot from &quot;./qscore.jsx&quot;
import jeffreysPlot from &quot;./qscore-pseudo.jsx&quot;

&lt;MathJax /&gt;

In this brief post we will examine the concepts of read accuracy and read
quality scores, and how they are calculated by Oxford Nanopore Technologies.

## Phred Scores

Quality scores may be given for a single base, a read, or a consensus sequence.
In each case quality scores simply reflect the probability of error in
the associated sequence. A higher score indicates a higher probability
that a sequence decision is correct whilst a lower score indicates a higher
probability that the decision is incorrect.

You are likely familiar with [Phred
scores](https://en.wikipedia.org/wiki/Phred_quality_score) which originated from
a base calling program written to identify fluorescently labelled DNA bases,
separated via electrophoresis. The quality score for each base pair was
calculated by comparing observed and expected chromatogram peak shapes and
resolution. 

The Phred quality score&apos;s relationship with error probability is: 

$$ Q = -10 \log_{10}P $$

where P is the error probability. The error probability can also be calculated
from a given quality score from the inverse transform:

$$ p = 10^\frac{-q}{10}$$

Using Log10 means that a quality score of 10 represents a 1 in 10 chance of an
incorrect base call (a base call accuracy of 90%), where as quality score
of 20 represents a 1 in 100 chance of incorrect base call (or 99% accuracy). 

Phred quality scores are usually recorded in fastq files using ASCII characters,
which you can learn more about by looking at our [Introduction to FastQ
tutorial](/notebooks/Introduction_to_fastq_file.html).

## Producing quality scores

The way per-nucleotide quality scores are calculated depends on the base caller. The quality
scores output by ONT base callers are based on the outputs of the neural
networks used to produce the base call. The direct per-base score output from
the neural networks do not reflect-well empirically observed errors: the scores
must be calibrated globally across all sequence positions in the training data.
After such calibration the probability implied by the score for each group of
bases (bases with that score), is expected to correlate with the empirical
probability that the bases in that group are correct, as judged by alignment to
reference sequences.

When thinking about quality scores, it is important to consider how
discriminative they are. Take an example of 100 bases at a locus, 50 of which
are incorrect. Now we could label all bases with ~Q3 indicating 50% error.
However, that’s rather unhelpful in determining the true identity of the base at
the locus. We&apos;d prefer a score which discriminates the correct bases from the
incorrect bases: so, a score that indicates 100% confidence in the correct bases
and 100% dismissal of the incorrect bases (though naturally if you knew a base
was definitely incorrect you might be better off randomly selecting another
base!). Algorithms which produce quality scores attempt to do the latter, with
varying degree of success. The scores which they output need to be interpreted
based on studies of populations.

## Read accuracy

Much has been made in the past of how exactly Oxford Nanopore Technologies
calculates single-read accuracy. The definition that we choose is 
the proportion of base identical match columns within an alignment of a read
to a reference sequence:

$$
\frac{N_{matches}}{N\_{matches} + N\_{mismatches} + N\_{deletions} + N\_{insertions}}
$$

This is the same definition as describe in Heng Li&apos;s [blog post](https://lh3.github.io/2018/11/25/on-the-definition-of-sequence-identity),
referred to there as BLAST identity. This definition was in use at ONT and used in all
our publically available materials long before it became fashionable to cite
Heng&apos;s post! The definition is used by the Guppy basecaller (when asked to
perform alignments on the fly), and several research tools including the `stats_from_bam`
program from [pomoxis](https://github.com/nanoporetech/pomoxis).

## Guppy read Q-scores and read accuracies

Let&apos;s have a closer inspection of the read quality scores from the base caller
Guppy. First let us look at the Guppy&apos;s predicted read Q-score (the
&quot;mean_qscore_template&quot; column of its summary file) versus the read accuracy
as measured by alignment to a reference sequence. This per-read quality score
is calculated from the per-base quality scores as:

$$
\text{read Q} = -10\log_{10}\big[\tfrac{1}{N}\sum 10^{-q_i/10}\big]
$$

That is to say it is the average per-base error probability, expressed on the
log (Phred) scale.

The data below are taken from around twenty thousand reads from chromosome 1
of our [NA24385 open dataset](/gm24385_2021.05).

&lt;Bokeh plotJson={accuracyPlot} plotName=&quot;accuracy-plot&quot;
    plotCaption=&quot;Empirical read error as calculated from read-to-reference alignment vs. Guppy &apos;mean Q-score&apos; read metric,
    for ~20k reads from chromosome 1 of NA24385.
    The Guppy per-base Q-scores are calibrated such that the mean Q-score predicts the empirical read error; this plot
    is therefore approximately linear.&quot;/&gt;

The black dashed line shown in the plot indicates what would be a perfect
correlation between the two quantities. We see that there is a strong correlation
between the quantities but some samples are overconfident and error for those reads was
higher than predicted. 

It is in fact these quantities which are used to calibrate the per-base quality
scores. The calibration procedure proceeds as follows. A fit between read
accuracy and mean Q-score is performed using iteratively reweighted least
squares regression with a Huber downweighting of outliers. The derived regression
coefficients are then used to rescale the per-base quality scores.

&gt; Guppy per-base quality scores are calibrated using the predicted mean per-base
&gt; error for a read and the read accuracy measured by alignment.

Let us now focus on the per-base quality score, as stored within fastq and bam
files. For each read base within an alignment we can determine whether it is
a match, mismatch, insertion, or deletion. Using the base&apos;s assigned Q-score
we can build a database of the proportion of bases which are errors for each
Q-score value. We can then plot the empirical error over all Q-scores: 

&lt;Bokeh plotJson={qscorePlot} plotName=&quot;qscore-plot&quot;
    plotCaption=&quot;Empirical error rate for bases labelled with given Q-score.
    Substitution, insertion, and deletion errors are included (with deletion
    events assigned the Q-score of the preceding base). Guppy&apos;s Q-score calibration
    does not attempt to directly optimise the fit of these quantities.&quot;/&gt;

Again the dashed line in the plot indicates what would be a perfect
correlation between these quantities. We see that up to around Q10 the per-base
quality scores do reflect the empirical error for that score. Quality scores of
greater than 10 tend to underestimate the error probability. For example of
all bases labeled as Q20 the proportional of them which are errors in the 
sequencing is equivalent to ≈Q12.5. This unfortunately compounds the use of the
per-base qualities when attempts are made to use them in algorithms such as
consensus or variant calling. Notwithstanding this observation, the calibration
performed by Guppy, such as it is, is performed globally across all sequence
positions: it does not attempt to model error contigent on the genomic context (although
we may presume that the neural network outputs take-in such effects to some extent).


## Highly accurate reads

&gt; This section was contributed by Tim Massingham, Research Fellow, Oxford
&gt; Nanopore Technologies, and edited by the EPI2ME Labs Team.

As read accuracies increase, read accuracy distributions can have an artifactual
mass on infinity due to shorter, perfect reads.

Extrapolating from these reads to declare that a proportion of reads from
sequencing are &gt;Q50 is unsafe and plotting raw read error-rate distributions
gives a misleading view of the sequencing accuracy. Can we derive alternative
ways of estimating per-read Q-scores.

Let&apos;s first define $e_i$ the number of errors in a read $i$, and $N_i$ the
alignment length of read $i$. The following are then all ways to produce
improved predicted read error rate distributions:

***Frequentist &quot;plus-4&quot; rule***

The [Wilson Score
Interval](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval)
is an improvement to the standard Normal confidence interval with better behaviour
for small samples and extremely skewed distributions.

Intuitively, the confidence interval pulls the centre of the distribution, and
so the point estimate of error rate, towards one-half in a manner dependent on the
sequence length. This leads to the &quot;plus 4 rule&quot;:

$$\hat{p_i} = \frac{e_i + 2}{N_i + 4}$$ 

***Bayesian: Jeffreys&apos; prior***

For those of a Bayesian persuasion, we could instead use a &quot;non-informative&quot;
[Jeffreys&apos; prior](https://en.wikipedia.org/wiki/Jeffreys_prior) over the error
proportion, this is the Beta distribution with parameters (1/2 , 1/2).

The Beta distribution is self-conjugate with respect to the Binomial distribution; the
posterior probability is also a Beta distribution and we can obtain a MAP estimate of the error rate:

$$
Post \sim \beta(\tfrac{1}{2} + e_i, \tfrac{1}{2} + N_i - e_i)
$$
$$
\hat{p_i} = \frac{e_i + \tfrac{1}{2}}{N_i + 1}
$$

In this case, a Jeffreys&apos; prior is equivalent to adding half an error and half a success
to the calculation of a read&apos;s error.

***Empirical Bayes***

We can go futher than a Jeffreys&apos; prior and estimate the error proportion from
all of the reads and assume a Beta prior distribution with this error rate.

As with the Jeffreys&apos; prior approach, the posterior distribution for the error
rate for each read is also a Beta distribution and we can obtain a MAP estimate
of the error rate, leading to:

$$
\hat{p} = \sum_i \frac{e_i}{N_i}
$$
$$
\hat{v} = \sum_i \frac{e_i^2}{N_i^2} - \hat{p}^2
$$
$$
w = \frac{\hat{p}(1-\hat{p})}{\hat{v}} - 1
$$
$$
\alpha = \hat{p}w
$$
$$
\hat{p_i} = \frac{e_i + \alpha}{N_i + w}
$$

Let&apos;s apply the second method to some &quot;duplex&quot; nanopore sequencing data:

&lt;Bokeh plotJson={jeffreysPlot} plotName=&quot;qscore-pseudo-counts&quot;
    plotCaption=&quot;Comparison of read error histograms with and without inclusion of a prior.
    raw: error calculated via alignment. Jeffreys&apos;: as for raw with inclusion of
    uninformative prior. In the former case reads with error &lt;1e-5 have been trucated
    to this value for display purposes. The count axis is truncated at 450.&quot;/&gt;

The effect of the prior is to regress the large values back toward the the uniform prior, that
is toward an error of one-half. 

## Summary

To summarise, quality scores are often taken for granted but there are many
aspects to consider and keep in mind when using quality scores for
evaluating samples or as part of further analysis. Knowing how they have been
produced, and what they reflect is crucial to using them effectively.

The data presented here suggest that current Guppy read &quot;mean Q-scores&quot; are
useful for filtering lower quality reads from a dataset, but that care should
be taken in using the per-base quality scores to compare bases between reads.</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs 21.07 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-21.07-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-21.07-release</guid><pubDate>Wed, 30 Jun 2021 00:00:00 GMT</pubDate><content:encoded>
Dear EPI2ME Labs Users

This release Wednesday we are pleased to launch [wf-human-sv](https://github.com/epi2me-labs/wf-human-sv), a new Nextflow
pipeline for the prediction and review of structural variants from whole human
genome sequencing data. We also provide a cornucopia of updates, improvements
and a couple of bug-fixes to our [LabsLauncher](https://labs.epi2me.io/downloads) product, the [EPI2ME Labs tutorials](/nbindex/)
and the [wf-artic](https://github.com/epi2me-labs/wf-artic) pipeline. 

[wf-human-sv](https://github.com/epi2me-labs/wf-human-sv) provides the core functionality from our research tool,
[pipeline-structural-variation](https://github.com/nanoporetech/pipeline-structural-variation). These pipelines both work by mapping reads to the
reference genome using LRA before identifying candidate SVs using cuteSV. The
VCF file of candidate SVs is filtered on the basis of sequence coverage and
length characteristics to enrich for high-quality candidate insertions and
deletions. The wf-human-sv workflow is implemented using Nextflow and the
requisite bioinformatics software has been crafted into a docker container;
there is no need to wrangle with the installation of the LRA, cuteSV or other
analysis components. In addition to collating the VCF file of quality SVs the
workflow prepares an HTML format report summarising the experiment. Please see
the [example report](/workflows/wf-human-sv-report.html) prepared from the example dataset included with the workflow.
The EPI2ME Labs SV tutorial will be updated to reflect these best-practices in
an up-coming release.

*wf-artic* has been updated to version v0.3.0:

* Improvements and updates for the targeted genotyping functionality.  The
* medaka software bundled in the container has been updated and a selection of
recommended models have also been included. The workflow&apos;s default medaka model
has been updated to r941_prom_variant_g360;  this is the recommended model for
sequence data base-called using guppy version 4.x. For Guppy 5.x based sequence
data, a medaka model should be specified based on pore, platform and
base-calling model. Suitable models include e.g. r941_min_hac_variant_g507 and
r941_prom_hac_variant_g507 for HAC data from R9.4.1 flowcells sequenced on
MinION and PromethION flowcells respectively or r103_hac_variant_g507 for
sequences prepared from R10.3 flowcells. The model used can be controlled using
the --medaka_model parameter.
* The workflow [changelog](https://github.com/epi2me-labs/wf-artic/blob/master/CHANGELOG.md) provides additional information on the changes in this version.

The *EPI2ME Labs Launcher* software has been updated to version v1.2.0:

* The LabsLauncher communicates with the Jupyter notebook server in the
epi2melabs-notebook (please see below) to better report when the notebook server
has successfully started; this avoids the issue where a connected web-browser
would report that the server is temporarily unavailable.
* The update also includes new functionality for selecting appropriate network
ports for the container. This simplifies the usage of the multiple instances of the
LabsLauncher software on a larger shared computer.
* The updated LabsLauncher is available from the [download](/downloads) page.

The *epi2melabs-notebook* container has been updated to version v1.1.1:

* This update includes usability and code updates to improve several tutorials.
* The [SARS-CoV-2 analysis workflow](/notebooks/SARS_CoV_2_Analysis_Workflow.html) has been better aligned with the wf-artic
workflow and now also includes both [NextClade](https://clades.nextstrain.org/) and [Pangolin](https://cov-lineages.org/pangolin.html) strain
identifications.
* The [EPI2ME_Labs_Tutorial](/notebooks/EPI2ME_Labs_Tutorial.html) provides a great resource for
learning how to write functional bioinformatics notebooks. Do you have a
notebook idea that you would like to share?  The epi2melabs-notebook container
is available through dockerhub and is best installed and controlled using the
EPI2ME Labs Launcher.


We look forwards to any feedback and recommendations for new workflows,
tutorials or improvements to our current offerings.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[Managing EPI2ME Labs workflows with the nextflow command-line interface]]></title><description><![CDATA[In this blogpost we explore a few details around the options available when using nextflow to run EPI2ME Labs workflows.]]></description><link>https://labs.epi2me.io/managing-epi2me-labs-workflows-with-the-nextflow-command-line-interface</link><guid isPermaLink="false">https://labs.epi2me.io/managing-epi2me-labs-workflows-with-the-nextflow-command-line-interface</guid><pubDate>Thu, 24 Jun 2021 00:00:00 GMT</pubDate><content:encoded>
Our [EPI2ME Labs Workflows](/wfindex) are designed to provide reproducible analysis
results and be deployed across a whole host of compute platforms. Our
[workflow quick start](/wfquickstart#generic-workflow-instructions) guide shows how to get started with our
workflows to analyse your own data.

In this blogpost we&apos;ll explore a few more details around the options available
when using nextflow to run EPI2ME Labs workflows.

## Running a workflow

All our workflows are available through [github.com/epi2me-labs](https://github.com/epi2me-labs)
and are named with the `wf-` prefix. We also index them [here](/wfindex). To see the options 
available for a workflow, just ask the workflow for help:

    nextflow run epi2me-labs/wf-artic --help

This will list all the available options and give some basic usage guidance. Replace `wf-artic`
in the above with the workflow of choice.

## Managing workflow versions

EPI2ME Labs workflows are actively developed and frequently updated. We follow a continuous
deployment strategy allowing us to provide updates to users with rapid iteration cycles. When
the team develops new features for a workflow and commits these to our mainline development
code branch, the code is automatically tested against example datasets to ensure no breaking
changes have been made. As soon as these tests have passed, the development version of the code
is available to end-users under the &quot;prerelease&quot; revision. These releases are further verified
by our internal scientists before final releases are made.

### Using revisions (`-revision`)

Any EPI2ME Labs users wishing to test out the bleeding-edge prerelease revisions are free
to do so and provide feedback through GitHub issues. To use a prerelease, provide nextflow with
the `-revision` option:

    nextflow run epi2me-labs/wf-artic -revision prerelease

Be aware that after doing this nextflow will use the prelease version by default for all future
runs of the workflow.

To check the version of the workflow currently in use nextflow provides the `info` option:

    nextflow info epi2me-labs/wf-artic

The output of the above will be something like:

    project name: epi2me-labs/wf-artic
    repository  : https://github.com/epi2me-labs/wf-artic
    local path  : /home/user/.nextflow/assets/epi2me-labs/wf-artic
    main script : main.nf
    revisions   :
      master (default)
    * prerelease
      v0.0.2 [t]
      v0.0.3 [t]
    ...
      v0.2.2 [t] 
      v0.2.3 [t]

The asterix (`*`) next to prerelease here indicates that revision is the in-use revision. To
switch back to the current stable release run:

   nextflow run epi2me-labs/wf-artic -revision master

Of course, its possible to set any revision available from the list. This might be useful
if users want to ensure strict reproducibility across workflow runs and do not mind falling
behind the curve in terms of the latest updated. Be aware that `master` always corresponds
to the latest versioned release (e.g. items such as `v0.2.3` in the above).

### Updating the workflow

To manually ensure that the latest workflow code is available nextflow provides the `pull` command,
which should be combined with first ensuring the revision is set as required:

    nextflow run epi2me-labs/wf-artic -revision master 
    nextflow pull epi2me-labs/wf-artic

The incantation above will ensure that the latest release version is available for use
without knowing the name of the release up front.

### Removing workflow code

Sometimes its the case that we want to completely ensure we&apos;re using a clean version of the
workflow code; perhaps the files were currupted or manually edited, or for some other reason
we just want a fresh start in life! Nextflow has the `drop` command for this reason:

    nextflow drop epi2me-labs/wf-artic

This will remove all trace of the workflow code from nextflow&apos;s project cache. To refetch the
code simply run the workflow anew:

    nextflow run epi2me-labs/wf-artic --help

## Summary

EPI2ME Labs leverages the nextflow workflow system to provide rapid software updates and
new functionality whilst also allowing complete reproducibility should users require it.
In this post we have reviewed some of the underlying functionality that nextflow provides
to manage workflow code allowing users to update their workflow version or keep it fixed.</content:encoded><content:thumbnail/></item><item><title><![CDATA[Conda or Mamba for production?]]></title><description><![CDATA[A short analysis of the use of the faster mamba package manager in a production software setting.]]></description><link>https://labs.epi2me.io/conda-or-mamba-for-production</link><guid isPermaLink="false">https://labs.epi2me.io/conda-or-mamba-for-production</guid><pubDate>Fri, 18 Jun 2021 00:00:00 GMT</pubDate><content:encoded> 

Conda is a great choice for installing scientific software, permitting users to manage multiple isolated and reproducible environments. It&apos;s known as a Python package manager, but really it&apos;s a general purpose system that is also highly portable.

- [Conda](https://docs.conda.io/en/latest/) is the package manager.
- [Anaconda](https://anaconda.org/) includes Conda and is the scientific distribution that comes with many packages pre-installed alongside Python.
- [Miniconda](https://docs.conda.io/en/latest/miniconda.html) installs Conda and Python, but it doesn&apos;t include all the extra scientific packages. This makes it ideal for quickly getting a new environment up and running.

To get up and running with Miniconda, the instructions from Bioconda are easy to follow:

- [https://bioconda.github.io/user/install.html](https://bioconda.github.io/user/install.html)

```
# E.g. for linux
curl -O &lt;https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh&gt;
sh Miniconda3-latest-Linux-x86_64.sh

# Installing packages is easy
conda install python=3.8 jupyter -c conda-forge

```

## Workflows in production

Miniconda is great for general purpose use, e.g. in research, but when it comes to moving bioinformatics software into production, there are some extra considerations we have to make.

In order to test and distribute our workflows, we have CI (continuous integration) pipelines set-up to automatically build [docker](https://www.docker.com/) images each time a workflow is updated, which can be many times a day for the development version of a given project.

Hence, we need to think about the following:

- How long does a CI job take? Conda has a reputation for taking its time when dealing with complex sets of dependencies and we owe it to ourselves to make sure that CI jobs don&apos;t take longer than they need to.
- How big will the docker image be? We want to make sure that our images are as small as possible to make it quicker to download for our users. Whilst Miniconda is small as compared with full-fat anaconda, the latest Miniconda3 Linux 64-bit Python 3.9 download size is 58.6 MiB, could this be better?

This is where [Mamba](https://github.com/mamba-org/mamba) comes in, the fast drop-in replacement for conda, which reimplements the slow bits in in C++. Mamba is most akin to Miniconda, in that it comes with Python, but doesn&apos;t ship with a whole load of extra software.

Here&apos;s how to get started with Mamba:

```
# Again, assuming linux
# If you already have conda
conda install mamba -n base -c conda-forge

# Or if you don&apos;t have anything, use miniforge&apos;s mambaforge
wget -O Mambaforge.sh &lt;https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$&gt;(uname -m).sh
bash Mambaforge.sh -b
source mambaforge/bin/activate

# Installing packages is similarly easy
mamba install python=3.8 jupyter -c conda-forge

```

### Speed

How much faster is Mamba? We took the environmental dependencies of the [Medaka](https://github.com/nanoporetech/medaka) project from Bioconda, unpinned their versions (to make it harder for the package manager to solve the problem) and timed how long Miniconda and Mamba took to re-create the environment respectively:

```
m5.12xLarge (48cpus, ~200gigs RAM)
mamba	 1m9.490s
conda	 8m14.317s

```

As we can see, Mamba&apos;s performance is significantly better when dealing with a complex environment like this. The results above aren&apos;t close to being a formal benchmark, but are simply representative of the kinds of gains you can make with mamba out of the box.

However, our workflows usually specify a small to medium number of pinned dependencies. So, is there still a benefit? We re-created the environment specified by our [wf-artic workflow](/wfindex) on three sizes of fresh [AWS ec2 instance](https://aws.amazon.com/ec2/getting-started/) using Mamba and Miniconda and measured the time taken.

```
t2.micro (1cpu, 1gig RAM)
mamba  2m32.842s
conda  killed after 14 mins, didn&apos;t finish

t2.large (1cpu, 8gigs RAM)
mamba  2m4.234s
conda  2m49.810s

m5.12xLarge (48cpus, ~200gigs RAM)
mamba  1m47.622s
conda  2m15.770s

```

In general, even for this relatively small environment, Mamba was approximately half a minute quicker to set everything up. This may seem like a significant step down from the massive gains seen previously, but factor this out over a year&apos;s worth of CI runs and one starts to see how this could be beneficial, especially if you&apos;re paying for CI time.

In addition, on the smallest instance Miniconda failed to complete its task because it ran out of memory and the process was unceremoniously terminated. This may be important if the computers where your CI jobs are running are particularly weak.

### Size

Mamba is definitely faster than Miniconda, but unfortunately it is still quite a wedge to download. Because it depends on Conda, either way of installing it means you end up with both package managers on your system. The download size for the mambaforge package is ~100 MiB, which is a fair bit larger than Miniconda is.

For regular use, this isn&apos;t a big deal, but when you&apos;re trying to create lean docker images in a production environment without requiring lots of space-saving manual intervention, it helps to have as small a download as possible.

This is where [Micromamba](https://aws.amazon.com/ec2/getting-started/) comes into play!

Micromamba is a standalone binary version of Mamba, i.e. with no dependency on Conda, and that doesn&apos;t include a default Python version making it perfect for setting up fresh environments with as small a footprint as possible. Best of all? It&apos;s only ~13 MiB to download.

This means you can save time and effort in building your images.

Here&apos;s how to install Micromamba:

```
# Assuming linux
wget -qO- &lt;https://micromamba.snakepit.net/api/micromamba/linux-64/latest&gt; | tar -xvj bin/micromamba
./bin/micromamba shell init -s bash -p ~/micromamba
source ~/.bashrc

# Installing packages is mostly similar
micromamba activate
micromamba install python=3.6 jupyter -c conda-forge

```

Here&apos;s the catch: Micromamba is still experimental, and lacks some features from Conda. In general, use Mamba in day to day use, and Micromamba in contexts just like the one we&apos;ve been discussing, i.e. building images in CI.

## Conclusions

Mamba is a great drop-in replacement as your daily-driver scientific package manager. In some cases it will significantly speed up your workflow over using Miniconda. However, consider using Micromamba if space or minimalism matters.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs 21.06 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-21.06-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-21.06-release</guid><pubDate>Wed, 16 Jun 2021 00:00:00 GMT</pubDate><content:encoded>
Dear EPI2ME Labs Users

With the June 2021 update to EPI2ME Labs, we are pleased to release a new
workflow, an updated workflow and a comment on the re-invigoration of our
tutorials.

[wf-transcript-target](https://github.com/epi2me-labs/wf-transcript-target) is a our new Nextflow pipeline that reviews and
consolidates transcripts of interest from direct RNA sequencing collections. The
ambition of the workflow was originally to address the presence and fidelity of
expected transcripts from in vitro transcription systems. A FASTQ format
sequence collection is mapped to one or more transcripts of interest and summary
information describing the mapping and coverage characteristics is collated. The
starting sequence reads are also used to prepare a consensus sequence which is
contrasted against the reference sequence.

This workflow can also be used for more general assessment of transcripts of
interest. The figure below shows an excerpt of the HTML report produced when we
look for the [gusb](https://www.ncbi.nlm.nih.gov/gene/2990) &apos;housekeeping&apos; gene in ENA sequence collection [ERX4296980](https://www.ebi.ac.uk/ena/browser/view/ERX4296980).

![Figure 1. Excerpt from the wf-transcript-target report. The workflow collates
information that can be used to assess how well a transcript of interest is
expressed within a direct RNA sequencing library. In this example 15 reads (from
518,000) map to the gusb gene. Summary information on the differences between
the polished consensus sequence and its reference are also shown.
](./report_extract.png)




Our [wf-metagenomics](https://github.com/epi2me-labs/wf-metagenomics) pipeline has been updated to collate additional taxonomic
information from the [Centrifuge](https://ccb.jhu.edu/software/centrifuge/manual.shtml) analyses. This updated workflow will form the
foundations for applied and focused metagenomic workflows that are in planning.
The workflow has also been enhanced so that the Centrifuge HPVC indices (please
see https://ccb.jhu.edu/software/centrifuge/ for more details) can optionally be
downloaded and configured at runtime to simplify the analysis of broad
metagenomic samples.

We are continuing to update and re-invigorate our EPI2ME Labs tutorials
following last month&apos;s update to Jupyter version 3. This housekeeping exercise
is synchronising the usage of libraries and plots between the Nextflow workflows
and the EPI2ME Labs tutorials. The [SARS-CoV-2 ARTIC analysis tutorial](https://labs.epi2me.io/notebooks/SARS_CoV_2_Analysis_Workflow.html) now
includes new functionality such as the Pangolin-based strain identification and
re-establishes functionality parity with [wf-artic](https://github.com/epi2me-labs/wf-artic), our Nextflow pipeline for
SARS-CoV-2 genome analysis.

The [medaka](https://github.com/nanoporetech/medaka) software has been updated to version 1.4.2 - the developments behind
this update are associated with the calling and genotyping of genetic variants;
please see the [changelog](https://github.com/nanoporetech/medaka/blob/master/CHANGELOG.md) for further details.


We look forwards to any feedback and would welcome recommendations for future
content.</content:encoded><content:thumbnail/></item><item><title><![CDATA[Snakemake vs Nextflow]]></title><description><![CDATA[Musing on the considerations when choosing a workflow manager.]]></description><link>https://labs.epi2me.io/snakemake-vs-nextflow</link><guid isPermaLink="false">https://labs.epi2me.io/snakemake-vs-nextflow</guid><pubDate>Sun, 13 Jun 2021 00:00:00 GMT</pubDate><content:encoded> 
These two popular workflow management projects were started in 2014 and 2013 respectively and likely stemmed from a common need to process the increasing quantities of scientific data. They both provide similar solutions to help break down analysis into tasks or processes and link them together in an analysis pipeline.  

Here are some of the main things we considered when deciding which to use for creating our own workflows – 

* [Nextflow](https://www.nextflow.io/) and [Snakemake](https://snakemake.readthedocs.io/en/stable/) both use domain specific language extensions of Groovy and Python respectively. Python is a well-known language among bioinformaticians potentially making Snakemake easier to learn and share. On closer inspection Groovy is an equally elegant Python style language run on Java which for moderately experienced programmers is easy to pick up. A benefit in both cases is the ability to use the underlying languages beyond the domain specification as required.

* Considering the way the pipelines run and execute commands - both enable automatic parallelization of jobs with each process running as soon as an input and computing resources are available. Furthermore, Nextflow will retry jobs that fail automatically, and in Snakemake it is possible to specify a number of retries for each process.

* Both require the user to define inputs for each task to expect and run tasks when inputs are received. Nextflow automates the naming of output folders and files unless specified and outputs may include data objects or in-memory values. The automatically created output folders include logs and other information that can help greatly with debugging.

* Snakemake differs in that the process execution is dependent on the actual input and output file names. The user has to explicitly define output file names and folders, as it is not automated. For a lot of command line tools we use, there are many output files so having to explicitly define each one may add a fair amount of additional code as well as having to think about naming of files.

* One benefit of Snakemake is that is has an option to allow dry test runs without any data and shows which steps would be ran, this can be useful to check the process flow.  With Nextflow you need to use small datasets, the upside is using test datasets can help catch errors early on. We note the [stub feature](https://www.nextflow.io/docs/latest/process.html#stub) was recently introduced into Nextflow which can help in testing workflows and examining their flow.

* Documentation is extensive and clear for both; each can be installed using [Conda](https://docs.conda.io/en/latest/) with a single command and both have easy to follow quick start guides. 

* Whilst the user community for Nextflow is only marginally bigger for Snakemake (going by Github repo stats), the active user community project [nf-core](https://nf-co.re/) collates curated pipelines that use Nextflow is of particular interest to us for the future. 

* A main feature of both is portability, to allow scientists to reproduce analysis on different computing environments with use of virtual environments, container technology and cloud services. Snakemake supports [Docker](https://www.docker.com/) and [Singularity](https://sylabs.io/singularity/) containers, Conda environments and some Amazon Web Services ([AWS](https://aws.amazon.com/)). Nextflow supports all of those as well as additional technologies including [Podman](https://podman.io/), [Charliecloud](https://hpc.github.io/charliecloud/), [Shifter](https://github.com/NERSC/shifter) and more. Nextflow also has specific documentation for AWS batch. Overall Nextflow gives us more choice when it comes to third party software and integrations.

Nextflow and Snakemake both seem like solid choices for developing scientific workflows. There may be cases where working with Snakemake may be preferable and we may use it on occasion but ultimately it made sense for our group to choose one workflow management system to use for the bulk of our work for which we have gone with Nextflow. 

</content:encoded><content:thumbnail/></item><item><title><![CDATA[Locating the Labs Launcher log file]]></title><description><![CDATA[A short guide on locating log files on your computer from the EPI2ME Labs Launcher.]]></description><link>https://labs.epi2me.io/locating-the-labs-launcher-log-file</link><guid isPermaLink="false">https://labs.epi2me.io/locating-the-labs-launcher-log-file</guid><pubDate>Fri, 11 Jun 2021 00:00:00 GMT</pubDate><content:encoded>As much as we&apos;d like the process of running the Labslauncher as seamless as
possible sometimes errors pop up --- whether this be due to something fun in
your local setup or a bug that has snuck in!

![Labslauncher error message](./error1.png)

In order for us to quickly assess the situation when you contact us either
via our github issues page or a customer services representative - please
include your log file.  The log file is an automatic record of any errors
that pop up during the run time of the launcher. Including this file will
make it much easier for us to assess your case as quick as possible!

## Windows 10

First open the windows File Explorer

![Windows explorer open highlighting address bar](./winexp-1.png)

Click the box highlighted above and paste in this file path:

```bash
C:\Users\%username%\.labslauncher
```

![Windows explorer open with path](./winexp-2.png)

This will take you to the correct folder - please include &quot;labslauncher&quot;
file you find in this folder in any help tickets you raise.

![Windows explorer open with path](./winexp-3.png)

## MacOS

Right click the Finder icon then select &quot;Go to Folder...&quot;

![Finder icon](./finder1.png)

This will open the &quot;Go to Folder&quot; dialogue box where you can paste in:

```bash
~/.labslauncher/
```

![Go to Folder dialogue box](./goto_dialogue1.png)

This will open a Finder window at this path where you should see `labslauncher.log`.

![Finder open with the log where it should be](./lablauncher_finder1.png)

## OSX/MacOS/Ubuntu Via Command line

Just as a tip, you can open any path (and most files!) using the `open` command
in terminal.  In order to open the `~/.labslauncher` directory (folder) you can
type the following in `terminal`:

```bash
open ~/.labslauncher/
```

Or if you would like to open up the log itself to read it you can run:

```bash
open ~/.labslauncher/labslauncher.log
```


</content:encoded><content:thumbnail/></item><item><title><![CDATA[Creating interactive, clean and functional graphics with aplanat]]></title><description><![CDATA[An introduction to the aplanat Python package for plotting bioinformatical data.]]></description><link>https://labs.epi2me.io/developing-aplanat</link><guid isPermaLink="false">https://labs.epi2me.io/developing-aplanat</guid><pubDate>Tue, 08 Jun 2021 00:00:00 GMT</pubDate><content:encoded>
import karyogramPlot from &quot;./karyogram.jsx&quot;


When we started creating bioinformatics tutorials for the EPI2ME Labs Notebooks
environment the desire to create interactive plots for our users quickly arose.
There are a variety of plotting libraries available for the Python programming
language, each with various pros and cons. We had some particular requirements:

1. basic interactivity out of the box: things like panning and zooming
1. support for more advanced interactivity like tooltips and data highlighting
1. inline display in JupyterLab notebooks
1. minimal boilerplate code
1. ability to embed plots in a standalone document

The first three of these are fairly self-explanatory; we wanted EPI2ME Labs users
to explore a plot created and displayed in a JupyterLab notebook. Requirement 4.
stems from the fact we did not want our tutorial notebooks to become stuffed
full of extraneous code. The focus of the notebooks was to be how to manipulate
nanopore sequencing data and analyses not how to plot a specific chart. We did
however want to show all the plotting code for our users to follow along.

There were two short-listed candidates to fulfill these aims: [bokeh](https://bokeh.org/)
and [plotly](https://plotly.com/). Bokeh is an open-source, community developed Javascript
library with an associated Python interface:

&gt; [Bokeh&apos;s] goal is to provide elegant, concise construction of versatile graphics,
&gt; and to extend this capability with high-performance interactivity over very
&gt; large or streaming datasets.

Plotly is also an open-source [graphing
library](https://plotly.com/python/) developed and maintained by a company of
the same name which develops also paid-for products.

&gt; plotly.py is an interactive, open-source, and browser-based graphing library for Python.
&gt; Built on top of plotly.js, plotly.py is a high-level, declarative charting library.

From the outside there&apos;s no clear winner between plotly and bokeh, and certainly the fact
that both have large, lively communities around them indicates that both are a good choice
for a modern plotting library in Python.


## Choosing between plotly and bokeh

So which should we use? Aside from subjective arguments around visual appearance
there appears to be little to choose between bokeh and plotly.

After a little research
and prototyping one thing that stands out as a benefit of plotly over bokeh is the 
more high-level nature of its programming interface. For example lets compare the examples
for creating boxplots with each library. This is a common plot type in the field of
bioinformatics where we want to summarize one of more distributions of data.

For this task plotly comes with its `express` interface:

&gt; [Plotly Express](https://plotly.com/python/plotly-express/) is the
&gt; easy-to-use, high-level interface to Plotly, which operates on a variety of
&gt; types of data and produces easy-to-style figures.

Following the [example](https://plotly.com/python/box-plots/) we can produce a plot
across multiple facets of the data with only four lines of code:

![PlotlyBoxPlot](./plotlybox.png &quot;Box Plot Produced by plotly&quot;)

Similarly the [bokeh documentation](https://docs.bokeh.org/en/latest/docs/gallery/boxplot.html)
shows how to produce:

![BokehBoxPlot](./bokehbox.png &quot;Box Plot Produced by bokeh&quot;)

with rather a lot more code. We should say however that bokeh has deliberately
focussed on providing the highest quality plotting primitives rather than bundling
templates for every conceivable plot type. For more templated plots in the vein
of `plotly.express` the [holoviews](http://holoviews.org/index.html) package can be used, the documentation for which
contains a [boxplot example](http://holoviews.org/gallery/demos/bokeh/boxplot_chart.html#demos-bokeh-gallery-boxplot-chart)

So it was primarily for this ease of plotting, from a single software package,
with little boilerplate that we chose plotly for the first prototypes of 
EPI2ME Labs notebooks.


## Reversing our decision

Early users of EPI2ME Labs will remember that we originally pointed people to
[Google Colab](https://colab.research.google.com/) as a front-end to our notebooks.
We detailed in a [previous blog post](/jupytermove) why we 
originally made this recommendation and why we ultimately moved away from Colab.

When using plotly with Colab and EPI2ME Labs we quickly found that plotly would
fail to plot the volume of data that we were requesting of it. These were not large
datasets, and we anticipated that our users would want to plot far larger datasets.
The plotting would often fail with obscure error messages; not helpful when you
are trying to build bioinformatics tutorials for non-specialists.

Bokeh on the otherhand seemed always to reliably produce plots from whatever data
we through at it. It remained responsive with large datasets. So for almost this
reason alone (and because bokeh plots do look nice 😀 ) we reversed our previous decision
and switched to using bokeh.


## Making bokeh useable (for us)

Having been somewhat coerced, but by no means forced, to used bokeh over plotly we
wanted to make it somewhat simple to use for basic plot types, particularly in the context of
JupyterLab notebooks.

At this point we could have tried to use holoviews for our plotting needs, afterall the reason
we chose plotly over bokeh in the first place was the lack of high-level plotting commands which
is what holoviews brings to bokeh. However, after a few days using holoviews we found it
lacking. There wasn&apos;t any big gotcha or flaw in holoviews but just lots of small aspects
where it didn&apos;t really make things too much simpler than using bokeh directly.

So what did we do? Well we did something we wouldn&apos;t ordinarily condone: we made yet
another plotting interface in Python, [aplanat](https://github.com/epi2me-labs/aplanat).
Please forgive the nerdy joke name, it was funny for one afternoon at least!

&gt; Aplanat provides a wrappers (templates) around the bokeh library to simplify
&gt; the plotting of common plots, with a particular focus on producing plots in
&gt; Juypyter notebook environments.


## Developing Aplanat

Making the decision to wrap bokeh in a higher level interface was not taken
lightly.  In doing so we had to consider all the things we didn&apos;t like about
existing plotting libraries (and their are lots of things to dislike), and try
to make our design not fall foul of the same issues.

One quickly realises why the authors of previous efforts ended up with the
designs that they have.  Sometimes its for wanting integration with other
libraries (commonly [pandas](https://pandas.pydata.org/) in Python), or
sometimes for making hard things easy. With aplanat we had a clear goal:

&gt; Aplanat attempts to make constructing common plots as simple as possible by
&gt; translating directly a users inputs into displayed data.

It has the aim to produce plots in a consistent manner with amongst
other behaviours sane defaults, sizing, layouts, and styling. It explicitely
does not try to be too clever with data, though some allowance is made
for plotting grids of plots for multi-facetted data.

### Plotting in aplanat

We won&apos;t detail how to use aplanat here, for one thing we are very conscience
of the fact that there&apos;s no particularly good reason anyone would or should
necessarily want to use it. For example, after the initial development of
aplanat we became aware of [chartify](https://github.com/spotify/chartify),
which serves a similar purpose. We will however take the liberty of highlighting
some of its features which enable a consistent user experience across
EPI2ME Labs notebooks and workflows.

As mentioned above aplanat&apos;s aim was to make simple things simple. To this end
we have plotting functions to plot data in the forms of lines, points, bars,
etc., together with utility functions to perform common data transforms like
kernel density estimates (an alternative to histograms). All plots in aplanat
have a common, simple interface and provide apply a standard set of stylings and
plot attributes which smooth over some (frankly bizarre) default behaviours in
bokeh.

&lt;Bokeh plotJson={karyogramPlot} plotName=&quot;karyogram-plot&quot;
    plotCaption=&quot;Example interactive &apos;karyogram&apos; plot generated with aplanat.&quot;/&gt;

As you can see aplanat also provides an easy where to export plots for embedding
into [React](https://reactjs.org/)-based websites.

### Creating standalone reports

With basic plotting out of the way, a second useful feature of aplanat is
producing standalone, rich, and interactive reports. Gone are the days of
boring static PDF documents. These reports operate without the need for
a webserver: they are single-document HTML files that can be opened
in any webbrowser. All EPI2ME Labs tutorials and workflows produce
a report in this form.

The reporting framework is built around two technologies: bokeh and
[markdown](https://en.wikipedia.org/wiki/Markdown). The combination
of these allows the EPI2ME Labs team to quickly turn a set of bokeh plots
and explanatory text into a report document that summarises the results
of a [tutorial notebook](/nbindex) or a [Nextflow workflow](/wfindex).
We can even embed third-party Javascript report elements like the [Nextclade](https://clades.nextstrain.org/)
QC report element found in the [wf-artic report](/workflows/wf-artic-report.html).

The aplanat reporting framework includes also some templated components,
which gives the team reusability across workflows for standard items like
read quality control, read demultiplexing, and genome coverage. This functionality 
is similar to the ideas of the popular [MultiQC](https://multiqc.info/) tool.

## Summary

The decision to create our own plotting library for EPI2ME Labs products
was taken after prototyping and experimentation with existing tools. Eventually
we settled on building the small aplanat library on top of bokeh. This approach
is not perfect: we still sometimes find ourselves venting anger toward bokeh when
developing new visualizations, but for our daily needs aplanat allows us to
focus on science rather than minutia of creating attractive, useful visualizations.

The EPI2ME Labs team&apos;s goal is to allow anyone, anywhere to analyse their
Oxford Nanopore Technologies&apos; sequencing data.</content:encoded><content:thumbnail/></item><item><title><![CDATA[Q20 single-read accuracy with ultra-long CliveOME dataset]]></title><description><![CDATA[A fresh release of the CliveOME using the latest Q20 pre-release chemistry.]]></description><link>https://labs.epi2me.io/cliveome_2010.05</link><guid isPermaLink="false">https://labs.epi2me.io/cliveome_2010.05</guid><pubDate>Fri, 21 May 2021 00:00:00 GMT</pubDate><content:encoded>
import accPlot from &quot;./accuracy.plt.jsx&quot;
import lenPlot from &quot;./cumlength.plt.jsx&quot;


We are pleased to announce a fresh release of the CliveOME using the latest Q20 pre-release
chemistry.


### Data location

As with previous releases the new dataset is available for anonymous download from
and Amazon Web Services S3 bucket. The bucket is part of the [Open Data on AWS](https://aws.amazon.com/opendata/)
project enabling sharing and analysis of a wide range of data.

The data is located in the bucket at:

    s3://ont-open-data/Q20_ULK_Cliveome/

See the [tutorials](/tutorials/) page for information on downloading the dataset.


### Basecalling

The dataset comprises the direct output of the sequencing device software MinKNOW,
along with basecalls computed post-run using the research-grade [bonito](https://github.com/nanoporetech/bonito) basecaller
with the &quot;Q20 early access model&quot; as follows:

    pip install ont-bonito==0.4.0
    bonito download --models
    bonito basecaller dna_r10.3_q20ea &lt;read directory&gt; | bgzip -c &gt; basecalls.fa.gz

Only reads passing the default quality filter (average Q-score &gt; 10) were processed by
`bonito`, i.e. only those `.fast5` files located within the `fast5_pass` MinKNOW
output folder.

### Data summary

&gt; The sequencing runs here represent data from pre-release versions of the
&gt; sequencing and analysis components. Data throughput and quality do not 
&gt; reflect that of a released product.

The dataset comprises eight PromethION sequencing runs from our R&amp;D lab using pre-release
chemistry components and R10.3 flowcells. A separately prepared sample was run on each
flowcells. The flowcells yielded between 10Gbases and 18Gbases with N50 read lengths between 60-95kb.

Basecalling accuracy was assessed by aligning the reads to the GRCh38 human reference using `minimap2`,
and alignment statistics calculated using the `stats_from_bam` program from the [pomoxis](https://github.com/nanoporetech/pomoxis)
software package.

&lt;Bokeh plotJson={accPlot} plotName=&quot;acc-plot&quot;
    plotCaption=&quot;Basecalling accuracy distribution for Q20 (early access) CliveOME dataset.&quot;/&gt;

&lt;Bokeh plotJson={lenPlot} plotName=&quot;len-plot&quot;
    plotCaption=&quot;Single-molecule read lengths for each of the eight flowcells.&quot;/&gt;
</content:encoded><content:thumbnail/></item><item><title><![CDATA[Rebasecalling of SRE and ULK GM24385 Dataset]]></title><description><![CDATA[An updated basecalling of our previously released GM24385 dataset.]]></description><link>https://labs.epi2me.io/gm24385_2021.05</link><guid isPermaLink="false">https://labs.epi2me.io/gm24385_2021.05</guid><pubDate>Tue, 18 May 2021 00:00:00 GMT</pubDate><content:encoded>
import accComparePlot from &quot;./accuracy.compare.jsx&quot;
import qComparePlot from &quot;./qscore.compare.jsx&quot;
import qFilterPlot from &quot;./qscore.filter.jsx&quot;

We are please to announce the addition of Guppy 5 basecalling results to our
short-read eliminator (SRE) and ultra-long (ULK) [GM24385
dataset](/gm24385_2020.11).

### Data location

As with previous releases the new dataset is available for anonymous download from
and Amazon Web Services S3 bucket. The bucket is part of the [Open Data on AWS](https://aws.amazon.com/opendata/)
project enabling sharing and analysis of a wide range of data.

The data is located in the bucket at:

    s3://ont-open-data/gm24385_2020.11/

See the [tutorials](/tutorials/) page for information on downloading the dataset.


### Rebasecalling

The Guppy 5 basecalling for the GM24385 dataset was performed using version
5.0.6, driven by the same [katuali](/katuali_human_pipeline/) analysis pipeline
as for the initial dataset release.  The new basecaller version was provided as
input the per-chromosome `.fast5` files created in the initial pipeline via
alignment of the Guppy 4.0.11 basecalls. This allows for easy comparison of
results on subsets of the data (but may lead to subtle side-effects). For
example the analysis data structure contains now entries of the form:

    gm24385_2020.11/analysis/r9.4.1/{flowcell}/guppy_v4.0.11_r9.4.1_hac_prom/align_unfiltered/{chromosome}/guppy_v5.0.6_r9.4.1_sup_prom/
    ├── align_unfiltered
    │   ├── align_to_ref.log
    │   ├── basecall_stats.log
    │   ├── calls2ref.bam
    │   ├── calls2ref.bam.bai
    │   └── calls2ref_stats.txt
    ├── pass
    ├── fail
    ├── basecalls.fastq.gz
    └── sequencing_summary.txt

The file `basecalls.fastq.gz` contains the reads passing Qscore filtering from
the new Guppy version (the pass and fail subfolders contain the reads split by
this criteria). Similar to the main folder structure the `align_unfiltered`
directory contains unfiltered alignments of the basecalls to the reference
sequence (`calls2ref.bam`) along with text files summarizing the properties of
the alignments.

### Accuracy improvement from the new basecaller

Guppy 5.0.6 implements the CRF-CTC models developed in the research-grade [bonito](https://github.com/nanoporetech/bonito)
basecaller. The previous Guppy 4.0.11 basecalls leveraged the &quot;flip-flop&quot; algorithm developed
in the [flappie](https://github.com/nanoporetech/flappie) research caller. For this
dataset we have chosen to the use the highest accuracy &quot;super&quot; basecalling model.
This is an optional model now available in Guppy, replacing the &quot;high&quot; accuracy
line as the models providing the state-of-the-art accuracy. The &quot;high&quot; accuracy
line remains the default, standard choice providing a good balance of accuracy 
and compute performance.

In order to compare the single-read accuracy of the two basecallers, we used the
alignment summaries produced by the [katuali pipeline](/katuali_human_pipeline).
The two alignment summary tables were joined on read ID to allow simple before
and after comparison of all reads:

&lt;Bokeh plotJson={qComparePlot} plotName=&quot;qScoreCompare&quot;
  plotCaption=&quot;Pairwise comparison of single-read accuracies for Guppy 5.0.6 and previous Guppy 4.0.11 basecalling. Majority of points lie above the black diagonal line indicating an improvement in call quality.&quot; /&gt;

The plot shows that a general improvement in read accuracy of around 1.8
percentage points, or a 32% reduction in the error rate. To see the change in
the average behaviour it is perhaps clearer to plot a density estimate of the
read accuracy distribution:

&lt;Bokeh plotJson={accComparePlot} plotName=&quot;accComparePlot&quot; plotCaption=&quot;Guppy 5.0.6 reduces read error rate by 32% compared
with previous Guppy 4.0.11 basecalling.&quot; /&gt;

### Changes to Qscore filtering

One consideration with the newer Guppy basecaller is that the default Qscore threshold
for partitioning &quot;pass&quot; and &quot;fail&quot; reads was amended in Guppy v4.5.2 (released 07/04/2021). The
rebasecalling here started from reads classed as &quot;pass&quot; by Guppy v4.0.11. It is expected and observed
that after rebasecalling a proportion of the data is now classified as failed. With improvements
to the basecaller the effect is to remove between 10-15% of data; it should be noted that the
data which is removed is of course the lower accuracy data and that end users are free to use 
both the pass- and fail-reads.


&lt;Bokeh plotJson={qFilterPlot} plotName=&quot;qFilterPlot&quot;
  plotCaption=&quot;Guppy v4.5.2 brought an increased Qscore threshold for determining pass and fail reads. This is carried into Guppy v5.0.6.&quot; /&gt;


### Further information

The 5.0.6 version of Guppy used here is a development release candidate version
of the software. The final release version will provide very similar results
to the version used here. Further details of the final release are available 
in the [Nanopore Community](https://community.nanoporetech.com/posts/guppy-v5-0-7-release-note) pages.

We hope that these data and analyses provide a useful resource to the community.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs 21.05 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-21.05-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-21.05-release</guid><pubDate>Mon, 17 May 2021 00:00:00 GMT</pubDate><content:encoded>
Our May 2021 update to EPI2ME Labs brings a smörgåsbord of fixes, new
functionality and a colourful new project website.


### EPI2ME Labs
*Jupyter3 upgrade and maintenance to the existing tutorials*

The EPI2ME Labs software hosts the tutorials from a Jupyter server. With this
month&apos;s release, the EPI2ME Labs Docker containers have been updated to include
the JupyterLab version 3.0.5 software. This upgrade from 2.2.9 brings a number
of interface improvements and helps EPI2ME Labs deliver cutting-edge data
science functionality. The existing tutorials have been updated to accommodate
these changes and the [IGV genome browsing tool](https://github.com/epi2me-labs/igv-jupyterlab) that we maintain now works with
this version of the Jupyter software.



### EPI2ME Labs workflows
*Updates and maintenance to the wf-hap-snp, wf-artic and
wf-alignment workflows*

The EPI2ME Labs workflows have minor maintenance updates. The changes are
largely aimed to the standardisation of the canonical workflow; we are aiming
to use input FASTQ sequence data from a directory that may contain one or more
files that may be compressed and/or barcoded. Other novel functionality in this
release includes:

* The [wf-artic](https://github.com/epi2me-labs/wf-artic) workflow can now report genotype information for the SARS CoV-2
spike protein&apos;s genetic variants of concern
* [wf-alignment](https://github.com/epi2me-labs/wf-alignment) has been updated to include a demultiplexing step when processing barcoded sequence content
* [wf-hap-snps](https://github.com/epi2me-labs/wf-hap-snps) has improved reporting and a standardised report name
* [wf-demultiplex](https://github.com/epi2me-labs/wf-demultiplex) is a demonstration workflow that shows how Guppy demultiplexing
may be included as a step in Nextflow pipelines


### EPI2ME Labs components
*New tools and updates*

The EPI2ME Labs and EPI2ME Labs workflows use a collection of
actively-maintained packages that provide functionality for simplifying and
abstracting common tasks. The [aplanat](https://github.com/epi2me-labs/aplanat) Python package provides the toolkit for
the preparation of graphs, charts and tables that are presented in our reports.
[Mapula](https://github.com/epi2me-labs/mapula) is a Python package that provides observational insight from streamed
mapping data (e.g. mapping quality and alignment identity). With this update we
introduce two new tools to simplify sequence analysis in both tutorials and
workflows.

* [fastcat](https://github.com/epi2me-labs/fastcat) is a tool to identify and stream FASTQ-format sequences into downstream
tools (e.g. alignment or de novo assembly) whilst harvesting basic sequence
characteristics. This allows workflows to accommodate input sequences from
creative directory structures and accommodates files that may be gzip
compressed.
* [ncbitaxonomy](https://github.com/epi2me-labs/ncbitaxonomy) is a lightweight tool featuring just the taxonomic
classification functionality from the [ETE3](http://etetoolkit.org/) toolkit. The ambition of this tool
is the provision of functionality for assessing sequence collections in
relation to overlaid information from the [NCBI taxonomy database](https://www.ncbi.nlm.nih.gov/taxonomy) - this has
applications in WIMP- and 16S-like workflows that address metagenomic
classification such as [wf-metagenomics](https://github.com/epi2me-labs/wf-metagenomics).


### Research software releases
*Medaka v1.3.3*

Updated functionality for reporting reference (non-variant) alleles of interest
in medaka variant Minor fix to medaka consensus; other changes and fixes are
documented in the [changelog](https://github.com/nanoporetech/medaka/blob/master/CHANGELOG.md).


*We look forward to any feedback and would welcome ideas and insight for
tutorials and workflows that would benefit your research.*

</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs Blog Revamp]]></title><description><![CDATA[A short post describing the refresh and consolidation of the EPI2ME Labs and ONT Open Datasets blog sites.]]></description><link>https://labs.epi2me.io/epi2me-labs-blog-revamp</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-blog-revamp</guid><pubDate>Mon, 17 May 2021 00:00:00 GMT</pubDate><content:encoded>
Previous users of this webpage will have noticed things have changed a little.
Gone is the purely functional style, replaced with a very more more vibrant
look besetting of the rapid software development undertaken of the EPI2ME Labs
team.

Our new site aims to consolidate the previous EPI2ME Labs site with the Oxford
Nanopore Open Data site into a single unified confluence of open science from
ONT. We have arranged the homepage to provide quick access to the three major
offerings:

* [**EPI2ME Labs Notebooks**](/nbindex) - our integrated bioinformatics analysis environment
  powered by JupyterLab
* [** EPI2ME Labs Workflows**](/wfindex) - our selection of tailored Nextflow analyses for
  Nanopore sequeuncing data, scalable from running on a desktop to running in the
  cloud.
* [** Oxford Nanopore Open Dataset**](/dataindex) - a selection of datasets published by Oxford
  Nanopore Technologies from in-house sequencing of selected samples.

We have quick links to each of these sections at the top and bottom of the website.


Finally, should you like any of our content and wish to share it on social media,
we have added quick links to do this at the bottom of each blog entry.

![social](./social.png &quot;Share content easily on social media&quot;)

Blog posts are now also categorised for easy browsing of related content. At the
bottom of all posts you will also now find a selection of related posts for
further reading. The categories are displayed on the home page as well as being 
navigable from posts in each category; just click the colour tab at the top of 
any posts to see more posts in that category

![category](./category.png &quot;Posts are collected in categories&quot;)

Together with broad categories we are now tagging posts with additional themes
to futher collect together content.

![tags](./tags.png &quot;Posts are also tagged with themes&quot;)

We hope you like the look of our new site and that it make our content easier
to find an share.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[Running EPI2ME Labs Notebooks on Remote Computers]]></title><description><![CDATA[A short introduction to running EPI2ME Labs tutorials on a remote networked computer.]]></description><link>https://labs.epi2me.io/running-epi2me-labs-notebooks-on-remote-computers</link><guid isPermaLink="false">https://labs.epi2me.io/running-epi2me-labs-notebooks-on-remote-computers</guid><pubDate>Wed, 17 Mar 2021 00:00:00 GMT</pubDate><content:encoded>

This blog post is a short introduction to running EPI2ME Labs tutorials on a remote machine that you have access to.  
This means you can still use the straight forward graphical interface for running the applications while utilising the 
greater compute power available on these remote servers.  
This is useful for those who have a separate computer that is more powerful than the one they&apos;re currently operating. 
This is a common setup in research institutions where you may have a less powerful work laptop and access to a much more 
powerful server for running more intensive software. (It is also possible to use this to bypass more stringent network controls!)

To do this we will be using `ssh`.  SSH (Secure Shell) is a method that enables you to run commands on a remote machine from your local machine safely.
This can be achieved using an ssh client -  a program that can establish this kind of connection. UNIX-style systems e.g. 
OSX/ubuntu etc can create an SSH tunnel via the command ssh.  
If you habitually run software on a separate server then you will already be familiar with this command to connect to a terminal on your server:

```bash
# ssh &lt;server&gt;

ssh myserver
```

With some additional arguments you can set this up so that information that is exposed on a particular port is presented on your host 
machine as if it were your remote machine. 

### Setting up an SSH Tunnel for EPI2ME Labs

It is important that the port that you provide is not currently being used by something else. 
This means that if you are running the EPI2MELabs Launcher on your local machine then you will need to change some of the configuration 
if you want to run the remote version at the same time.  This will be discussed below.

EPI2MELabs Launcher by default uses two ports, 8888 and 8889.  
This can be customised in the GUI version of the launcher in the settings/preferences dialogue.  
We build on the first ssh command by adding a couple of command line args and port information.  
This:

* `-X` or `-Y`: Tells your remote machine that you want the application displays from your remote machine on your local machine (X11 forwarding).  Please note that -X is more secure but might render things slower, -Y is faster but less secure, you should only use this on servers that you trust as much as your local machine.
* `-L` Connect specific ports e.g.  so that when you look at 8888 in your browser you are actually looking at what 8888 is displaying -on your remote machine. 

```bash
# -X Enables X11 forwarding
# -L Which port/sockets to on the local (client) host are to be forwarded to the remote machine
# destination 

ssh -X -L 8888:localhost:8888 -L 8889:localhost:8889 myserver
```

### Troubleshooting (skip this step if launcher window loads correctly)

If you run this command and the launcher window does not appear on your host machine and you get the error message:
```bash
Error: Can&apos;t open display: 
```
This means that is isn&apos;t sure where to display the GUI window.  
One way to achieve this is to tell the terminal that you want your GUI to display on your monitor.  
We do this by assigning a value to the environment variable DISPLAY.  
Because we only want this value to change while running this command and not change the environment 
variable on your host machine in general we don&apos;t export the variable but prepend the assignment 
to the ssh command (see example below).
```
# &lt;VARIABLE&gt;=&lt;VALUE&gt; &lt;COMMAND&gt;
DISPLAY=:0 ssh...
So the final command will be:
```
&gt; **_NOTE:_** Remember to replace myserver with your server address!

```
DISPLAY=:0 ssh -Y -L 8888:localhost:8888 -L 8889:localhost:8889 myserver
```
If this doesn&apos;t work please contact your local IT department as there may be custom configurations peculiar to your setup to be made.

###  Navigating to the EPI2ME Labs landing page
Then once you are on the remote machine, run the launcher as you would do normally. You can run the headless or GUI version of the launcher and connect to it like you would normally in your browser.

Open your web browser and go to:

(https://localhost:8888)[https://localhost:8888]

If your launcher is running you should be now faced with the jupyter login page.  You need to enter the token for your launcher.

&gt; **_NOTE:_** The default token is &quot;EPI2MELabs&quot; however we recommend that you change this when appropriate.

</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs Launcher Command-line interface]]></title><description><![CDATA[By popular request, an introduction to the new command-line interface for managing EPI2ME Labs notebook servers.]]></description><link>https://labs.epi2me.io/epi2me-labs-launcher-command-line-interface</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-launcher-command-line-interface</guid><pubDate>Wed, 17 Mar 2021 00:00:00 GMT</pubDate><content:encoded>


Feedback from EPI2ME Labs users revealed the desired for people to be able to control the
running of the EPI2ME Labs notebook server from the command-line without opening a
graphical user interface. Such users are typically running the server on a different,
and more computationall capable, computer from where they are using the notebook 
web interface.

With the [v1.0.8 release](/downloads) of the launcher we have introduced the ability
to start, stop, and check the status of the EPI2ME Labs notebook server from the
command-line. The application is install to

    /usr/local/bin/EPI2ME-Labs-Launcher/EPI2ME-Labs-Launcher

on Linux systems. On Ubuntu systems this is aliased as two commands, `labslauncher` and
`labsmanager`, installed to:

    /usr/local/bin

which is typically listed in the `PATH` environment variable.

Running the `EPI2ME-Labs-Launcher` (or `labslauncher` shortcut) will run the standard
EPI2ME Labs Launcher with a graphic interface, and with logging to the terminal. However
running:

    /usr/local/bin/EPI2ME-Labs-Launcher/EPI2ME-Labs-Launcher manager --help

(or simply `labsmanager --help` on Ubuntu) will initiate a simple command-line interface.
The interface has the following commands:

```
  --start      Start the server.
  --restart    Restart the server.
  --stop       Stop a running server and cleanup resources.
  --status     Report server status.
  --update     Update server.
```

all of which should be fairly self-explanatory. The program will alert users if
updates are available for the server or for the program itself. The last
command in the above can be used to update the notebook server component. For
updates to the Launcher application users should continue to download these
from the [downloads](/downloads) page. (Users on Oxford Nanopore Technologies
sequencing devices can also upgrade the Launcher through `apt`).

When starting an EPI2ME Labs notebook server the full array of options
available in the graphical application are available to be set; running the
`--help` command will display these. In contrast to the graphical interface,
when using the command-line options are not stored in the applications settings
for subsequent use: users wishing to override the default value of options must
supply these options every time the program is run. This provides a more
natural command-line interface.

We hope that users will find the new command-line interface useful and look
forward to feedback.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs Embraces Nextflow]]></title><description><![CDATA[Background article on the choice of Nextflow for authoring bioinformatics workflows for nanopore sequencing.]]></description><link>https://labs.epi2me.io/epi2me-labs-embraces-nextflow</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-embraces-nextflow</guid><pubDate>Wed, 10 Mar 2021 00:00:00 GMT</pubDate><content:encoded>

We are excited to launch a new bioinformatics offering using the [Nextflow](https://www.nextflow.io/)
reactive workflow framework. Nextflow has been selected as a preferred
framework because of its integration with container technologies, software
package managers and its scalability to cluster- and cloud-scale installations.
Nextflow also has growing user adoption through projects such as nf-core. These
advantages will help us deliver varied workflows with minimal requirements for
the installation of additional software.



* [wf-artic](https://github.com/epi2me-labs/wf-artic) packages the [ARTIC
  Fieldbioinformatics](https://github.com/artic-network/fieldbioinformatics)
  software in a convenient containerised package that can be used to locally
  process multiplexed SARS-CoV-2 sequence data in a more automated manner.
* [wf-hap-snps](https://github.com/epi2me-labs/wf-hap-snps) is a workflow designed to perform haploid SNP calling from
  whole genome sequencing of haploid samples.
* [wf-alignment](https://github.com/epi2me-labs/wf-alignment) packages the [minimap2](https://github.com/lh3/minimap2) software and streamlines the
  process of mapping sequence reads to a reference genome and preparing summary
  statistics. It can also analyse the abundance of known molarity control
  experiments and use this information to derive the abundances of other species
  present in the sample.
* [wf-metagenomics](https://github.com/epi2me-labs/wf-metagenomics) includes the [Centrifuge](https://ccb.jhu.edu/software/centrifuge/) software and appropriate
  indexes to facilitate the taxonomic classification of sequence reads from
  metagenome samples.

Our Nextflow workflows are intended to build upon the best-practice workflows
demonstrated in our [EPI2ME Labs tutorials](/nbindex). The workflows use the same
methods demonstrated in the tutorials but should be more appropriate and
scalable for larger and/or multiplexed datasets. The Nextflow workflows are
distributed via GitHub and we hope that users will fork and customise the
repositories.

The Nextflow workflows all prepare reports that include summary statistics,
tabular data and figures to describe the sequences processed and to highlight
important features of the results obtained. Example reports for each of the
workflows are available from the [Notebook Index](/nbindex).

We support the installation and usage of these workflows on our GridION
sequencing devices; the workflows have also been tested on a variety of Linux
platforms.

We look forwards to feedback and would [welcome requests](https://github.com/epi2me-labs/epi2me-labs.github.io/discussions/) for workflows and
tutorials to be included in future updates.


![ARTIC Workflow Report](./fig1.png &quot;Extract of the wf-artic workflow report&quot;)

**Figure 1.** An excerpt from the wf-artic report; the information shown in the
plots can be used to assess depth-of-coverage across the sequenced primer sets
and to identify primers and amplicons that may have dropped out of the
analysis. Additional data and their provenance are shown to further summarise
the data and SARS-CoV-2 variants contained within. The dataset summarised in
this sample report is from sequencing study [PRJNA650037](https://www.ncbi.nlm.nih.gov/bioproject/?term=PRJNA650037).

</content:encoded><content:thumbnail/></item><item><title><![CDATA[Workflow Command-line Usage]]></title><description><![CDATA[A short guide to getting started with EPI2ME Labs Nextflow workflows with instructions for installation on Oxford Nanopore Technologies' sequencing devices and Linux and MacOS.]]></description><link>https://labs.epi2me.io/wfquickstart</link><guid isPermaLink="false">https://labs.epi2me.io/wfquickstart</guid><pubDate>Sun, 07 Mar 2021 00:00:00 GMT</pubDate><content:encoded>

EPI2ME Labs workflow are built using the [nextflow](https://www.nextflow.io/) workflow
framework:

&gt;*Nextflow enables scalable and reproducible scientific workflows using
&gt;software containers. It allows the adaptation of pipelines written in the most
&gt;common scripting languages.*
&gt;
&gt;*Its fluent DSL simplifies the implementation and the deployment of complex
&gt;parallel and reactive workflows on clouds and clusters.*


## Prerequisites

In this guide we will walkthrough usage of EPI2ME Labs Nextflow workflows from
the command-line on either Linux, macOS or, Windows through WSL2. It is assumed
that Nextflow has been installed either as part of the EPI2ME Labs desktop application,
or in the case of macOS and Linux installed as in our [Installation guide](/installation).

EPI2ME Labs workflows can currently be run using either
[Docker](https://www.docker.com/products/docker-desktop), 
[Singularity](https://sylabs.io/docs/) or
to provide isolation of the required software. Each of these methods is automated out-of-the-box provided
Docker or Singularity is installed.

&gt; EPI2ME Labs workflows no longer support conda as a means of managing their
&gt; software environments. This decision was taken after encountering many
&gt; issues (both ourselves and from end-users) while using conda for dependency
&gt; management.

## Generic Workflow instructions

The code behind all EPI2ME Labs workflows is hosted publically on our Github
space: https://github.com/epi2me-labs/. Workflow projects are prefixed with `wf-`.
For example the code powering our [ARTIC](https://github.com/artic-network/fieldbioinformatics)-based
SARS-CoV-2 analysis is available at: https://github.com/epi2me-labs/wf-artic.

For the most part, users will not need to interact directly with the Github
code repositories as Nextflow has the ability to automatically manage 
workflows available on Github. 

***The instructions below are provided using [wf-artic](https://github.com/epi2me-labs/wf-artic) as an examplar workflow,
for other workflow simple replace `wf-artic` with `wf-name-of-workflow`.***

### Downloading and Running Workflows

With the prerequisites installed, users can run:

    nextflow run epi2me-labs/wf-artic --help

to see the options for a specific workflow. The help message will display all
common options available for augmenting the workflows behaviour. See 
*Configuration and tuning* below for information regarding manipulating
how workflows are run.

To run the workflow using Docker containers supply the `-profile standard`
argument to `nextflow run`:

*The command below uses test data available from the [github repository](https://github.com/epi2me-labs/wf-artic/tree/master/test_data)
It can be obtained with `git clone https://github.com/epi2me-labs/wf-artic`.*

```
# run the pipeline with the test data
OUTPUT=my_artic_output
nextflow run epi2me-labs/wf-artic \
    -w ${OUTPUT}/workspace
    -profile standard
    --fastq test_data/sars-samples-demultiplexed/
    --samples test_data/sample_sheet \
    --out_dir ${OUTPUT}
```

### Configuration and tuning

&gt; This section provides some minimal guidance for changing common options, see
&gt; the [Nextflow documentation](https://www.nextflow.io/docs/latest/config.html) for further details.

The default settings for the workflow are described in the configuration file `nextflow.config`
found within the git repository. The default configuration defines an *executor* that will 
use a specified maximum CPU cores (four at the time of writing) and RAM (eight gigabytes).

If the workflow is being run on a device other than a GridION, the available memory and
number of CPUs may be adjusted to the available number of CPU cores. This can be done by
creating a file `my_config.cfg` in the working directory with the following contents:

```
executor {
    $local {
        cpus = 4
        memory = &quot;8 GB&quot;
    }
}
```

and running the workflow providing the `-c` (config) option, e.g.:

```
# run the pipeline with custom configuration
nextflow run epi2me-labs/wf-artic \
    -c my_config.cfg \
    ...
```

The contents of the `my_config.cfg` file will override the contents of the default
configuration file. See the [Nextflow documentation](https://www.nextflow.io/docs/latest/config.html)
for more information concerning customized configuration.

### Updating workflows

Periodically when running workflows, users may find that a message is displayed
indicating that an update to the workflow is available.

To update the workflow simply run (for e.g. the `wf-artic` workflow):

    nextflow pull epi2me-labs/wf-artic

### Managing disk space

When running workflows with large data sets, intermediate steps can take up 
considerable disk space so it may be worth assigning the work(`-w`) and 
output(`--out_dir`) parameters to a directory with plenty of disk space.
When working on a GridION this may be on a mounted drive.

After running a few workflows you may want to clear up intermediate
and log files created as part of the workflow that are stored
in the work directory.

To clean up the work directory, from the directory where you ran your cmd run: 

    nextflow clean -f 


## Building the docker container from source

The docker images used for running the EPI2ME Labs workflows are available on
[dockerhub](https://hub.docker.com/repository/docker/ontresearch/).
The image is built from the Dockerfile present in the git repository. Users
wishing to modify and build the image can do so with:

```
CONTAINER_TAG=ontresearch/wf-artic:latest

git clone https://github.com/epi2me-labs/wf-artic
cd wf-artic

docker build \
    -t ${CONTAINER_TAG} -f Dockerfile \
    --build-arg BASEIMAGE=ontresearch/base-workflow-image:v0.1.0 \
    .
```

In order to run the workflow with this new image it is required to give
`nextflow` the `--wfversion` parameter:

```
nextflow run epi2me-labs/wf-artic \
    --wfversion latest
```
## Tidying up docker

If you want to remove old docker images. Type `docker images` to find 
image id of image to delete and then `docker rmi &lt;image-id&gt;`.

If you want to remove old docker containers. Type `docker ps -a` to find
container id and then `docker rm &lt;container-id&gt;`.


## Useful links

* [nextflow](https://www.nextflow.io/) The workflow management system used by EPI2ME Labs workflows.
* [docker](https://www.docker.com/products/docker-desktop) A software container platform that can be optionally used by EPI2ME Labs.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs 21.02 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-21.02-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-21.02-release</guid><pubDate>Wed, 10 Feb 2021 00:00:00 GMT</pubDate><content:encoded>

We are delighted to release this month&apos;s update to EPI2ME Labs. The 2021-02.01
release addresses some Community feedback and introduces new and updated
tutorials.

**ERCC spike-in quantification**

A new tutorial to assess performance of a transcriptomics study through the use
of spike-in controls.

Our ERCC tutorial introduces the synthetic sequencing controls developed by the
External RNA Controls Consortium (ERCC). These spike-in control sequences are
explored within an expression profiling study and demonstrate how the
performance of a transcriptomics study may be benchmarked.


![ERCC Notebook](./ercc_screenshot.png &quot;Evaluating observed counts of spike-in control samples.&quot;)

**Figure 1.** The ERCC tutorial introduces a workflow that looks for the
presence of synthetic RNA transcripts and considers their observed abundance.
The correlation between the observations of each ERCC with its expected counts
can be used to assess study performance. The figure above shows some basic
count measurements that are presented during the analysis.

**SARS-CoV-2 ARTIC update**

The SARS-CoV-2 tutorial introduces the [ARTIC
FieldBioinformatics](https://github.com/artic-network/fieldbioinformatics)
workflow for the assembly and polishing of SARS-CoV-2 genomes.  The workflow
has been updated to demultiplex sequence collections using an expanded range of
both native and rapid barcoding kits - the workflow has been tested with the
&quot;Eco&quot; version of the PCR tiling of SARS-CoV-2 virus protocol. We appreciate
that some users may prefer to perform demultiplexing during sequencing in
MinKNOW; the tutorial now optionally processes FASTQ sequences that have
already been demuliplexed. The tutorial includes a new and more interesting set
of example data that have been picked from project PRJNA650037 at the [European
Nucleotide Archive](https://www.ebi.ac.uk/ena/browser/view/PRJNA650037).


![ARTIC Notebook](./sars_cov_bcs.png &quot;QC of demultiplexed ARTIC amplicon data.&quot;)

**Figure 2.** The SARS-CoV-2 ARTIC notebook walks users through the assessment
of their ARTIC amplicon data to assess multiplexing efficiency. The final output
of the workflow is a single FASTA formatted file containing a consensus sequence
for each sample suitable for upload to [GISAID](https://www.gisaid.org/)
or inspection with [NextClade](https://clades.nextstrain.org/).

**EPI2ME Labs Launcher support on more computer platforms**

In addition to our previously packaged installations, we are now also packaging
the EPI2ME Labs Launcher for Centos 7, Centos 8 and Ubuntu 20.04. The complete
set of EPI2ME Labs Launcher software can be found at
https://labs.epi2me.io/downloads/.

**Command-line interface (CLI) to Labs Launcher**

EPI2ME Labs has been installed on a variety of computer platforms. User
feedback has revealed that there is interest in the installation of the EPI2ME
Labs container on headless servers that offer greater memory and CPU resources.
To support the installation of EPI2ME Labs on these more powerful computer
systems, the EPI2ME Labs Launcher has been updated to include CLI functionality
to start, stop, and update the appropriate services. More information on the
CLI interface to the EPI2ME Labs Launcher can be found in a [separate blog
post](/launcher_cli).

**Maintenance, updates and utility**

The software environment in the epi2melabs-notebook has been updated to use
Python version 3.8 (from Python 3.6). Housekeeping during the preparation of
the docker container has considerably reduced the size of the container
download - the download is now only approximately 1.3G in size. To increase the
legibility of the functional code within the EPI2ME Labs tutorials and
notebooks, we are introducing new software packages to simplify and structure
analyses. [aplanat](https://github.com/epi2me-labs/aplanat)
has been used in earlier tutorials to simplify the
preparation of figures - in this release we are introducing
[mapula](https://github.com/epi2me-labs/mapula) to simplify the mathematics
of processing read statistics and count information from BAM alignment files.


We look forwards to your feedback. We would welcome suggestions for future
tutorials and we would gratefully review Community contributions.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs Blog Launch]]></title><description><![CDATA[Welcome message to our new blog site.]]></description><link>https://labs.epi2me.io/epi2melabs-blog-launch</link><guid isPermaLink="false">https://labs.epi2me.io/epi2melabs-blog-launch</guid><pubDate>Thu, 03 Dec 2020 00:00:00 GMT</pubDate><content:encoded>

Welcome!

On this site we hope to bring additional information regarding the EPI2MELabs notebook
environment. We will present user guides, help pages and demonstrations of the available
notebooks. All in an open setting.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs 20.11 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-20.11-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-20.11-release</guid><pubDate>Thu, 03 Dec 2020 00:00:00 GMT</pubDate><content:encoded>
We are pleased to introduce a new release of our EPI2ME Labs.


We are delighted to introduce new tutorials, updated functionality, and cleaner
aesthetics with our 2020-11.01 release of EPI2ME Labs. This is available
immediately, a new LabsLauncher is available on the [downloads](/downloads) page.


In this release we have include IGV.js. IGV is a leading track-based genome
browser. Bringing interactive genome browsing into our notebooks enables
clearer illustration of results and opens new possibilities for customer
developed interactive applications. The JupyterLab [IGV extension](https://github.com/epi2me-labs/igv-jupyterlab)
is available as a standalone component outside of EPI2MELabs.


![IGV Webviewer](./igv-view.png &quot;Viewing alignments with the IGV extension&quot;)

**Figure 1.** Representation of the SNP benchmark tutorial that shows an IGV view
of the genome and the variants contained within. Bringing this popular genome
viewer into notebooks makes data interpretation more intuitive.


The 2020-11.01 update integrates also new benchmarking tutorials with our recent
[s3://ont-open-data](https://nanoporetech.github.io/ont-open-datasets/)
releases. These benchmark tutorials help users
replicate our performance metrics and demystify the steps involved in
benchmarking SNP and SV detection performance.

A benchmarking analysis of human structural variants is illustrated using SVs
called from our GM24385 dataset using the updated [pipeline-structural-variation](https://github.com/nanoporetech/pipeline-structural-variation)
v2.0.2 software. This tutorial introduces the
SV truth sets maintained by the Genome in a Bottle consortium and demonstrates
a workflow for measuring performance and visually assessing apparent
false-positive SVs.

The same GM24385 dataset is also used for benchmarking our
performance in identifying human single nucleotide variants with a workflow
based on Medaka and DeepVariant. Further analysis of the called
variants illustrates the challenges of identifying SNVs in various genomic
contexts.


![Precision and Recall](./precrec.png &quot;Our benchmark notebooks detail the concepts and processes involved.&quot;)

**Figure 2.** The benchmarking tutorials included in this release use known and
validated datasets to assess the performance of methods used to identify SNPs
and SVs. The tutorials introduce the scoring of true positives, false positives
and false negatives. The tutorials demonstrate the calculation of precision and
recall using appropriate software packages.


Bug fixes and improvements to the core EPI2ME Labs environment include:

* Numerous minor fixes and amendments to notebooks.
* Fixed the export of graphs
produced in notebooks running on macOS.
* Improvements to the handling of the Pavian webservice used in our metagenomics notebook.
* Updates to the caching and storage of notebooks allows for simpler .ipynb support.
* Aesthetic changes to make the Jupyter server experience a little less orange.


We hope that you find these updates useful. The EPI2ME Labs team would love to
collaborate in the development of new workflows that showcase your software and
we would welcome recommendations for new tutorials and notebooks.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[Structural variation calling with GM24385]]></title><description><![CDATA[An exploration of structural variant calling with our HG002/GM24385 Askenazi Son data release.]]></description><link>https://labs.epi2me.io/gm24385_2020.11_sv</link><guid isPermaLink="false">https://labs.epi2me.io/gm24385_2020.11_sv</guid><pubDate>Tue, 01 Dec 2020 16:02:00 GMT</pubDate><content:encoded>
In this blog post we will explore structural variant calling using the
[recently released](/gm24385_2020.11) HG002 (GM24385 Ashkenazi Son) data
release.

The GM24385 dataset comprises whole genome sequencing of a well-characterised
human cell line. It therefore provides a useful benchmark sample; the cell line
was also used as a &quot;seen&quot; sample in the recent
[PrecisionFDA Truth
Challenge V2](https://precision.fda.gov/challenges/10/view) competition for small variant calling.


### Structural variant calling with lra and cuteSV

As an easily reproducible example we will focus on a single flowcell of the
GM24385 2020.11 data release.

&gt; *This walkthrough assumes some familiarity with standard bioinformatic tools
&gt; for handling genomics data. A working installation of
&gt; [samtools](http://www.htslib.org/),
&gt; [snakemake](https://snakemake.readthedocs.io/en/stable/),
&gt; [git](https://git-scm.com/), and the [AWS command-line
&gt; tools](https://aws.amazon.com/cli/) are required to follow the process
&gt; below.*


#### Data preparation

We will start by downloading Guppy 4.0.11 basecalls from a PromethION
sequencing experiment (see our [tutorial](/tutorials) FAQs) for more
information on downloading data):

    aws s3 --no-sign-request cp \
        s3://ont-open-data/gm24385_2020.11/analysis/r9.4.1/20201026_1644_2-E5-H5_PAG07162_d7f262d5/guppy_v4.0.11_r9.4.1_hac_prom/basecalls.fastq.gz \
        basecalls.fastq.gz

The `.fastq` file downloaded above contains the QC pass calls from the experiment
amounting to around 200 Gbases.

To run the SV calling pipeline and perform benchmarking we will need release 37
of the human reference sequence:

    wget http://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.gz


#### Running the variant calling

To perform structural variant calling Oxford Nanopore Technologies recommends
using the
[pipeline-structural-variation](https://github.com/nanoporetech/pipeline-structural-variation)
snakemake workflow.  This workflow as been recently updated to use
[lra](https://github.com/ChaissonLab/LRA) and
[cuteSV](https://github.com/tjiangHIT/cuteSV), replacing the previous
[minimap2](https://github.com/lh3/minimap2) and
[sniffles](https://github.com/fritzsedlazeck/Sniffles) based approach. After
installation of this software we use it with its default settings:

    conda activate pipeline-structural-variation-v2
    snakemake call --config \
        input_fastq=basecalls.fastq.gz \
        reference_fasta=human_g1k_v37.fasta \
        threads=76 \
        sample_name=PAG07162-hg37

The useful output for our purposes is the single Variant Call Format file; a copy
of the file is available in the dataset S3 bucket at:

    s3://ont-open-data/gm24385_2020.11/extra_analyses/sv_calling/PAG07162-hg37/sv_calls/PAG07162-hg37_cutesv_filtered.vcf.gz

### Evaluation

The veracity of the variant calling performed above can be obtained by
comparing the results to the [Genome In A
Bottle](https://www.nist.gov/programs-projects/genome-bottle) truth sets for
the GM24385 sample. The truth sets can be downloaded from the
[NCBI](https://www.ncbi.nlm.nih.gov/) repository:

    truth_name=&quot;HG002_SVs_Tier1_v0.6&quot;
    for ext in &quot;.vcf.gz&quot;, &quot;.vcf.gz.tbi&quot;, &quot;.bed&quot;; do
        wget -O $truth_name$ext \
            https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_SVs_Integration_v0.6/$truth_name$ext
    done

With these reference data we will use
[truvari](https://github.com/spiralgenetics/truvari) to assess the recall and precision
of the variant calls made by the calling pipeline:

    truth_vcf=&quot;${truth_name}.vcf.gz&quot;
    truth_bed=&quot;${truth_name}.bed&quot;
    reference=&quot;human_g1k_v37.fasta&quot;
    input_vcf=&quot;PAG07162-hg37_cutesv_filtered.vcf.gz&quot;
    output_dir=&quot;truvari&quot;
    truvari bench --passonly --pctsim 0 \
        -b $truth_vcf --includebed $truth_bed \
        -f $reference -c $input_vcf \
        -o $output_dir

Truvari outputs precision and recall figures for the structural variants.  With
a little work (detailed in the [EPI2MELabs Structural Variation
Benchmarking](https://epi2me-labs.github.io/) tutorial) we can separate the
counts for deletion and insertion (including duplication) variants:

|     Type        |     Deletions   |   Insertions    |
|-----------------|-----------------|-----------------|
|     Recall      |     0.9667      |     0.9427      |
|     Precision   |     0.9893      |     0.9667      |
|     f1-score    |     0.9779      |     0.9545      |

With still a little more work we can produce the following depicting the
f1-score alongwith counts of SVs in the truthset:

![sv_length_f1](./sv_f1.png &quot;Structural variation calling f1-score&quot;)
</content:encoded><content:thumbnail/></item><item><title><![CDATA[November 2020 GM24385 Dataset Release]]></title><description><![CDATA[The data referenced in this page have been superceded by the June 2021 Guppy 5 rebasecalling release.]]></description><link>https://labs.epi2me.io/gm24385_2020.11</link><guid isPermaLink="false">https://labs.epi2me.io/gm24385_2020.11</guid><pubDate>Thu, 26 Nov 2020 11:38:00 GMT</pubDate><content:encoded>
&gt; The data referenced in this page have been superceded by the [June 2021](/gm24385_2021.05) Guppy 5 rebasecalling

We are happy to annouce an updated release of nanopore sequencing
of the Genome in a Bottle sample GM24385.

Multiple PromethION flowcells using R9.4.1 nanopores were used, along with two
different sample preparation methods. The direct sequencer output is included,
raw signal data stored in .fast5 files and basecalled data in .fastq file.
Additional secondary analyses are included, notably alignments of sequence data
to the reference genome are provided along with statistics derived from these.

Highlights of the dataset include:

- Demonstration of &gt;200Gb from improved flowcell reliability,
- Inclusion of data from the new Ultra Long Kit.

A brief report comparing the current release to the previous can be found
[here](/misc/gm24385_compare.html). 


***Whats included?***

The dataset comprises data from multiple R9.4.1 flowcells.

The initial sequencer outputs are included in self container directories.
In addition derived outputs from an automated pipeline are stored
separately.


***Details concerning sample preparations***

Below is described briefly the method of analyte preparation. Standard, published
protocols were followed with no intentional deviation.

*The following cell line samples were obtained from the NIGMS Human Genetic Cell
Repository at the Coriell Institute for Medical Research: GM24385*

The extracted DNA was subjected to one of two protocols. The first protocol
entailed:

- High molecular weight DNA from GM24385 lymphoblastoid cells was prepared by 
  [Evotec](https://www.evotec.com/en).
- Circulomics Short Read Eliminator XL
  protocol was used to deplete DNA fragments &lt; 40kb in length.
- DNA was end repaired and dA tailed prior to LSK based library preparation.

The second protocol used the new Ultralong Sequencing Kit (ULK).

In both cases sequencing was performed using PromethION device.

The dataset comprises multiple flowcells for each pore:

| Treatmeant       | Flowcells          |
|:----------------:|:------------------:|
|        SRE       | PAG07162, PAG07165 |
|        ULK       | PAG07506           |


***Location of the data***

The data is located in an Amazon Web Services S3 bucket at:

    s3://ont-open-data/gm24385_2020.11/

See the [tutorials](/tutorials/) page for information on downloading the dataset.


***Description of available data***

The uploaded dataset has been prepared using a snakemake pipeline to:

1. Align basecalls to reference sequence. All primary, secondary and
supplementary alignments are kept
2. Filter .bam file to list of regions defined in configuration file
retaining only primary alignments.
3. Produce read statistics from per-region .bams.
4. Repack/group source .fast5 files according to primary alignment .bams
to produce per-region .fast5 file sets.

For more details see our [post](/katuali_human_pipeline/) detailing the
pipeline and its outputs.

</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs 20.10 Release]]></title><description><![CDATA[Software release note.]]></description><link>https://labs.epi2me.io/epi2me-labs-20.10-release</link><guid isPermaLink="false">https://labs.epi2me.io/epi2me-labs-20.10-release</guid><pubDate>Mon, 26 Oct 2020 00:00:00 GMT</pubDate><content:encoded>
We are pleased to introduce a new release of our EPI2ME Labs.

With this update we are now supporting only the Jupyter interface to EPI2ME
Labs. This Jupyter support has the consequence that users no longer require a
Google account to run or save EPI2ME Labs tutorials and EPI2ME Labs is now
available to users regardless of their geographic location. Additional
information on our reasons for the move to Jupyter are described in a 
separate [post](/jupytermove)


![Notebook launch](./notebooksplash.png &quot;Launch a notebook&quot;)

*The EPI2ME Labs landing. This provides a link to the available tutorials and
workflows that are packaged inside the accompanying docker container. As well
as any recent documents of your own*

This release includes a number of extensions to the Jupyter software to support
usability. These extensions include

Inclusion of [autorun code
cells](https://github.com/epi2me-labs/jupyterlab-autorun-cells) that are
automatically computed when a notebook is opened. This functionality allows for
the preparation of forms for data-entry and for the setting of contextual
notebook variables.

The [code cell
collapser](https://github.com/epi2me-labs/jupyterlab-code-cell-collapser)
enables larger blocks of computer code to be hidden (or unhidden) within a
workflow. This makes the more technical notebooks simpler to read by
encouraging focus to results and the written texts.

The [play button](https://github.com/epi2me-labs/jupyterlab-play-cell-button)
extension replicates the functionality of the Google Colaboratory button; a
play icon is placed on the code cell that is to be run. This is simpler than
the default Jupyter play button that may execute code that is elsewhere and
outside of the visible screen.  

We hope that these extensions may also be of use in the preparation of your own
notebooks and workflows. Future EPI2ME Labs releases will continue to further
extend the Jupyter environment to simplify bioinformatics analyses and the
presentation of results.

All of our EPI2ME Labs tutorials have been converted to the Jupyter flavour.
See the [Quick start guide](/quickstart) guide for instructions of installing
and launching the EPI2ME Labs software.

We look forwards to your feedback on this EPI2ME Labs update and we would
welcome any recommendations for workflows that you would like to see in
upcoming releases.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs moves to JupyterLab]]></title><description><![CDATA[An update on moving EPI2ME Labs away from Google Colab and onto a self-contained JupyterLab interface.]]></description><link>https://labs.epi2me.io/jupytermove</link><guid isPermaLink="false">https://labs.epi2me.io/jupytermove</guid><pubDate>Fri, 23 Oct 2020 00:00:00 GMT</pubDate><content:encoded>
When we released EPI2MELabs earlier this year we wanted to create an
environment where &quot;non-coders&quot;, people with little to no experience of writing
computer code, could learn to analyse their nanopore sequencing data. To achieve this
we needed a slick, minimal, welcoming presentation of the data analysis. We
wanted to enable poeple to learn the ropes of sequence analysis - without being scared by
it.

As a first step, we elected to couple our analysis software with Python notebooks.  
However we felt that the standard JupyterLab interface for Python notebooks was 
a little too &quot;made by software developers, for software
developers&quot;, so we started to look for alternatives. The Google Colab interface
fit the bill: it has a clean design with no complex menus, allows code to be
hidden and contains many nice visual hints as to what is happening when code is
run. We therefore built our analysis notebooks and tutorials around Google
Colab and recommended that it be used with our customised Jupyter notebook
server.

However, for our purposes, Google Colab is not without its flaws: connecting the
user interface to the notebook server is a clunky affair; the interface is
geared toward using Google&apos;s cloud compute and storing documents in Google
Drive. It is also not available for users in some territories.

To simplify the EPI2MELabs experience for our growing userbase, starting with
version 0.2.6, we are recommending that users no longer use the Google Colab
interface and instead use the more standard JupyterLab interface included with the
EPI2MELabs server software. The EPI2MELabs Launcher application has been
updated to version 0.6.4 and streamlines the opening of the interface. We will
no longer be supporting the use of EPI2MELabs with Google Colab.

But given the reasons above for not using JupyterLab, why not use it from the outset?
In the last month we have worked hard to optimise the analysis notebooks and
tutorials for the JupyterLab interface specifically. This has involved adding additional
functionality to JupyterLab; small user interface tweaks have been made here
and there to create a more comfortable experience. Of particular note, on
opening EPI2MELabs in their browser, users will now be shown a list of all
available analyses and tutorials. When selecting a notebook template a copy
will be made and stored on the users computer. It may be edited and saved freely
without fear of irreveribly losing the original template. The user&apos;s
saved
notebooks can also be reopened from the launch page providing a convenient
location to start new analyses, or resume old ones.

The work to improve our notebooks for use with the JupyterLab interface has not finished.  We intend to further enhance the visual display of code within Jupyterlab to provide a more enticing experience for all users.  Going forward, we plan to contribute what we develop here back to the wider Jupyter community - so watch this space!

</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2MELabs Quick Start Guide]]></title><description><![CDATA[A guide illustrating installation and use of the EPI2ME Labs notebook server.]]></description><link>https://labs.epi2me.io/nbquickstart</link><guid isPermaLink="false">https://labs.epi2me.io/nbquickstart</guid><pubDate>Thu, 22 Oct 2020 00:00:00 GMT</pubDate><content:encoded>
Redirecting to /installation
&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; URL=&apos;/installation&apos;&quot; /&gt;</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2MELabs Quick Start Guide]]></title><description><![CDATA[A guide illustrating installation and use of the EPI2ME Labs notebook server.]]></description><link>https://labs.epi2me.io/quickstart</link><guid isPermaLink="false">https://labs.epi2me.io/quickstart</guid><pubDate>Thu, 22 Oct 2020 00:00:00 GMT</pubDate><content:encoded>
Redirecting to /nbquickstart
&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; URL=&apos;/nbquickstart&apos;&quot; /&gt;</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs downloads and resources]]></title><description><![CDATA[Software downloads and links to all our open software repositories.]]></description><link>https://labs.epi2me.io/downloads</link><guid isPermaLink="false">https://labs.epi2me.io/downloads</guid><pubDate>Thu, 22 Oct 2020 00:00:00 GMT</pubDate><content:encoded>
The EPI2ME Labs Launcher is used to start, stop, and update the EPI2MELabs notebook
server, our interactive environment for bioinformatics exploration and learning.
We provide executables for Windows 10, macOS, and Linux.

| Platform     | Download                                                                                                                                                              |
|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Windows 10   | [ont-epi2melabs-installer-v3.1.5-win10.exe](https://github.com/epi2me-labs/labslauncher/releases/download/v3.1.5/ont-epi2melabs-installer-v3.1.5-win10.exe)           |
| macOS        | [ont-epi2melabs-v3.1.5-macos.pkg](https://github.com/epi2me-labs/labslauncher/releases/download/v3.1.5/ont-epi2melabs-v3.1.5-macos.pkg)             |
| Ubuntu 16.04 | [ont-epi2melabs-3.1.5-0.xenial.deb](https://github.com/epi2me-labs/labslauncher/releases/download/v3.1.5/ont-epi2melabs-3.1.5-0.xenial.deb)         |
| Ubuntu 18.04 | [ont-epi2melabs-3.1.5-0.bionic.deb](https://github.com/epi2me-labs/labslauncher/releases/download/v3.1.5/ont-epi2melabs-3.1.5-0.bionic.deb)         |
| Ubuntu 20.04 | [ont-epi2melabs-3.1.5-0.focal.deb](https://github.com/epi2me-labs/labslauncher/releases/download/v3.1.5/ont-epi2melabs-3.1.5-0.focal.deb)           |
| CentOS 7     | [ont-epi2melabs-3.1.5-1.el7.x86_64.rpm](https://github.com/epi2me-labs/labslauncher/releases/download/v3.1.5/ont-epi2melabs-3.1.5-1.el7.x86_64.rpm) |
| CentOS 8     | *No longer supported* - [CentOS EOL](https://www.centos.org/centos-linux-eol/) |

The launcher is written in Python with [Qt](https://www.qt.io/) and the
sourcecode is available from GitHub:
https://github.com/epi2me-labs/labslauncher.

Below are listed all resources on which EPI2ME Labs is built. We aim for all
components of EPI2MELabs to be open source (including this website) and
available through our GitHub repository:
https://github.com/epi2me-labs/.

If you find something is not available please contact support@nanoporetech.com

## EPI2ME Labs resources

*A listing of all available notebooks is available on the [Notebook
Index](/nbindex) page.*

The notebooks used within EPI2ME Labs are available in their source form from
the GitHub repository: https://github.com/epi2me-labs/tutorials. These are
updated regularly with enhancements and fixes.

### Docker containers

The docker container for the EPI2MELabs notebook server is hosted on dockerhub:
https://hub.docker.com/r/ontresearch/epi2melabs-notebook. This source code used
to build this image is contained within the git repositories [nanolabs](https://github.com/epi2me-labs/nanolabs) and [tutorials](https://github.com/epi2me-labs/tutorials). The notebook server is based on the
[docker-stacks](https://github.com/jupyter) project. The EPI2ME Labs team have added
bioinformatics software as well as various Jupyter extensions (including some custom additions).

## Nextflow workflows

*A listing of all available notebooks is available on the [Workflows
Index](/wfindex) page.*

All EPI2ME Labs workflows are available open-source through our
[Github space](https://github.com/epi2me-labs/); workflow projects
are named with a `wf-artic` prefix.

The workflows can be run easily using nextflow without explicitely
downloading anything from Github, for example:

```
nextflow run epi2me-labs/wf-artic --help
```

Will download our SARS-CoV-2 ARTIC sequencing workflow and display
its help text. See the [Workflow Quick Start](/wfquickstart) for
further information.

## Other software

In the course of creating informative tutorials and tailored workflows, the
[Customer Workflows group](/authors) at Oxford Nanopore Technologies has created
several standalone packages that might be useful to other scientists in their
own right.

### Aplanat

[Aplanat](https://github.com/epi2me-labs/aplanat) is a plotting library built
on top of [bokeh](https://docs.bokeh.org/en/latest/). It allows graphs to be
contructed and displayed in notebooks with minimal boiler plate. Aplanat
also provides functionality to generate static HTML report documents, as used
in the EPI2ME Labs Nextflow workflows.

### Mapula
[Mapula](https://github.com/epi2me-labs/mapula) is an alignment statistics
package designed to work on streaming data; it can be inlined in a typical
[samtools](http://www.htslib.org/) pipeline to calculate statistic on-the-fly.

### Fastcat
[Fastcat](https://github.com/epi2me-labs/fastcat) is a small FASTQ utility
primarily used to concatenate transparently compressed and uncompressed FASTQ
files, whilst producing summaries of the input files.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs Installation]]></title><description><![CDATA[A guide illustrating installation and use of EPI2ME Labs.]]></description><link>https://labs.epi2me.io/installation</link><guid isPermaLink="false">https://labs.epi2me.io/installation</guid><pubDate>Thu, 22 Oct 2020 00:00:00 GMT</pubDate><content:encoded>
The following guide illustrates installation and use of EPI2ME Labs for running workflows and tutorials.

The EPI2ME Labs environment runs in a
[Docker](https://www.docker.com/resources/what-container) container in order to
provide an isolated and resettable environment. It can be controlled using the
EPI2ME Labs application. The instructions below will install both of
these tools onto your system. For the most part, these instructions simply
reiterate the documentation from the linked resources.

## Windows 10 installation

*Windows 11 support is untested, but is expected to work much the same as Windows 10*

&gt; For the workflows in  EPI2ME Labs to work you will require Windows build  18362.1049 or higher. To check this: Click Start &gt; search ‘Windows update settings’ &gt; at the bottom select ‘OS Build info’. If your build is earlier you may have Windows updates available at the top of this settings menu which you can install.
&gt; Our [Nextflow on Windows](/nextflow-on-windows) guide provides a second
&gt; method to set up an environment to run our Nextflow workflows on
&gt; old versions of Microsoft Windows. Users having issues with the method presented here or wanting to run our workflows from Windows cmd line may wish to read the alternative instructions.


Our [downloads page](https://labs.epi2me.io/downloads) contains a link to the latest EPI2ME Labs installer. The installer will guide users through the installation; it should be noted that during the install your computer may restart.

We are now ready to 
[install Docker Desktop on Windows](https://docs.docker.com/desktop/windows/install/)
following Docker&apos;s instructions. Follow the link to download the installer for the
WSL2 Backend variant. When the download is finished launch
the installer and be sure that the checkbox to install Windows components for WSL2 is
checked as in the below when given the option:

![Docker install Windows ](./docker-install-win-01.png &quot;Docker installation options&quot;)

When the installation is complete you may be asked to logout of Windows. Promptly log
back in and launch Docker Desktop from the desktop icon. The application will start
and walk you through some basics. Follow these at you leisure or simply skip to the
opening Powershell and running the command:

```
docker run -d -p 80:80 docker/getting-started
```

This will likely pop up a Windows Security Alert which you should allow:

![Docker install Windows ](./docker-install-win-02.png &quot;Docker Getting Started demonstration&quot;)

Within the Docker Desktop application window you should see a new entry:

![Docker install Windows ](./docker-install-win-03.png &quot;Docker Getting Started is alive&quot;)

The Delete icon can be safely clicked to remove the running Docker container.

In Docker Desktop settings check that under resources WSL integration Ubuntu is
enabled.

![Docker install Windows ](./docker-install-win-04.png &quot;Docker &quot;)


You&apos;re now ready to use EPI2ME Labs type `epi2melabs` in the Windows search bar to find and open the application:

![Running EPI2ME Labs Windows](./labs-install-win-03.png &quot;Opening EPI2ME Labs for the first time&quot;)

### Resource Limits for Docker on Windows

Resource limits for Docker on Windows will be managed by Windows. You need to 
create a new or edit the existing file - `C:/Users/&lt;USERNAME&gt;/.wslconfig`. 

Copy and paste the following and adjust. We recommend
setting the Memory limit to at least 8G and the CPUs to one or two
less than the maximum value for your system

```
[wsl2]
memory=8GB 
processors=4
localhostForwarding=true 
``` 

## MacOS installation

A link to the current version of the EPI2ME Labs can be found on our
[Downloads](/downloads) page. The application is provided as a standard MacOS
disk image (`.dmg` file), the contents of which can be unpacked into a users
Applications directory in the usual manner.

MacOS users are encouraged to follow the installation instruction for Docker Desktop
found on the [Docker Website](https://www.docker.com/products/docker-desktop).

To use the workflow launching features of EPI2ME Labs, Java and must be installed. 
Java can be installed by following the links from: https://www.oracle.com/java/technologies/downloads/. 
For macs with M1 processers the Arm 64 installer is required: https://download.oracle.com/java/18/latest/jdk-18_macos-aarch64_bin.dmg. 
For older, Intel Macs the x64 installer should be downloaded instead: https://download.oracle.com/java/18/latest/jdk-18_macos-x64_bin.dmg

### Resource Limits for Docker Desktop on MacOS

By default Docker Desktop sets fairly conservative resource limits for itself,
you may wish to change these in the Docker Desktop settings pane. We recommend
setting the Memory limit to at least 8Gb (some EPI2ME Labs tutorials may
require more as indicated in their introductions) and the CPUs to one or two
less than the maximum value for your system.

![docker resource](./8.png &quot;Docker resources&quot;)


## GridIon/PromethION/Ubuntu install

Docker is not installed by default on older GridION and PromethION devices.
If an updated MinKNOW has been installed then Docker should be present. We 
advise users to first try running:

```docker run hello-world```

from a terminal window as in the instructions below to check the status of any 
Docker installation. If this command fails, ensure that MinKNOW is up to date.

### Installing docker

&gt; This step should not be required on GridION and PromethION devices with up-to-date operating system installations.

To install Docker without an up-to-date MinKNOW install follow the instructions below.

1. Click on the **Search your computer** button.

![search computer 1](./1.png &quot;Search for terminal&quot;)

2. Search for &quot;terminal&quot;, and click on the Terminal application:

![search computer 2](./2.png &quot;Search for terminal&quot;)

A terminal window will open:

![terminal window](./3.png &quot;Terminal window&quot;)

3. Run the Docker installation command (you can copy and paste this into the
   terminal window and press **Enter**):
   ```
   sudo apt update
   sudo apt install docker.io
   sudo usermod -aG docker &lt;username&gt;
   ```
   where `&lt;username&gt;` should be replaced with your computer username.
4. Close your terminal window.
5. Log out of your GridION and log back in again.
6. Open a new terminal window.
7. To test docker is working correctly, run ``` docker run hello-world ```

![docker hello](./4.png &quot;Docker hello world&quot;)

### Installing Nextflow

Installation of the software on Oxford Nanopore Technologies&apos; sequencing device
can be performed using the command:

    sudo apt install ont-nextflow

This will install a [Java runtime](https://openjdk.java.net/), Nextflow, and
Docker. If docker has not already been configured the command below can be
used to provide user access to the docker services. Please logout of your
computer after this command has been typed.

    sudo usermod -aG docker $USER

&gt; When installing Nexflow using the following please ensure that Nextflow version &gt;= 20.07.1 is installed.

For hardware running Ubuntu (with is not an Oxford Nanopore Technologies sequencing device) the following should suffice to install `nextflow` in order to run workflows from within EPI2ME Labs.

1. Install a Java runtime environment (JRE):

   ```sudo apt install default-jre```

2. Download and install Nextflow may be downloaded from https://www.nextflow.io:

   ```curl -s https://get.nextflow.io | bash```

   This will place a `nextflow` binary in the current working directory, you 
   may wish to move this to a location where it is always accessible, e.g:

   ```sudo mv nextflow /usr/local/bin```


### Installing EPI2ME Labs on GridION and PromethION

EPI2ME Labs is most easily installed:

1. Copy and paste the following into the terminal window and press **Enter**:
   ```
   sudo apt update
   sudo apt install ont-epi2melabs
   ```
2. Click on the Search your computer icon, and search for &quot;EPI2ME&quot;. Then click
   on the **EPI2ME-Labs Server Control** icon. This will start EPI2ME Labs.
   
### Reinstallation of EPI2ME Labs on GridION and PromethION

In the event you wish to reinstall EPI2ME Labs on GridION and PromethION it is 
vital to run purge to remove the original version of EPI2ME Labs and its associated 
configuration files.

1. Copy and paste the following into the terminal window and press **Enter**:
   ```
   sudo apt purge ont-epi2melabs
   sudo apt autoremove
   ```
2. Copy and paste the following into the terminal window and press **Enter**:
   ```
   sudo apt update
   sudo apt install ont-epi2melabs
   ```
The first step removes EPI2ME Labs and any associated configuration files and dependencies 
that are only required by EPI2ME Labs. The second step reinstalls the  EPI2ME Labs.

</content:encoded><content:thumbnail>https://labs.epi2me.io/static/f43792dc5fe4e6365fad247722a39785/thumbnail.jpg</content:thumbnail></item><item><title><![CDATA[Small variant calling with GM24385]]></title><description><![CDATA[An exploration of small variant calling using our previously released HG002/GM24385 Ashkenazi Son data release.]]></description><link>https://labs.epi2me.io/gm24385_snp</link><guid isPermaLink="false">https://labs.epi2me.io/gm24385_snp</guid><pubDate>Thu, 15 Oct 2020 00:00:00 GMT</pubDate><content:encoded>
In this blog post we will explore small variant calling using the [previously
released](/gm24385_2020.09) HG002 (GM24385 Ashkenazi Son) data release.

The GM24385 dataset comprises whole genome sequencing of a well-characterised
human cell line. It therefore provides a useful benchmark sample; the cell line
was also used as a &quot;seen&quot; sample in the recent
[https://precision.fda.gov/challenges/10/view](PrecisionFDA Truth
Challenge V2) competition.


### Variant Calling with Medaka and DeepVariant

As an easily reproducible example we will focus on chromosome 20 of the genome
rather than performing computation on the whole genome.

&gt; *This walkthrough assumes some familiarity with standard bioinformatic tools
&gt; for handling genomics data. A working installation of
&gt; [samtools](http://www.htslib.org/),
&gt; [bedtools](https://bedtools.readthedocs.io/en/latest/),
&gt; [medaka](https://github.com/nanoporetech/medaka),
&gt; [docker](https://www.docker.com/get-started), and the [AWS command-line
&gt; tools](https://aws.amazon.com/cli/) are required to follow the process
&gt; below.*


#### Data preparation

To start let us download the pre-aligned reads corresponding to chromosome 20
from the dataset resource (see our [tutorial](/tutorials) FAQs) for more
information on downloading data):

    for ext in .bam .bam.bai; do
        aws s3 --no-sign-request cp s3://ont-open-data/gm24385_2020.09/analysis/r9.4.1/20200914_1354_6B_PAF27096_e7c9eae6/guppy_v4.0.11_r9.4.1_hac_prom/align_unfiltered/chr20/calls2ref${ext} PAF27096.chr20${ext}
        aws s3 --no-sign-request cp s3://ont-open-data/gm24385_2020.09/analysis/r9.4.1/20200914_1357_1-E11-H11_PAF27462_d3c9678e/guppy_v4.0.11_r9.4.1_hac_prom/align_unfiltered/chr20/calls2ref${ext} PAF27462.chr20${ext}
    done

and merge these into a single `.bam` file:

    samtools merge chr20.bam PAF27096.chr20.bam PAF27462.chr20.bam
    samtools index chr20.bam

As an additional step we will filter the `chr20.bam` file to leave only primary
alignments, removing secondary and supplementary alignments. This is necessary
as DeepVariant, which we will use later, will otherwise process these
additional alignments.

    samtools view chr20.bam -F 2308 -@ 64 -b &gt; chr20.primary.bam
    samtools index chr20.primary.bam

To perform variant calling we also require the human reference sequence, the
same sequence as used to create the alignment files prepared above. An indexed
copy of this is available from the dataset resource:

    for ext in .fasta .fasta.fai; do
        aws s3 --no-sign-request cp s3://ont-open-data/gm24385_2020.09/config/ref/GCA_000001405.15_GRCh38_no_alt_analysis_set${ext} .
    done

We have now all the inputs required to perform variant calling with medaka
and DeepVariant.

#### Running Medaka

[Medaka](https://github.com/nanoporetech/medaka) is Oxford Nanopore
Technologies&apos; software for performing consensus and small variant calling from
nanopore long-read data. It can perform diploid variant calling either in
isolation or in tandem with
[DeepVariant](https://github.com/google/deepvariant); to enable this second
use-case small modifications have been made to how medaka represents variants
to allow it to function as a variant-candidate generator for DeepVariant.

To perform candidate generation with medaka we run:

    medaka_variant -i chr20.primary.bam -f GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta \
        -r chr20 -t 8 -P 0 -l

The final two options here (`-P 0 -l`) instruct medaka to leave its
substitution calls unfiltered and to decompose multi-nucleotide substitutions
into independent single nucleotide substitutions. Details of workflow employed
by the `medaka_variant` program can be found in the medaka
[documentation](https://nanoporetech.github.io/medaka/snp.html#).

The result of running the above command will be a directory `medaka_variant`
containing (amongst other files):

 * *`round_0_hap_mixed_phased.bam`*: alignments (as in `chr20.primary.bam`), tagged with a calculated haplotype,
 * *`round_1.vcf`*: the final output variant candidates for DeepVariant.

#### Running DeepVariant

With variant-candidate generation performed by medaka we can now use
DeepVariant to calculate our final variant calls. In order to simplify this we
will download a program that automates the execution of the docker container
used to run DeepVariant:

    wget https://gist.githubusercontent.com/cjw85/23d2b0675ec5a5c7fd4074456524c971/raw/c716c85639f047b9a9cff2079be2868bccb61659/run_deepvariant.sh
    chmod +x run_deepvariant.sh

This script simply gathers together the required inputs before executing
DeepVariant within the docker container, it can be run simply with:

    ./run_deepvariant.sh \
        -b medaka_variant/round_0_hap_mixed_phased.bam \
        -v medaka_variant/round_1.vcf \
        -r GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta \
        -o deep_variant -t 64 -q

The final variant calls will be present at `deepvariant/deepvariant.vcf.gz`.
The `-q` option here runs a &quot;high quality&quot; version of the DeepVariant
calculation; this version incurs a runtime cost of aaround a factor of
four and gives marginable improvement in results.

### Evaluation

The veracity of the variant calling performed above can be obtained by
comparing the results to the [Genome In A
Bottle](https://www.nist.gov/programs-projects/genome-bottle) truth sets for
the GM24385 sample. The truth sets can be downloaded from the
[NCBI](https://www.ncbi.nlm.nih.gov/) repository:

    for ext in .bed .bed.gz .bed.gz.tbi .vcf.gz .vcf.gz.tbi; do
        wget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.1/GRCh38/HG002_GRCh38_1_22_v4.1_draft_benchmark${ext}
    done

With these reference data we will use
[hap.py](https://github.com/Illumina/hap.py) to assess the recall and precision
of the variant calls made by DeepVariant. Hap.py is most easily run using the
docker container provided by it&apos;s authors:

    docker run -it -v ${PWD}:${PWD} pkrusche/hap.py /opt/hap.py/bin/hap.py \
        ${PWD}/HG002_GRCh38_1_22_v4.1_draft_benchmark.vcf.gz ${PWD}/deepvariant/deepvariant.vcf.gz \
        -f ${PWD}/HG002_GRCh38_1_22_v4.1_draft_benchmark.bed \
        -r ${PWD}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta \
        -o happy_out --pass-only -l chr20 --engine=vcfeval --threads=20

The output of the above will be a table summarising the results of the
comparison. An abbreviated form is given below:

|            Type        |     INDEL       |     SNP         |
|------------------------|-----------------|-----------------|
|     TRUTH.TOTAL        |     11271       |     71334       |
|     METRIC.Recall      |     0.5927      |     0.9972      |
|     METRIC.Precision   |     0.8384      |     0.9973      |
|     METRIC.F1_Score    |     0.6944      |     0.9973      |


An active area of research is the calling of variants in low complexity regions.
The NCBI reference data includes an index of such regions, we can mask these
regions from the comparison:
    
    wget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v2.0/GRCh38/LowComplexity/GRCh38_notinAllTandemRepeatsandHomopolymers_slop5.bed.gz
    bedtools intersect \
        -a HG002_GRCh38_1_22_v4.1_draft_benchmark.bed \
        -b GRCh38_notinAllTandemRepeatsandHomopolymers_slop5.bed.gz 
        &gt; HG002_GRCh38_1_22_v4.1_draft_benchmark.norepeat.bed

    docker run -it -v ${PWD}:${PWD} pkrusche/hap.py /opt/hap.py/bin/hap.py \
        ${PWD}/HG002_GRCh38_GIABv4.1.vcf.gz ${PWD}/deepvariant/deepvariant.vcf.gz \
        -f ${PWD}/ HG002_GRCh38_1_22_v4.1_draft_benchmark.norepeat.bed \
        -r ${PWD}/truths/GRCh38_no_alt_chr20.fa \
        -o happy_out --pass-only -l chr20 --engine=vcfeval --threads=20

With these regions masked we now obtain:


|            Type        |     INDEL       |     SNP         |
|------------------------|-----------------|-----------------|
|     METRIC.Recall      |     0.9458      |     0.9992      |
|     METRIC.Precision   |     0.9827      |     0.9992      |
|     METRIC.F1_Score    |     0.9639      |     0.9992      |

The latest research basecallers are better able to provide accurate calls
through low complexity regions. The example below compares basecalls produced
with Guppy 4.0.11 and the soon to be release Bonito version 0.3.0 basecaller.

![bonito_low_complexity_calls](./bonito_igv.png &quot;Bonito Low Complexity Basecalls&quot;)

The [IGV](http://software.broadinstitute.org/software/igv/) screenshot shows
how the Bonito basecaller is less prone to long deletion tracks in the low
complexity region. With improvements to basecalling we therefore anticipate
being able to produce highly accurate INDEL calls in these regions in the near
future.

### Acknowledgements

We kindly acknowledge Andrew Carroll of Google Health and Benedict Paten&apos;s
group at the Computational Genomics Lab, UC Santa Cruz Genomics Institute for
releasing the DeepVariant inference models for nanopore data and discussions
concerning how to present variant candidates to DeepVariant.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[Bonito basecalling with R9.4.1]]></title><link>https://labs.epi2me.io/bonito</link><guid isPermaLink="false">https://labs.epi2me.io/bonito</guid><pubDate>Thu, 15 Oct 2020 00:00:00 GMT</pubDate><content:encoded>
**Updated 2020-10-30: This page was edited to reflect the formal release of Bonito v0.3.0**

We are please to announce the addition of
[bonito](https://github.com/nanoporetech/bonito) basecalling results to the
GM24385 dataset. Bonito is a research-grade, open source basecaller utilising
the [PyTorch](https://pytorch.org/) library; its development explores
alternative basecalling frameworks to those use in the product-grade Guppy
basecalling software.

The Bonito basecalling for the GM24385 dataset was performed using version
0.3.0, driven by the same [katuali](/katuali_human_pipeline/) analysis pipeline
as for the initial dataset release.  The Bonito basecaller was provided as
input the per-chromosome `.fast5` files created in the initial pipeline via
alignment of the Guppy 4.0.11 basecalls. This allows for easy comparison of
results on subsets of the data (but may lead to subtle side-effects). For
example the analysis data structure contains now entries of the form:

    gm24385_2020.09/analysis/r9.4.1/{flowcell}/guppy_{suffix}/align_unfiltered/{chromosome}/bonito_v0.3.0/
    ├── align_unfiltered
    │   ├── align_to_ref.log
    │   ├── basecall_stats.log
    │   ├── calls2ref.bam
    │   ├── calls2ref.bam.bai
    │   └── calls2ref_stats.txt
    ├── basecalls.fastq.gz
    └── basecalls.fastq.gz_summary.tsv

The file `basecalls.fastq.gz` contains the basecalling results from Bonito. The
quality scores in these files have been mocked as the pre-release build of Bonito used
does not yet provide quality scores. Similar to the main folder structure the
`align_unfiltered` directory contains unfiltered alignments of the basecalls to
the reference sequence (`calls2ref.bam`) along with text files summarizing the
properties of the alignments.

### Comparison with Guppy 4.0.11 basecalls

As a basis for comparison with the current Guppy basecaller we can use the alignment summary
files for both the Guppy and Bonito basecalls. To simplify the analysis we compare only chromosome 1
data for a single flowcell; we can download the files with:

    aws s3 cp --no-sign-request s3://ont-open-data/gm24385_2020.09/analysis/r9.4.1/20200914_1354_6B_PAF27096_e7c9eae6/guppy_v4.0.11_r9.4.1_hac_prom/align_unfiltered/chr1/calls2ref_stats.txt guppy.stats
    aws s3 cp --no-sign-request s3://ont-open-data/gm24385_2020.09/analysis/r9.4.1/20200914_1354_6B_PAF27096_e7c9eae6/guppy_v4.0.11_r9.4.1_hac_prom/align_unfiltered/chr1/bonito_v0.3.0/align_unfiltered/calls2ref_stats.txt bonito.stats

The following [python code](./plot.py),

    from concurrent.futures import ProcessPoolExecutor
    import pandas as pd
    import aplanat.util
    from aplanat import lines
    
    def read_data(args):
        caller, filename = args
        df = pd.read_csv(filename, sep=&apos;\t&apos;)
        xs, ys = aplanat.util.kernel_density_estimate(df[&apos;acc&apos;], step=0.05)
        df = pd.DataFrame({&apos;accuracy&apos;:xs, &apos;density&apos;:ys})
        df[&apos;caller&apos;] = caller
        return df
    
    data_sets = {
        &apos;bonito&apos;: &apos;bonito.stats&apos;,
        &apos;guppy&apos;: &apos;guppy.stats&apos;}
    
    with ProcessPoolExecutor() as executor:
        dfs = list(executor.map(read_data, data_sets.items()))
    plot = lines.line(
        [df[&apos;accuracy&apos;] for df in dfs],
        [df[&apos;density&apos;] for df in dfs],
        colors=[&apos;red&apos;, &apos;blue&apos;],
        names=[&apos;bonito&apos;, &apos;guppy&apos;],
        xlim=(85,100),
        x_axis_label=&apos;Alignment accuracy&apos;,
        y_axis_label=&apos;Density&apos;)
    plot.legend.location = &apos;top_left&apos;

can be used to plot a kernel density estimate for the read alignment accuracy:

![accuracy comparison](./accuracy.png &quot;Basecalls: Bonito CTC-CRF vs. Guppy 4.0.11&quot;)

The plot indicates a decrease of one-third in the modal error of reads.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[Katuali analysis pipeline for preparing human datasets]]></title><description><![CDATA[A description of the snakemake pipeline used to process many of our ONT Open Dataset releases.]]></description><link>https://labs.epi2me.io/katuali_human_pipeline</link><guid isPermaLink="false">https://labs.epi2me.io/katuali_human_pipeline</guid><pubDate>Tue, 22 Sep 2020 00:04:00 GMT</pubDate><content:encoded>
Our recent [GM24385 data release](/gm24385_2020.09) contains data from multiple
flowcells and analytes for both the R9.4.1 and R10.3 flowcell chemistries. The
uploaded data contains the primary sequencer output data; the full MinKNOW
output directory for the runs is included verbatim. The release seperately
contains a directory structure resulting from the application of a snakemake
analysis pipeline. Here we provide details of how this workflow was executed
and its outputs.

### Background

[Katuali](https://github.com/nanoporetech/katuali) is a set of
[Snakemake](https://snakemake.readthedocs.io/) analysis pipelines for basic
analysis of nanopore sequencing data. It can perform basic tasks such as
basecalling, alignment of reads, assembly, and evaluation and benchmarking of
such algorithms. This can be performed at scale on large compute clusters on
local or cloud infrastructure.

For the GM24385 release Katuali was used to construct secondary analyses in a
documented and reproducible fashion. As katuali is open source, it is possible
for users to reconstruct these secondary analyses for themselves from the
primary data. We have uploaded the results of these analysis to provide
benchmarking data and make available useful resources for others to perform
further analysis.

The Katuali pipeline used for the [GM24385 data release](/gm24385_2020.09)
provides four main outputs:

1. Align basecalls to reference sequence retaining all primary, secondary and
   supplementary alignments are kept
2. Filter .bam file to list of regions defined in configuration file retaining
   only primary alignments.
3. Produce read statistics from per-region .bams.
4. Repack/group source .fast5 files according to primary alignment .bams to
   produce per-region .fast5 file sets.

These outputs provide added value to the primary data, and users can extend and
adapt the katuali pipeline and configuration to calculate additional outputs.

### Katuali configuration for GM24385 release

Katuali builds on native Snakemake functionality to provide a way of mapping
and analysis pipeline across multiple inputs with minimal fuss. How this is
achieved is describe in the katuali [documentation](https://nanoporetech.github.io/katuali/examples.html#automatic-generation-of-custom-pipeline-targets). This functionality can be used to simulataneously
process data from multiple flowcells.

A single configuration file is used to control Katuali&apos;s behaviour: what input
data it will use, what pipelines it will run, and the configuration of external
programs that it runs. The configuration file can be created from the 
provided template using the [katuali_config](https://nanoporetech.github.io/katuali/tests.html#predefined-workflows)
command.

For the purposes of the GM24385 data release this file was then customised with
details of the input datasets (the `.fast5`/`.fastq` files from MinKNOW) and a
description of the outputs that were required. The resulting files are included
in the data release under the `config` folder at:

    s3://ont-open-data/gm24385_2020.09/config/

See our [tutorials](/tutorials/) page for details on how to download these
files.

#### Setup of input directories

Katuali can be used to perform basecalling from `.fast5` files to produce
standard `.fastq` sequence files. However since basecalling was performed
during the sequencing experiments we can sidestep the basecalling procedure
and simply bootstrap the Katuali output directory with the already computed
basecalls. To do this the `setup_katuali.sh` program, located at:

    s3://ont-open-data/gm24385_2020.09/config/setup_katuali.sh

was used. This prepares a directory structure that Katuali would otherwise
produce itself whilst avoiding some expensive computations. Seperate top-level
Katuali directories were created to group flowcell data from R9.4.1 and R10.3
flowcells.

#### Important aspects of configuration file

The template configuration files need only minor customisation for the GM24385
dataset. Firstly the `DATA:` section requires specifying, for example the
R9.4.1 file contains an entries such as the following (one per flowcell):

    DATA:
        &apos;20200914_1356_6F_PAF26223_da14221a&apos;:
            &apos;REFERENCE&apos;: &apos;ref/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta&apos;
            &apos;SPLIT_FAST5_REGIONS&apos;:
                [&apos;chr1&apos;, &apos;chr2&apos;, &apos;chr3&apos;, &apos;chr4&apos;, &apos;chr5&apos;, &apos;chr6&apos;, &apos;chr7&apos;, &apos;chr8&apos;, &apos;chr9&apos;, &apos;chr10&apos;,
                &apos;chr11&apos;, &apos;chr12&apos;, &apos;chr13&apos;, &apos;chr14&apos;, &apos;chr15&apos;, &apos;chr16&apos;, &apos;chr17&apos;, &apos;chr18&apos;, &apos;chr19&apos;, &apos;chr20&apos;,
                &apos;chr21&apos;, &apos;chr22&apos;, &apos;chrX&apos;, &apos;chrY&apos;]

The first item here is simply the name MinKNOW output directory. The
`REFERENCE` entry is a relative filepath to the genomic reference sequence
appropriate to the sample; in the case of the GM24385 dataset this is simply
the human reference genome. The final entry in the above is a list of sequence
identifies (corresponding to entries in the `REFERENCE` file) indicating how
the dataset should be separated after alignment of sequences to the reference.

The second important customisation of the template configuration file is the specification
of which output files should be created. This appears in the `PIPELINES` section:

    PIPELINES:
        all_initial: [
            # do alignments, split bams and fast5s by regions
            &quot;{DATA}/guppy_v4.0.11_r10.3_hac_prom/align_unfiltered/{SPLIT_FAST5_REGIONS}/fast5/&quot;,
        ]
        all_add_alignment_stats: [
            # calculate alignment stats
            &quot;{DATA}/guppy_v4.0.11_r10.3_hac_prom/align_unfiltered/calls2ref_stats.txt&quot;,
            &quot;{DATA}/guppy_v4.0.11_r10.3_hac_prom/align_unfiltered/{SPLIT_FAST5_REGIONS}/calls2ref_stats.txt&quot;
        ]

What these so-called &quot;targets&quot; cause Katuali and Snakemake to calculate is discussed below.
To have katuali perform the calculation for all items in the `DATA` section it is sufficient
to run katuali with, for example:

    katuali all_initial --configfile ../../config/r9.4.1.config

from the directory corresponding to:

    s3://ont-open-data/gm24385_2020.09/analysis/r9.4.1/

Katuali replaces the `{DATA}` and `{SPLIT_FAST5_REGIONS}` tags in the listings above with
all possible values listed in the `DATA` section of the configuration file. The resulting
matrix of targets is given to Snakemake to perform the workflows.

### Pipeline data flow and output descriptions

The filepath targets defined in the Katuali configuration files trigger
Snakemake to perform all necessary calculations required to produce the
requested files. Users interested in the Snakemake rules used to produce all
files should consult the Katuali documentation and source files
[here](https://github.com/nanoporetech/katuali/tree/master/katuali/data), which
are grouped logically according to function.

Katuali has for the most part a convention that outputs which are derived
directly from an input (or previous intermediate output) are stored in a
sub-directory of that previous input. This leads to a continued deepening of
the directory structure. A benefit of this approach is the ability to
recursively calculate new outputs of the same type without having to write
multiple rules. Downside of the approach are that it is not always easy to see
which items are the immediate outputs of an analysis stage and which are the
outputs of subsequent stages. To aid users who do not wish to examine the
Snakemake files included in Katuali, the directory listing below will aid
comprehension.

Katuali generically labels results of analysis stages using a
`&lt;stage&gt;_&lt;suffix&gt;` form, for example in the below the top level
`&lt;guppy_v4.0.11_r10.3_hac_prom&gt;` indicates results under this level
are results of the Guppy basecaller using the settings specified
in the Katuali configuration file under the tag `v4.0.11_r10.3_hac_prom`.
Similarly `&lt;align_unfiltered&gt;` indicates results from the alignment
rule generated using the `unfiltered` settings.

    .
    ├── guppy_v4.0.11_r10.3_hac_prom
    │   ├── basecalls.fastq
    │   ├── sequencing_summary.txt
    │   ├── align_unfiltered
    │   │   ├── calls2ref.bam
    │   │   ├── calls2ref.bam.bai
    │   │   ├── calls2ref_stats.txt
    │   │   ├── chr1
    │   │   │   ├── calls2ref.bam
    │   │   │   ├── calls2ref.bam.bai
    │   │   │   ├── calls2ref_stats.txt
    │   │   │   ├── fast5
    │   │   │   │   ├── batch0.fast5
    │   │   │   │   ├── batch1.fast5
    │   │   │   │   ├── ...
    │   │   │   │   └── filename_mapping.txt
    │   │   │   └── readlist.txt
    │   │   ├── chr2
    ┊	┊   ┊	...
    └── reads -&gt; &lt;link to MinKNOW fast5_pass directory&gt;

(Log files have been omitted from the above listing).

**The guppy analysis stage**

The Guppy analysis stage has two outputs:

* `basecalls.fastq` - all basecalls from the basecaller in a single file.
* `sequencing_summary.txt` - per-read summary information (as produced by MinKNOW).

**Alignment analysis stage**

The alignment stage of the workflow produced the following files under the
`align_unfiltered` directory. The `unfiltered` suffix relates to the fact that
all alignments are retained, not simply the primary alignment of each read.

* `calls2ref.bam` - the alignments of reads to the supplied reference.
* `calls2ref.bam.bai` - an index file for the alignments.

An auxiliary target of the Katuali pipeline produces the following:

* `calls2ref_stats.txt` - per-read statistics calculated from the corresponding
  `calls2ref.bam`.

**Alignment filtering stage**

Having aligned the basecall data, Katuali separates the basecalls and read data
stored in the source `.fast5` data by the regions specified in the Katuali
config. A directory is produced by region, for example `chr1` in the listing
above. Under this we find:

* `calls2ref*` - files analagous to those in the `align_unfiltered` directory
  but containing only those reads with primary alignments to the given region.
* `readlist.txt` - a simple text table containing the read identifiers of the
  requisite reads.
* `fast5` - a directory containing `.fast5` files constructed from the original
MinKNOW `.fast5` files but containing only the requisite reads. The file
`filename_mapping.txt` provides a read identifier to filename mapping.


</content:encoded><content:thumbnail/></item><item><title><![CDATA[ONT Open Datasets Tutorials]]></title><description><![CDATA[Helpful information and links to tutorials for datasets within the Open Datasets project.]]></description><link>https://labs.epi2me.io/tutorials</link><guid isPermaLink="false">https://labs.epi2me.io/tutorials</guid><pubDate>Tue, 22 Sep 2020 00:01:00 GMT</pubDate><content:encoded>
Below you will find helpful information and links out to tutorials for
datasets within the Open Datasets project.

***How can I access the data?***

All the data is stored under in an Amazon Web Services S3 bucket:

    s3://ont-open-data

This public has public permissions for anyone to obtain the data
without requiring login credentials. To download the data we
recommend using the [AWS command line interface](https://aws.amazon.com/cli/).
With the CLI installed listing the datasets can be performed with:

    aws s3 ls --no-sign-request s3://ont-open-data/

There will be a subdirectory per dataset release. Inside each dataset
will be contained a README.md file with brief details. This website
will contain additional details for each dataset.

To download datasets or extracts thereof we recommend using the `sync`
command:

    aws s3 sync --no-sign-request s3://ont-open-data/gm24385_2020.09 gm24385_2020.09

Datasets may be added to or amended over time so using `sync` can be
used with a previously fetch copy to update with the latest changes
and additions.

***Running workflows using ONT&apos;s EPI2ME platform***

Oxford Nanopore&apos;s [EPI2ME](https://epi2me.nanoporetech.com/)
platform includes several workflows for analysing
genomics datasets in the cloud with Amazon Web Services, reducing the
computational burden on the users local compute infrastructure.
For example, the [FASTQ Human Alignment GRCh38](https://community.nanoporetech.com/protocols/epi2me/v/mte_1014_v1_revba_11apr2016/human-alignment-grch38)
can be used to perform alignment of nanopore sequencing reads to 
the human reference genome to obtain industry standard BAM file outputs.

***How can I recreate the analysis directories?***

With each dataset release we will include details of how the analysis
directories were constructed from the primary inputs. For example for
the [GM24385](/gm24385_2020.09) dataset release [this page](/katuali_human_pipeline)
contains details of the Snakemake pipeline used.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[ONT Open Datasets Launch]]></title><description><![CDATA[Welcome message to our Open Datasets initiative.]]></description><link>https://labs.epi2me.io/ont-open-datasets-launch</link><guid isPermaLink="false">https://labs.epi2me.io/ont-open-datasets-launch</guid><pubDate>Tue, 22 Sep 2020 00:00:00 GMT</pubDate><content:encoded>
The ont-open-data registry provides reference sequencing data from Oxford
Nanopore Technologies to support,

1. Exploration of the characteristics of nanopore sequence data,
2. Assessment and reproduction of performance benchmarks,
3. Example datasets for analysis on EPI2ME, Oxford Nanopore Technologies&apos; cloud
   compute infrastructure,
4. Development of tools and methods.

The data deposited showcases DNA sequences from a representative subset of
sequencing chemistries. The datasets correspond to publicly-available reference
samples (e.g. GM24385 as reference human). Raw data are provided with metadata
and scripts to describe sample and data provenance.

</content:encoded><content:thumbnail/></item><item><title><![CDATA[GM24385 Dataset Release]]></title><description><![CDATA[The data referenced in this page have been superceded by the June 2021 Guppy 5 rebasecalling of the November 2020 dataset.]]></description><link>https://labs.epi2me.io/gm24385_2020.09</link><guid isPermaLink="false">https://labs.epi2me.io/gm24385_2020.09</guid><pubDate>Tue, 22 Sep 2020 00:00:00 GMT</pubDate><content:encoded>

&gt; The R9.4.1 data referenced in this page have been superceded by the [June 2021](/gm24385_2021.05) Guppy 5 rebasecalling of the [November 2020](/gm24385_2020.11) dataset.

We are happy to annouce the release of a Nanopore sequencing dataset
of the Genome in a Bottle sample GM24385.

Multiple PromethION flowcells using both the R9.4.1 and R10.3 nanopores.
The direct sequencer output is included, raw signal data stored in
.fast5 files and basecalled data in .fastq file. Additional secondary
analyses are included, notably alignments of sequence data to the
reference genome are provided along with statistics derived from these.


***Whats included?***

The dataset comprises multiple R9.4.1 and R10.3 flowcells of multiple
separately prepared samples; each sample was run on each flowcell type.

The initial sequencer outputs are included in self container directories.
In addition derived outputs from an automated pipeline are stored
separately.


***Details concerning sample preparations***

Below is described briefly the method of analyte preparation. Standard, published
protocols were followed with no intentional deviation.

*The following cell line samples were obtained from the NIGMS Human Genetic Cell
Repository at the Coriell Institute for Medical Research: GM24385*

- High molecular weight DNA from GM24385 lymphoblastoid cells was prepared by 
  [Evotec](https://www.evotec.com/en).
- Circulomics Short Read Eliminator XL
  protocol was used to deplete DNA fragments &lt; 40kb in length.
- DNA was end repaired and dA tailed prior to LSK based library preparation.
- DNA sequencing was performed using PromethION device.

The dataset comprises multiple flowcells for each pore:

| Pore   | Treatmeant       | Flowcells          |
|:------:|:----------------:|:------------------:|
| R9.4.1 |        SRE       | PAF27096, PAF27462 |
| R10.3  |        SRE       | PAF26223, PAF26161 |


***Location of the data***

The data is located in an Amazon Webservice S3 bucket at:

    s3://ont-open-data/gm24385_2020.09/

See the [tutorials](/tutorials/) page for information on downloading the dataset.


***Description of available data***

The uploaded dataset has been prepared using a snakemake pipeline to:

1. Align basecalls to reference sequence. All primary, secondary and
supplementary alignments are kept
2. Filter .bam file to list of regions defined in configuration file
retaining only primary alignments.
3. Produce read statistics from per-region .bams.
4. Repack/group source .fast5 files according to primary alignment .bams
to produce per-region .fast5 file sets.

For more details see our [post](/katuali_human_pipeline/) detailing the
pipeline and its outputs.
</content:encoded><content:thumbnail/></item><item><title><![CDATA[EPI2ME Labs Workflows]]></title><description><![CDATA[Introduction to EPI2ME Labs Nextflow bioinformatic workflows: available workflows and how to install and use them.]]></description><link>https://labs.epi2me.io/wfindex</link><guid isPermaLink="false">https://labs.epi2me.io/wfindex</guid><pubDate>Tue, 01 Sep 2020 00:00:00 GMT</pubDate><content:encoded>
EPI2ME Labs maintains a collection of [Nextflow](https://www.nextflow.io/) bioinformatics
workflows tailored to Oxford Nanopore Technologies long-read sequencing data. They
are curated and actively maintained by experts in long-read sequence analysis.

We are excited to offer our bioinformatics solutions using the Nextflow
reactive workflow framework. Nextflow has been selected as a preferred
framework because of its integration with container technologies, software
package managers and its scalability to cluster- and cloud-scale installations.
Nextflow also has growing user adoption through projects such as
[nf-core](https://nf-co.re/). These advantages will help us deliver varied
workflows with minimal requirements for the installation of additional
software.

## Installation

See the [Installation](/labsquickstart) page for a walkthrough to guide
installation of the pre-requisites for using our workflows through EPI2ME Labs.

For more information on running workflows through the EPI2ME Labs application
see the [Running a Workflow](/labsquickstart#running-a-workflow) section of our quick-start guide.

Our workflows are also freely available to use from the command-line on Linux, macOS, and Windows through WSL2. See our [Workflow Command-line Usage](/wfquickstart) page for more information.

## Available workflows

Below you will find a complete list of available workflows with a short
description of each. Each description is accompanied by a link to the
Github repository hosting the workflow and a sample workflow report.

### Basic Tasks

* [wf-alignment](https://github.com/epi2me-labs/wf-alignment) packages the [minimap2](https://github.com/lh3/minimap2) software and streamlines the
  process of mapping sequence reads to a reference genome and preparing summary
  statistics. It can also analyse the abundance of known molarity control
  experiments and use this information to derive the abundances of other species
  present in the sample. ([Sample report](/workflows/wf-alignment-report.html)) ([Documentation](/workflows/wf-alignment))
* [wf-demultiplex](https://github.com/epi2me-labs/wf-demultiplex) packages the
  standard Oxford Nanopore Technologies read demultiplexing software. Other
  workflows include this step such that this workflow is not typically
  required to be run in isolation; it is supplied however supplied standalone
  for users that require it. ([Sample report](/workflows/wf-demultiplex-report.html))

### Human genetics

* [wf-human-variation](https://www.github.com/epi2me-labs/wf-human-variation): our
all-in-one human variation workflow consolidates both the small variant calling from
wf-human-snp with the structual variant calling from wf-human-sv. This pipeline performs
the steps of the two pipelines simultaneously and the results are generated and output
in the same way as they would have been had the pipelines been run separately.
Users should migrate their analyses to wf-human-variation as the separate wf-human-snp
and wf-human-sv pipelines are no longer supported. ([Documentation](/workflows/wf-human-variation))


### Assembly

* [wf-clone-validation](https://www.github.com/epi2me-labs/wf-clone-validation): assembly
  of small plasmid sequences, for verifying the results of molecular
  cloning experiments. ([Sample report](/workflows/wf-clone-validation-report.html)) ([Documentation](/workflows/wf-clone-validation))
* [wf-bacterial-genomes](https://www.github.com/epi2me-labs/wf-bacterial-genomes): a workflow for
  running assembly or alignment of bacterial genomes and annotation. ([Sample report](/workflows/wf-bacterial-genomes-report.html)) ([Documentation](/workflows/wf-bacterial-genomes))

### Metagenomics

* [wf-metagenomics](https://github.com/epi2me-labs/wf-metagenomics) includes the [Kraken2](https://github.com/DerrickWood/kraken2) and [Minimap2](https://github.com/lh3/minimap2) software to facilitate the taxonomic classification of sequence reads from metagenome samples. ([Sample report](/workflows/wf-metagenomics-report.html)) ([Documentation](/workflows/wf-metagenomics))

### Direct RNA sequencing and cDNA

* [wf-transcript-target](https://github.com/epi2me-labs/wf-transcript-target) to review and consolidates transcripts of interest from direct RNA sequencing collections. This workflow can also be used for more general assessment of transcripts of interest. ([Sample report](/workflows/wf-transcript-target-report.html)) ([Documentation](/workflows/wf-transcript-target))
* [wf-transcriptomes](https://github.com/epi2me-labs/wf-transcriptomes): transcript assembly from cDNA or direct RNA reads using either a reference-guided or an experimental *de novo* reconstruction options. This workflow also provides differential gene expression and differential transcript usage analysis.

### Miscellaneous

* [wf-artic](https://www.github.com/epi2me-labs/wf-artic): a Nextflow workflow for
  running the ARTIC SARS-CoV-2 workflow on multiplexed MinION, GridION, and
  PromethION runs. ([Sample report](/workflows/wf-artic-report.html)) ([Documentation](/workflows/wf-artic))
* [wf-mpx](https://www.github.com/epi2me-labs/wf-mpx): a basic workflow for alignment of Monkeypox virus reads to a reference and alignment with flye. Produces a useful QC report.([Sample report](/workflows/wf-mpx-report.html)) ([Blog Post](https://labs.epi2me.io/basic-monkeypox-workflow))
* [wf-cas9](https://www.github.com/epi2me-labs/wf-cas9): a workflow for
  analysing on-target, off-target, and background sequencing from a
  Cas9 native DNA enrichment experiment.([Sample report](/workflows/wf-cas9-report.html)) ([Documentation](/workflows/wf-cas9))
</content:encoded><content:thumbnail>https://labs.epi2me.io/static/7b955fd3bff78ff21f10b5769a188346/thumbnail.jpg</content:thumbnail></item><item><title><![CDATA[EPI2ME Labs Tutorials]]></title><description><![CDATA[EPI2ME Labs maintains a collection of example bioinformatics notebooks for running analysis in your web browser. Our Python notebook tutorials walk through various topics suitable for bioinfomatics beginners to virtuosos.]]></description><link>https://labs.epi2me.io/nbindex</link><guid isPermaLink="false">https://labs.epi2me.io/nbindex</guid><pubDate>Tue, 01 Sep 2020 00:00:00 GMT</pubDate><content:encoded>
&gt; Development of EPI2ME Labs notebooks is currently on hold while we focus
&gt; on expanding our range of Nextflow [workflows](/wfindex). The information
&gt; below may be out of date. The ability to run our notebooks from within the
&gt; EPI2ME Labs application was removed from version 4.0.0.


EPI2ME Labs prior to version 4.0 extended the JupyterLab notebook framework with a pre-configured
analysis environment. The notebooks contain Python code needed to run data analysis,
however zero configuration of the code is required. The notebooks can be used
both by students and researchers new to nanopore sequence data analysis, and
more experienced bioinformaticians who are comfortable working with Python.


## Available notebooks

Below you will find a complete list of available notebooks with a short
description of each. Listed also are notebooks in development. The links direct
to pre-rendered versions of the notebooks to provide some illustrations of the
capabilities of each notebook; these pages are not interactive.


### Basic Tasks

* [Introduction to FASTQ](/notebooks/Introduction_to_fastq_file.html) - An introduction
  to the FASTQ format and simple file operations

* [Introduction to VCF](/notebooks/Introduction_to_Variant_Call_Format_(vcf)_files.html) - An
  introduction to Variant Call Format files and their manipulation in
  Python and on the command line.

* [Introduction to BAM](/notebooks/Introduction_to_SAM_and_BAM_files.html) - An
  introduction to the SAM and BAM formats for storing alignment data and their
  manipulation with `samtools`.

* [Introduction to FAST5](/notebooks/Introduction_to_Fast5_files.html) - A guide to
  handling raw sequencing data from Oxford Nanopore Technologies&apos; sequencing
  devices.

* [Basic QC](/notebooks/Basic_QC_Tutorial.html) - A simple workflow using the sequencing
  summary file, produced by MinKNOW and Guppy, to QC measures such as read
  length and quality.

* ***Alignment QC*** (Planned) - Walking through alignment of reads to a
  reference sequence, and and introduction to the world of `samtools` and
  `pysam`.

* [Adaptive Sampling Inputs](/notebooks/Curating_Adaptive_Sampling_input_files_for_MinKNOW.html) - A
  short snippet to aid the creation of the necessary files required by MinKNOW
  for exome Adaptive Sampling experiment.

### Assembly

* [Assembly tutorial](/notebooks/Assembly_Tutorial.html) - A workflow utilising Flye and
  Medaka to produce high quality assemblies of small to mid-sized genomes.

* ***Assembly assessment*** (Planned) - An introduction and tips on assessing
  the quality of an assembly using both reference-based and reference-free
  methods.



### Metagenomics
* [Post-EPI2ME 16S Analysis](/notebooks/Analysis_of_EPI2ME_16S_CSV_Output.html) - A
  quick demonstration of adding lineage information to EPI2ME 16S output, and
  grouping counts by rank.
* ***Metagenomic assembly*** (Planned) - A guided approach to metagenomic
  assembly and assessment based on Flye&apos;s metagenomic assembly.


  
### cDNA and RNA

* [Differential Gene Expression](/notebooks/Differential_gene_expression.html) -
  Pipeline for differential gene expression (DGE) and differential transcript
  usage (DTU) analysis using Nanopore long reads.
  
  __Note__: A new cDNA/RNA Nextflow workflow is planned that will include isoform detection,
  differential expression, fusion transcript identification and more.



### Other

* [EPI2ME Labs Tutorial](/notebooks/EPI2ME_Labs_Tutorial.html) - A meta tutorial
  showcasing the unique (or perhaps esoteric and idiosyncratic?) features of the
  EPI2ME Labs notebook environment. This is particularly useful for contributors
  to read.

* [Clone validation](/notebooks/Clone_validation_tutorial.html) - Validation of
  synthetic biological constructs using Nanopore sequencing rather than Sanger
  sequencing.

* [SARS-CoV-2 Analysis](/notebooks/SARS_CoV_2_Analysis_Workflow.html) - A notebook based
  around the ARTIC pipeline for the analysis of SARS-Cov-2 multiplexed amplicon
  datasets.

* [Modified bases](/notebooks/Modified_Base_Tutorial.html) - A demonstration of the use
  of Medaka to process and summarise the optional modified base output of
  Guppy.

* [Medaka tech. demo](/notebooks/Introduction_to_how_ONT&apos;s_medaka_works.html) - A deep
  dive into the inner workings of Oxford Nanopore Technologies&apos; Medaka software
</content:encoded><content:thumbnail>https://labs.epi2me.io/static/d5f2779e5a84bf92e5d939ed7cc9b20e/thumbnail.jpg</content:thumbnail></item><item><title><![CDATA[ONT Open Datasets]]></title><description><![CDATA[The Oxford Nanopore Technologies Open Data (ont-open-data) provides reference sequencing data from ONT sequencing devices.]]></description><link>https://labs.epi2me.io/dataindex</link><guid isPermaLink="false">https://labs.epi2me.io/dataindex</guid><pubDate>Tue, 01 Sep 2020 00:00:00 GMT</pubDate><content:encoded>
The Oxford Nanopore Technologies Open Data (ont-open-data) provides reference sequencing data from ONT sequencing devices. Data
access is provided through the [Registry of Open Data on AWS](https://registry.opendata.aws/ont-open-data/).

## Use of ONT Open Datasets

The data are freely available to all and can be used for:

1. Exploration of the characteristics of nanopore sequence data,
2. Assessment and reproduction of performance benchmarks,
3. Example datasets for analysis on EPI2ME, Oxford Nanopore Technologies’ cloud compute infrastructure,
4. Development of tools and methods.

The data deposited showcases DNA sequences from a representative subset of
sequencing chemistries. The datasets correspond to publicly-available reference
samples including the widely available [Genome In A Bottle](https://www.nist.gov/programs-projects/genome-bottle)
human reference samples. Raw data are provided with metadata and scripts to describe sample and
data provenance.

## Latest datasets and analyses

The list below represents the most recent dataset or analysis of its class.
As new datasets, basecallers, and analysis methods are developed this list
will change.

* [Genome In a Bottle Ashkenazi trio](/askenazim-lsk114) sequenced with R10.4.1 and LSK114. (COMING SOON)
* [Phased CpG modification calling with Remora](/gm24385-5mc-remora) in matched bisulfite and nanopore sequencing.
* [Reduced Representation Methylation Sequencing](/rrms2022.07) (RRMS) Seqeuncing of two cancer normal pairs with adaptive sampling.

We strive to keep all analyses current, though some may fall behind the more
current sequencing runs and basecalling results.

## Archived datasets

Historical datasets can also by found in the repository. Our previous data release blog posts are archived
under the [data-releases](/category/data-releases/) category.

## Data access

All data is available under from the [Registry of Open Data on AWS](https://registry.opendata.aws/ont-open-data/). See the [ONT Open Data Tutorials](/tutorials) page for more information.
</content:encoded><content:thumbnail>https://labs.epi2me.io/static/d625388ba09b90e0b9b8a67a5e80bdb7/thumbnail.jpg</content:thumbnail></item></channel></rss>